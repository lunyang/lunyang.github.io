<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>N-Shot å­¦ä¹ </title>
      <link href="/n-shot-learning/"/>
      <url>/n-shot-learning/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒè¿æ¥ï¼š<a href="https://blog.floydhub.com/n-shot-learning/" target="_blank" rel="noopener">https://blog.floydhub.com/n-shot-learning/</a></p><hr><blockquote><p><em>Artificial Intelligence is the new electricity - Andrew NG</em></p></blockquote><p>If AI is the new electricity, then data is the new coal.<br>Unfortunately, just as weâ€™ve seen a hazardous depletion in the amount of available coal, many AI applications have little or no data accessible to them.</p><p>New technology has made up for a lack of physical resources; likewise, new techniques are needed to allow applications with little data to perform satisfactorily. This is the issue at the heart of what is becoming a very popular field: <strong>N-shot Learning</strong>.</p><p><img src="/images/N-Shot%20Learning/em.gif" alt="Eminem image surrounded by the intro of Lose Yourself"></p><p>You may be asking, what the heck is a shot, anyway? Fair question. A shot is nothing more than a single example available for training, so in N-shot learning, we have N examples for training. With the term â€œfew-shot learningâ€, the <em>â€œfewâ€ usually lies between zero and five</em>, meaning that training a model with zero examples is known as zero-shot learning,  one example is one-shot learning, and so on. All of these variants are trying to solve the same problem with differing levels of training material.</p><h3 id="ä»€ä¹ˆæ˜¯-N-Shot"><a href="#ä»€ä¹ˆæ˜¯-N-Shot" class="headerlink" title="ä»€ä¹ˆæ˜¯ N-Shot?"></a>ä»€ä¹ˆæ˜¯ N-Shot?</h3><p>Why do we need this when we are already getting less than a 4% error in ImageNet?</p><p>To start, ImageNetâ€™s dataset contains a multitude of examples for machine learning, which is not always the case in fields like medical imaging, drug discovery and many others where AI could be crucially important. Typical deep learning architecture relies on substantial data for sufficient outcomes- ImageNet, for example, would need to train on hundreds of hotdog images before accurately assessing new images as hotdogs. And some datasets, much like a fridge after a 4th of July celebration, are greatly lacking in hotdogs.</p><p>There are many use cases for machine learning where data is scarce, and that is where this technology comes in. We need to train a deep learning model which has millions or even billions of parameters, all randomly initialized, to learn to classify an unseen image using no more than 5 images. To put it succinctly, our model has to train using a very limited number of hotdog images.</p><p>To approach an issue as complex as this one, we need to first define it clearly.<br>In the N-shot learning field, we have $n$ labeled examples of each $K$ classes, i.e. $Nâˆ—K$ total examples which we call support set $S$ . We also have to classify Query Set $Q$, where each example lies in one of the $K$ classes.  N-shot learning has three major sub-fields: zero-shot learning, one-shot learning, and few-shot learning, which each deserve individual attention.</p><h3 id="Zero-Shot-Learning"><a href="#Zero-Shot-Learning" class="headerlink" title="Zero-Shot Learning"></a>Zero-Shot Learning</h3><p>To me, this is the most interesting sub-field. With zero-shot learning, the target is to classify unseen classes without a single training example.</p><p>How does a machine â€œlearnâ€ without having any data to utilize?</p><p>Think about it this way. Can you classify an object without ever seeing it?</p><p>Yes, you can if you have adequate information about its appearance, properties, and functionality. Think back to how you came to understand the world as a kid. You could spot Mars in the night sky after reading about its color and where it would be that night, or identify the constellation Cassiopeia from only being told â€œitâ€™s basically a malformed â€˜Wâ€™â€.</p><p>According to this year trend in NLP, <a href="https://blog.floydhub.com/ten-trends-in-deep-learning-nlp/#9-zero-shot-learning-will-become-more-effective" target="_blank" rel="noopener">Zero shot learning will become more effective</a>.</p><p>A machine utilizes the metadata of the images to perform the same task. The metadata is nothing but the features associated with the image. Here is a list of a few papers in this field which gave excellent results.</p><ul><li><a href="https://arxiv.org/pdf/1711.06025v2.pdf" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a></li><li><a href="https://arxiv.org/pdf/1605.05395v1.pdf" target="_blank" rel="noopener">Learning Deep Representations of Fine-Grained Visual Descriptions</a></li><li><a href="https://arxiv.org/abs/1412.6568v3" target="_blank" rel="noopener">Improving zero-shot learning by mitigating the hubness problem</a></li></ul><h3 id="One-Shot-å­¦ä¹ "><a href="#One-Shot-å­¦ä¹ " class="headerlink" title="One-Shot å­¦ä¹ "></a>One-Shot å­¦ä¹ </h3><p>In one-shot learning, we only have a single example of each class. Now the task is to classify any test image to a class using that constraint. There are many different architectures developed to achieve this goal, such as <a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="noopener">Siamese Neural Networks</a>, which brought about major progress and led to exceptional results, and then <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">matching networks</a>, which also helped us make great leaps in this field.</p><p>Now there are many excellent papers for understanding one-shot learning, as below.</p><ul><li><a href="https://arxiv.org/pdf/1703.03400v3.pdf" target="_blank" rel="noopener">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li><li><a href="https://arxiv.org/pdf/1605.06065v1.pdf" target="_blank" rel="noopener">One-shot Learning with Memory-Augmented Neural Networks</a></li><li><a href="https://arxiv.org/pdf/1703.05175v2.pdf" target="_blank" rel="noopener">Prototypical Networks for Few-shot Learning</a></li></ul><h3 id="Few-Shot-å­¦ä¹ "><a href="#Few-Shot-å­¦ä¹ " class="headerlink" title="Few-Shot å­¦ä¹ "></a>Few-Shot å­¦ä¹ </h3><p>Few-shot learning is just a flexible version of one-shot learning, where we have more than one training example (usually two to five images, though most of the above-mentioned models can be used for few-shot learning as well).</p><p>During the 2019 Conference on Computer Vision and Pattern Recognition, <a href="https://arxiv.org/pdf/1812.02391v3.pdf" target="_blank" rel="noopener">Meta-Transfer Learning for Few-Shot Learning</a> was presented. This model set the precedent for future research; it gave state-of-the-art results and paved the path for more sophisticated meta-transfer learning methods.</p><p>Many of these meta-learning and reinforcement-learning algorithms are combined with typical deep learning algorithms to produce remarkable results. Prototypical networksï¼ˆåŸå‹ç½‘ç»œï¼‰ are one of the most popular deep learning algorithms, and are frequently used for this task.</p><p>In this article, weâ€™ll accomplish this task using <a href="https://arxiv.org/pdf/1703.05175v2.pdf" target="_blank" rel="noopener">Prototypical Networks</a> and understand how it works and why it works.</p><h2 id="åŸå‹ç½‘ç»œèƒŒåçš„æ€æƒ³"><a href="#åŸå‹ç½‘ç»œèƒŒåçš„æ€æƒ³" class="headerlink" title="åŸå‹ç½‘ç»œèƒŒåçš„æ€æƒ³"></a>åŸå‹ç½‘ç»œèƒŒåçš„æ€æƒ³</h2><p><img src="https://lh5.googleusercontent.com/dM2dhO5xN_JAAtPZy4Ns5x1rBuKU-bGZl8Hj6bO71qIP-F48nsCgmaqKVtotqEmunEoyLJIUZWQ2P7l1YqglZ3_XArvZ1yyOmicJdMJ48Bzw9k9jAvRTKL4cHDpHREEM97CwDkES" alt="A diagram of the function of the prototypical network. An encoder maps an image into a vector in the embedding space (N-Shot%20Learning/img1-1577192734392.png). Support images are used to define the prototype (stars). Distances between prototypes and encoded query images are used to classify them. Source"></p><blockquote><p>A diagram of the function of the prototypical network. An encoder maps an image into a vector in the embedding space (dark circles). Support images are used to define the prototype (stars). Distances between prototypes and encoded query images are used to classify them. <a href="https://www.semanticscholar.org/paper/Gaussian-Prototypical-Networks-for-Few-Shot-on-Fort/feaecb5f7a8d29636650db7c0b480f55d098a6a7/figure/1" target="_blank" rel="noopener">Source</a></p></blockquote><p>Unlike typical deep learning architecture, prototypical networks do not classify the image directly, and instead learn the mapping of an image in <a href="https://en.wikipedia.org/wiki/Metric_space" target="_blank" rel="noopener">metric space</a>.</p><p>For anyone needing a mathematics refresher, metric space deals with the notion of â€œdistanceâ€.  It does not have a distinguished â€œoriginâ€ point; instead, in metric space we only compute the distance of one point to another. You therefore lack the operations of addition and scalar multiplication that you have in a vector space (because, unlike with vectors, a point only represents a coordinate, and adding two coordinates or scaling a coordinate makes no sense!). Check out <a href="https://math.stackexchange.com/questions/114940/what-is-the-difference-between-metric-spaces-and-vector-spaces" target="_blank" rel="noopener">this</a> link to learn more about the difference between vector space and metric space.</p><p>Now that we have that background, we can begin to understand how prototypical networks do not classify the image directly, but instead learn the mapping of an image in metric space. As can be seen in the above diagram, the encoder maps the images of the same class within tight proximity to each other, while different classes are spaced at a considerable distance. This means that whenever a new example is given, the network just checks the nearest cluster and classifies the example to its corresponding class. The underlying model in the prototypical net that maps images into metric space can be called an â€œImage2Vectorâ€ model, which is a Convolutional Neural Network (CNN) based architecture.</p><p>Now for those who donâ€™t know a lot about CNNs, you can read more here:</p><ul><li>Check out the list of best deep learning courses <a href="https://blog.floydhub.com/best-deep-learning-courses-updated-for-2019/" target="_blank" rel="noopener">here</a>.</li><li>Check out the list of best deep learning book <a href="https://blog.floydhub.com/best-deep-learning-books-updated-for-2019/" target="_blank" rel="noopener">here</a>.</li><li>To learn and apply it quickly refer to <a href="https://blog.floydhub.com/building-your-first-convnet/" target="_blank" rel="noopener">Building Your First ConvNet</a></li></ul><h3 id="A-brief-Introduction-to-Prototypical-Networks"><a href="#A-brief-Introduction-to-Prototypical-Networks" class="headerlink" title="A brief Introduction to Prototypical Networks"></a>A brief Introduction to Prototypical Networks</h3><p>Simply put, their aim is to train a classifier. This classifier can then make generalizations regarding new classes that are unavailable during training, and only needs a small number of examples of each new class. Hence, the training set contains images of a set of classes, while our test set contains images of another set of classes which is entirely disjointed from the former one. In this model, the examples are divided randomly into the support set and query set.</p><h3 id="Overview-of-Prototypical-Network"><a href="#Overview-of-Prototypical-Network" class="headerlink" title="Overview of Prototypical Network"></a>Overview of Prototypical Network</h3><p><img src="https://lh3.googleusercontent.com/D1r0cQ9QlrF3b-v4PlM1T_8kmdo7adxrTak5JcDZbhPxucxcdME9nHZsvC1qOtjIpj5SqcYVvw8NRrjBj9ryl6deOPJWPlOJqNnwMHM24hSOUIPgh1TkA4ZhGZTosr_PVNPk_lOj" alt="Few-shot prototypes $C_k$ are computed as the mean of embedded support examples for each class. The encoder maps new image(N-Shot%20Learning/img2.png) and classifies it to the closest class like $C_2$ in the above image."></p><blockquote><p>Few-shot prototypes $C_k$ are computed as the mean of embedded support examples for each class. The encoder maps new image($X$) and classifies it to the closest class like $C_2$ in the above image. <a href="https://arxiv.org/pdf/1703.05175.pdf" target="_blank" rel="noopener">Source</a></p></blockquote><p>In the context of few-shot learning, a training iteration is known as an episode. An episode is nothing but a step in which we train the network once, calculate loss and backpropagate the error.  In each episode, we select $Nc$ classes at random from the training set. For each class, we randomly sample $Ns$ images. These images belong to the support set and the learning model is known as NsNs-shot model. Another randomly sampled Nq images are obtained which belongs to the query set. Here NcNc, NsNs &amp; NqNq are just hyperparameters in the model where NcNc is the number of classes per iteration, NsNs is the number of support examples per class and NqNq is the number of query examples per class.</p><p>After that, we retrieve D-dimensional points from the support set images by passing them through â€œImage2Vectorâ€ model. This model encodes an image with its corresponding point in the metric space. For each class we now have multiple points, but we need to represent them as one point for each class. Hence, we compute geometric center, i.e. mean of the points, for each class. After that, we also need to classify the query images.</p><p>To do that, we first need to encode every image in the query set into a point. After that, the distance from each centroid to each query point is calculated. At last, each query image is predicted to lie in the class which is nearest to it. Thatâ€™s how the model works in general.</p><p>But the question now is, what is the architecture of this â€œImage2Vectorâ€ model?</p><h3 id="Image2Vector-function"><a href="#Image2Vector-function" class="headerlink" title="Image2Vector function"></a>Image2Vector function</h3><p><img src="/images/N-Shot%20Learning/2019090318384447.png" alt="imagessssss"></p><blockquote><p>Image2vector CNN architecture used in the paper.</p></blockquote><p>For all practical purposes, 4â€“5 CNN blocks are used. As shown in the above image, each block consists of a CNN layer followed by batch normalization, then by a ReLu activation function which leads into a max pool layer. After all the blocks, the remaining output is flattened and returned as a result. This is the architecture used in the <a href="https://arxiv.org/pdf/1703.05175v2.pdf" target="_blank" rel="noopener">paper</a> and you can use any architecture you like. It is necessary to know that though we call it â€œImage2Vectorâ€ model, it actually converts an image into a 64-dimensional point in the metric space. To understand the difference more, check out these <a href="https://math.stackexchange.com/questions/645672/what-is-the-difference-between-a-point-and-a-vector" target="_blank" rel="noopener">math stack exchange</a> answers.</p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p><img src="N-Shot%20Learning/nll.jpg" alt="The working of negative log-likelihood. "></p><blockquote><p>The working of negative log-likelihood. <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll" target="_blank" rel="noopener">Source</a>.</p></blockquote><p>Now that we know how the model is working, you might be wondering how weâ€™re going to calculate loss function. We need a loss function which is robust enough for our model to learn representation quickly and efficiently. Prototypical Nets use log-softmax loss, which is nothing but log over softmax loss. The log-softmax has the effect of heavily penalizing the model when it fails to predict the correct class, which is what we need. To know more about the loss function go<a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/" target="_blank" rel="noopener"> here</a>. <a href="https://discuss.pytorch.org/t/logsoftmax-vs-softmax/21386" target="_blank" rel="noopener">Here</a> is a very good discussion about softmax and log-softmax.</p><h3 id="Dataset-overview"><a href="#Dataset-overview" class="headerlink" title="Dataset overview"></a>Dataset overview</h3><p><img src="N-Shot%20Learning/omniglot.jpg" alt="A few classes of images in Omniglot dataset"></p><blockquote><p> A few classes of images in Omniglot dataset. <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Source</a>.</p></blockquote><p>The network was trained on the <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Omniglot dataset</a>. The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1,623 different handwritten characters from 50 different alphabets. Then, to increase the number of classes, all the images are rotated by 90, 180 and 270 degrees, with each rotation resulting in an additional class. Hence the total count of classes reached to 6,492(1,623 * 4) classes. We split images of 4,200 classes to training data while the rest went to the test set. For each episode, we trained the model on 5 examples from each of the 64 randomly selected classes. We trained our model for 1 hour and got about 88% accuracy. The official paper claimed to achieve the accuracy of 99.7% after training for a few hours and tuning a few parameters.</p><p><strong>Time to get your hands dirty!</strong></p><p>You can easily run <a href="https://github.com/Hsankesara/Prototypical-Networks" target="_blank" rel="noopener">the code</a> by clicking on the button below.</p><p>Letâ€™s dive into the code!</p><pre class=" language-lang-python"><code class="language-lang-python">class Net(nn.Module):    """    Image2Vector CNN which takes the image of dimension (28x28x3) and return column vector length 64    """    def sub_block(self, in_channels, out_channels=64, kernel_size=3):        block = torch.nn.Sequential(            torch.nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),            torch.nn.BatchNorm2d(out_channels),            torch.nn.ReLU()            torch.nn.MaxPool2d(kernel_size=2))        return block    def __init__(self):        super(Net, self).__init__()        self.convnet1 = self.sub_block(3)        self.convnet2 = self.sub_block(64)        self.convnet3 = self.sub_block(64)        self.convnet4 = self.sub_block(64)    def forward(self, x):        x = self.convnet1(x)        x = self.convnet2(x)        x = self.convnet3(x)        x = self.convnet4(x)        x = torch.flatten(x, start_dim=1)        return x</code></pre><p>The above snippet is an implementation of image2vector CNN architecture. It takes images of dimensions 28x28x3 and returns a vector of length 64.</p><pre class=" language-lang-python"><code class="language-lang-python">class PrototypicalNet(nn.Module):    def __init__(self, use_gpu=False):        super(PrototypicalNet, self).__init__()        self.f = Net()        self.gpu = use_gpu        if self.gpu:            self.f = self.f.cuda()    def forward(self, datax, datay, Ns,Nc, Nq, total_classes):        """        Implementation of one episode in Prototypical Net        datax: Training images        datay: Corresponding labels of datax        Nc: Number  of classes per episode        Ns: Number of support data per class        Nq:  Number of query data per class        total_classes: Total classes in training set        """        k = total_classes.shape[0]        K = np.random.choice(total_classes, Nc, replace=False)        Query_x = torch.Tensor()        if(self.gpu):            Query_x = Query_x.cuda()        Query_y = []        Query_y_count = []        centroid_per_class  = {}        class_label = {}        label_encoding = 0        for cls in K:            S_cls, Q_cls = self.random_sample_cls(datax, datay, Ns, Nq, cls)            centroid_per_class[cls] = self.get_centroid(S_cls, Nc)            class_label[cls] = label_encoding            label_encoding += 1            Query_x = torch.cat((Query_x, Q_cls), 0) # Joining all the query set together            Query_y += [cls]            Query_y_count += [Q_cls.shape[0]]        Query_y, Query_y_labels = self.get_query_y(Query_y, Query_y_count, class_label)        Query_x = self.get_query_x(Query_x, centroid_per_class, Query_y_labels)        return Query_x, Query_y    def random_sample_cls(self, datax, datay, Ns, Nq, cls):        """        Randomly samples Ns examples as support set and Nq as Query set        """        data = datax[(datay == cls).nonzero()]        perm = torch.randperm(data.shape[0])        idx = perm[:Ns]        S_cls = data[idx]        idx = perm[Ns : Ns+Nq]        Q_cls = data[idx]        if self.gpu:            S_cls = S_cls.cuda()            Q_cls = Q_cls.cuda()        return S_cls, Q_cls    def get_centroid(self, S_cls, Nc):        """        Returns a centroid vector of support set for a class        """        return torch.sum(self.f(S_cls), 0).unsqueeze(1).transpose(0,1) / Nc    def get_query_y(self, Qy, Qyc, class_label):        """        Returns labeled representation of classes of Query set and a list of labels.        """        labels = []        m = len(Qy)        for i in range(m):            labels += [Qy[i]] * Qyc[i]        labels = np.array(labels).reshape(len(labels), 1)        label_encoder = LabelEncoder()        Query_y = torch.Tensor(label_encoder.fit_transform(labels).astype(int)).long()        if self.gpu:            Query_y = Query_y.cuda()        Query_y_labels = np.unique(labels)        return Query_y, Query_y_labels    def get_centroid_matrix(self, centroid_per_class, Query_y_labels):        """        Returns the centroid matrix where each column is a centroid of a class.        """        centroid_matrix = torch.Tensor()        if(self.gpu):            centroid_matrix = centroid_matrix.cuda()        for label in Query_y_labels:            centroid_matrix = torch.cat((centroid_matrix, centroid_per_class[label]))        if self.gpu:            centroid_matrix = centroid_matrix.cuda()        return centroid_matrix    def get_query_x(self, Query_x, centroid_per_class, Query_y_labels):        """        Returns distance matrix from each Query image to each centroid.        """        centroid_matrix = self.get_centroid_matrix(centroid_per_class, Query_y_labels)        Query_x = self.f(Query_x)        m = Query_x.size(0)        n = centroid_matrix.size(0)        # The below expressions expand both the matrices such that they become compatible with each other in order to calculate L2 distance.        centroid_matrix = centroid_matrix.expand(m, centroid_matrix.size(0), centroid_matrix.size(1)) # Expanding centroid matrix to "m".        Query_matrix = Query_x.expand(n, Query_x.size(0), Query_x.size(1)).transpose(0,1) # Expanding Query matrix "n" times        Qx = torch.pairwise_distance(centroid_matrix.transpose(1,2), Query_matrix.transpose(1,2))        return Qx</code></pre><p>The above snippet is an implementation of a single episode in Prototypical Net. It is well commented, but if you have any doubts just ask in the comments or create an issue <a href="https://github.com/Hsankesara/DeepResearch/" target="_blank" rel="noopener">here</a>.</p><p><img src="/images/N-Shot%20Learning/pipeline.jpg" alt="Overview of the  Network"></p><blockquote><p>Overview of the Network. <a href="https://youtu.be/wcKL05DomBU" target="_blank" rel="noopener">Source</a>.</p></blockquote><p>The code is structured in the same format in which the algorithm is explained. We give the prototypical network function the following inputs: input image data, input labels, number of classes per iteration i.e NcNc , number of support examples per class i.e NsNs and number of query examples per class i.e. NqNq. The function returns QueryxQueryx, which is a distance matrix from each Query point to each mean point and QueryyQueryy which is a vector containing labels corresponding to QueryxQueryx. QueryyQueryy stores the class in which images of QueryxQueryx actually belong.  In the above image, we can see that 3 classes are used, i.e. NcNc =3, and that for each class, a total of 5 examples are used for training, i.e. NsNs=5. Above SS represents the support set that contains those 15 (Nsâˆ—NcNsâˆ—Nc ) images and XX represents the query set. Notice that both support set and query set passes through ff, which is nothing but our â€œImage2Vectorâ€ function. It mapped all the images in metric space. Letâ€™s break the whole process down step by step.</p><p>First of all, we choose NcNc classes randomly from the input data. For each class, we randomly select a support set and a query set from the images using the <code>random_sample_cls</code> function. In the above image, SS is the support set and XX is the query set. Now that we chose the classes (C1C1, C2C2, and C3C3), we pass all the support set examples through the â€œImage2vectorâ€ model and compute the centroid for each class using the <code>get_centroid</code> function.  The same can be observed in the nearby image where C1C1 and C2C2 are the center, computed using the neighboring points. Each centroid represents a class and will be used for classifying queries.</p><p><img src="N-Shot%20Learning/helper.png" alt="Centroid calculation in the Network"></p><blockquote><p>Centroid calculation in the Network. <a href="https://youtu.be/wcKL05DomBU" target="_blank" rel="noopener">Source</a>.</p></blockquote><p>After computing centroid for each class, we now have to predict the query image to one of the classes. For that, we need actual labels corresponding to each query, which we get by using the <code>get_query_y</code> function. The QueryyQueryy is categorical data and the function converts this categorical text data into a one-hot vector, which will only be â€œ1â€ in the row label where the image corresponding to the column point actually belongs, and will be â€œ0â€ else in the column.</p><p>After that, we need points corresponding to each QueryxQueryx image in order to classify it. We get the points using â€œImage2Vectorâ€ model and now we need to classify them. For that purpose, we calculate the distance between each point in QueryxQueryx to each class center. This gives us a matrix where index ijij represents the distance of the point corresponding to ith query image from the center of jth class. We used the <code>get_query_x</code> function to construct the matrix and save the matrix in the QueryxQueryx variable. The same can be seen in the nearby image. For each example in the query set, The distance it has from C1C1, C2C2 and C3C3 is being calculated. In this case, xx is closest to C2C2 and we can therefore say that xx is predicted to belong to class C2C2.</p><p>Programmatically, we can use a simple argmin function to do the same, i.e. to find out the class where the image was predicted to lie. Then we use the predicted class and actual class to calculate loss and backpropagate the error.</p><p>If you want to use the trained model or just have to retrain again for yourself, <a href="https://github.com/Hsankesara/DeepResearch/tree/master/Prototypical_Nets" target="_blank" rel="noopener">here</a> is my implementation. You can use it as an API and train the model using a couple of lines of code. You can find this network in action <a href="https://www.kaggle.com/hsankesara/prototypical-net/" target="_blank" rel="noopener">here</a>.</p><h3 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h3><p>Here are a few resources that might help you learn this topic thoroughly:</p><ul><li><a href="https://sorenbouma.github.io/blog/oneshot/" target="_blank" rel="noopener">One Shot Learning with Siamese Networks using Keras</a></li><li><a href="https://towardsdatascience.com/one-shot-learning-face-recognition-using-siamese-neural-network-a13dcf739e" target="_blank" rel="noopener">One-Shot Learning: Face Recognition using Siamese Neural Network</a></li><li><a href="https://github.com/AntreasAntoniou/MatchingNetworks" target="_blank" rel="noopener">Matching network official implementation</a></li><li><a href="https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch" target="_blank" rel="noopener">Prototypical Network official implementation.</a></li><li><a href="https://arxiv.org/abs/1803.00676" target="_blank" rel="noopener">Meta-Learning for Semi-Supervised Few-Shot Classification</a></li></ul><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>Though prototypical networks produce great results, they still have limitations. The first one is the lack of generalization. It works on the Omniglot dataset well because all the images in there are images of a character, and hence share a few similar characteristics. However, if we were to try using the model to classify different breeds of cats, it wouldnâ€™t give us accurate results. Cats and character images share few characteristics, and the number of common features which can be exploited to map the image on the corresponding metric space is negligible.</p><p>Another limitation to prototypical networks is that they only use mean to decide center, and ignore the variance in support set. This hinders the classifying ability of the model when the images have noise. This limitation is overcome by using <a href="https://arxiv.org/abs/1708.02735" target="_blank" rel="noopener">Gaussian Prototypical Networks</a> which utilizes the variance in the class by modeling the embedded points using Gaussian formulations.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Few-Shot learning has been a topic of active research for a while. There are many novel approaches which use prototypical networks, like this <a href="https://arxiv.org/abs/1803.00676" target="_blank" rel="noopener">meta-learning</a> one, and which show great results. Researchers are also exploring it with reinforcement-learning, which also has great potential. The best thing about this model is that it is simple and easy to understand, and it gives incredible results.</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world/"/>
      <url>/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>äº’æ€¼çš„è‰ºæœ¯ï¼šä»é›¶ç›´è¾¾ WGAN-GP</title>
      <link href="/001-hu-dui-de-yi-zhu-cong-ling-zhi-da-wgan-gp/"/>
      <url>/001-hu-dui-de-yi-zhu-cong-ling-zhi-da-wgan-gp/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒèµ„æ–™ï¼šPaperWeekly ç¬¬41æœŸ | äº’æ€¼çš„è‰ºæœ¯ï¼šä»é›¶ç›´è¾¾ WGAN-GP</p><hr><p> <strong>1</strong> <strong>å‰è¨€</strong></p><p>GANï¼Œå…¨ç§° Generative Adversarial Netsï¼Œä¸­æ–‡åæ˜¯ç”Ÿæˆå¯¹æŠ—å¼ç½‘ç»œã€‚å¯¹äº GAN æ¥è¯´ï¼Œæœ€é€šä¿—çš„è§£é‡Šå°±æ˜¯â€œé€ å‡è€…-é‰´åˆ«è€…â€çš„è§£é‡Šï¼Œå¦‚è‰ºæœ¯ç”»çš„ä¼ªé€ è€…å’Œé‰´åˆ«è€…ã€‚ä¸€å¼€å§‹ä¼ªé€ è€…å’Œé‰´åˆ«è€…çš„æ°´å¹³éƒ½ä¸é«˜ï¼Œä½†æ˜¯é‰´åˆ«è€…è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“é‰´åˆ«å‡ºä¼ªé€ è€…ä¼ªé€ å‡ºæ¥çš„è‰ºæœ¯ç”»ã€‚ä½†éšç€ä¼ªé€ è€…å¯¹ä¼ªé€ æŠ€æœ¯çš„å­¦ä¹ åï¼Œå…¶ä¼ªé€ çš„è‰ºæœ¯ç”»ä¼šè®©é‰´åˆ«è€…è¯†åˆ«é”™è¯¯ï¼›æˆ–è€…éšç€é‰´åˆ«è€…å¯¹é‰´åˆ«æŠ€æœ¯çš„å­¦ä¹ åï¼Œèƒ½å¤Ÿå¾ˆç®€å•çš„é‰´åˆ«å‡ºä¼ªé€ è€…ä¼ªé€ çš„è‰ºæœ¯ç”»ã€‚è¿™æ˜¯ä¸€ä¸ªåŒæ–¹ä¸æ–­å­¦ä¹ æŠ€æœ¯ï¼Œä»¥è¾¾åˆ°æœ€é«˜çš„ä¼ªé€ å’Œé‰´åˆ«æ°´å¹³çš„è¿‡ç¨‹ã€‚ ç„¶è€Œï¼Œç¨å¾®æ·±å…¥äº†è§£çš„è¯»è€…å°±ä¼šå‘ç°ï¼Œ<strong>è·Ÿç°å®ä¸­çš„é€ å‡è€…ä¸åŒï¼Œé€ å‡è€…ä¼šä¸æ—¶ä¿±è¿›åœ°ä½¿ç”¨æ–°ææ–™æ–°æŠ€æœ¯æ¥é€ å‡ï¼Œè€Œ GAN æœ€ç¥å¥‡è€Œåˆè®©äººå›°æƒ‘çš„åœ°æ–¹æ˜¯å®ƒèƒ½å¤Ÿå°†éšæœºå™ªå£°æ˜ å°„ä¸ºæˆ‘ä»¬æ‰€å¸Œæœ›çš„æ­£æ ·æœ¬ï¼Œæœ‰å™ªå£°å°±æœ‰æ­£æ ·æœ¬ï¼Œè¿™ä¸æ˜¯æ— æœ¬ç”Ÿæ„å—ï¼Œå¤šåˆ’ç®—ã€‚</strong></p><p>å¦ä¸€ä¸ªæƒ…å†µæ˜¯ï¼Œè‡ªä» WGAN æå‡ºä»¥æ¥ï¼ŒåŸºæœ¬ä¸Š GAN çš„ä¸»æµç ”ç©¶éƒ½å·²ç»å˜æˆäº† WGAN ä¸Šå»äº†ï¼Œä½† WGAN çš„å½¢å¼äº‹å®ä¸Šå·²ç»è·Ÿâ€œä¼ªé€ è€…-é‰´åˆ«è€…â€å·®å¾—æ¯”è¾ƒè¿œäº†ã€‚è€Œä¸” WGAN è™½ç„¶æœ€åçš„å½¢å¼å¹¶ä¸å¤æ‚ï¼Œä½†æ˜¯æ¨å¯¼è¿‡ç¨‹å´ç”¨åˆ°äº†è¯¸å¤šå¤æ‚çš„æ•°å­¦ï¼Œä½¿å¾—æˆ‘æ— å¿ƒç ”è¯»åŸå§‹è®ºæ–‡ã€‚è¿™è¿«ä½¿æˆ‘è¦æ‰¾ä»ä¸€æ¡ç®€æ˜ç›´è§‚çš„çº¿ç´¢æ¥ç†è§£ GANã€‚å¹¸å¥½ï¼Œç»è¿‡ä¸€æ®µæ—¶é—´çš„æ€è€ƒï¼Œæœ‰ç‚¹æ”¶è·ã€‚ </p><p>åœ¨æ­£æ–‡ä¹‹å‰ï¼Œå…ˆå£°æ˜ï¼š<strong>ç¬”è€…æ‰€æœ‰çš„ GAN çš„çŸ¥è¯†ï¼Œä»…ä»…ä»ç½‘ä¸Šçš„ç§‘æ™®æ–‡æ‰€è¯»è€Œæ¥ï¼Œæˆ‘å¹¶æ²¡æœ‰ç›´æ¥è¯»è¿‡ä»»ä½•å…³äº GAN çš„è®ºæ–‡ï¼Œå› æ­¤ï¼Œæ–‡ä¸­çš„ç»“æœå¯èƒ½è·Ÿä¸»æµçš„ç»“æœæœ‰é›·åŒï¼Œä¹Ÿå¯èƒ½æœ‰å¾ˆå¤§å‡ºå…¥ï¼Œè€Œä¸”æœ¬æ–‡çš„è®²è¿°æ–¹æ³•å¹¶ä¸ç¬¦åˆ GAN çš„å†å²å‘å±•è¿›ç¨‹ã€‚ä¸¥è°¨æ²»å­¦è€…æ…å…¥ã€‚</strong></p><p>æ³¨ï¼šå¦‚æ— æŒ‡æ˜ï¼Œæœ¬æ–‡æ‰€è°ˆåˆ°çš„ GAN éƒ½æ˜¯å¹¿ä¹‰çš„ï¼Œå³åŒ…æ‹¬åŸå§‹ GANã€WGAN ç­‰ç­‰ï¼Œå¯¹å®ƒä»¬ä¸ä½œåŒºåˆ†ã€‚æ–‡ä¸­å‡ºç°çš„æ­£æ ·æœ¬ã€çœŸå®æ ·æœ¬ï¼Œéƒ½æ˜¯æŒ‡é¢„å…ˆæŒ‡å®šçš„ä¸€æ‰¹æ ·æœ¬ï¼Œè€Œç”Ÿæˆæ ·æœ¬åˆ™æŒ‡çš„æ˜¯éšæœºå™ªå£°é€šè¿‡ç”Ÿæˆæ¨¡å‹ G å˜æ¢æ‰€å¾—çš„ç»“æœã€‚</p><p> <strong>2</strong> <strong>ä¸€é“é¢è¯•é¢˜</strong></p><p>ä¸€é“ç»å…¸çš„é¢è¯•é¢˜æ˜¯ï¼š<strong>å¦‚æœæœ‰ä¸€ä¸ªä¼ªéšæœºæ•°ç¨‹åºèƒ½å¤Ÿç”Ÿæˆ [0,1] ä¹‹é—´çš„å‡åŒ€éšæœºæ•°ï¼Œé‚£ä¹ˆå¦‚ä½•ç”±å®ƒæ¥ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„ä¼ªéšæœºæ•°ï¼Ÿæ¯”å¦‚æ€ä¹ˆå°† U[0,1] æ˜ å°„æˆ N(0,1)ï¼Ÿ</strong></p><p>è¿™é“é¢˜ä¸åŒçš„è§’åº¦æœ‰ä¸åŒçš„åšæ³•ï¼Œå·¥ç¨‹ä¸Šçš„åšæ³•æœ‰ï¼šåŒæ—¶è¿è¡Œ n ä¸ªè¿™æ ·çš„ä¼ªéšæœºæ•°ç¨‹åºï¼Œæ¯æ­¥äº§ç”Ÿ n ä¸ªéšæœºæ•°ï¼Œé‚£ä¹ˆè¿™ n ä¸ªæ•°çš„å’Œå°±è¿‘ä¼¼æœä»æ­£æ€åˆ†å¸ƒäº†ã€‚ä¸è¿‡ï¼Œè¿™é‡Œä¸å…³å¿ƒå·¥ç¨‹åšæ³•ï¼Œè€Œå…³å¿ƒç†è®ºä¸Šçš„åšæ³•ã€‚ç†è®ºä¸Šçš„åšæ³•æ˜¯ï¼šå°† Xâˆ¼U[0,1] ç»è¿‡å‡½æ•° Y=f(X) æ˜ å°„ä¹‹åï¼Œå°±æœ‰ $Yâˆ¼N(0,1)$ äº†ã€‚è®¾ $Ï(x)$ æ˜¯ $U[0,1] $æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œé‚£ä¹ˆ $[x,x+dx] $å’Œ $[y,y+dy] $è¿™ä¸¤ä¸ªåŒºé—´çš„æ¦‚ç‡åº”è¯¥ç›¸ç­‰ï¼Œè€Œæ ¹æ®æ¦‚ç‡å¯†åº¦å®šä¹‰ï¼Œ$Ï(x)$ ä¸æ˜¯æ¦‚ç‡ï¼Œ$Ï(x)dx$ æ‰æ˜¯æ¦‚ç‡ï¼Œå› æ­¤æœ‰</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640.webp" alt="640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=" style="zoom:67%;" /></p><p>é‚£ä¹ˆï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613337068.webp" alt="img" style="zoom:67%;" /></p><p>è¿™é‡Œ Î¦(y) æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼Œæ‰€ä»¥ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613349673.webp" alt="img" style="zoom:67%;" /></p><p>æ³¨æ„åˆ°ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ˜¯æ— æ³•ç”¨åˆç­‰å‡½æ•°æ˜¾å¼è¡¨ç¤ºå‡ºæ¥çš„ï¼Œæ›´ä¸ç”¨è¯´å®ƒçš„é€†å‡½æ•°äº†ã€‚è¯´ç™½äº†ï¼ŒY=f(X) çš„ f çš„ç¡®æ˜¯å­˜åœ¨çš„ï¼Œä½†å¾ˆå¤æ‚ã€‚æ­£æ€åˆ†å¸ƒæ˜¯å¸¸è§çš„ã€ç›¸å¯¹ç®€å•çš„åˆ†å¸ƒï¼Œä½†è¿™ä¸ªæ˜ å°„å·²ç»è¿™ä¹ˆå¤æ‚äº†ã€‚å¦‚æœæ¢äº†ä»»æ„åˆ†å¸ƒï¼Œç”šè‡³æ¦‚ç‡å¯†åº¦å‡½æ•°éƒ½ä¸èƒ½æ˜¾å¼å†™å‡ºæ¥ï¼Œé‚£ä¹ˆå¤æ‚åº¦å¯æƒ³è€ŒçŸ¥ã€‚</p><p><strong>3. ç¥ç»å¤§æ³•å¥½</strong></p><p>ç°åœ¨æˆ‘ä»¬å°†é—®é¢˜ä¸€èˆ¬åŒ–ï¼š<strong>å¦‚ä½•æ‰¾åˆ°æ˜ å°„ Y=f(X)ï¼ŒæŠŠæœä»å‡åŒ€åˆ†å¸ƒ X æ˜ å°„åˆ°æŒ‡å®šçš„åˆ†å¸ƒï¼Ÿåœ¨ä¸€èˆ¬æƒ…å½¢ä¸‹ï¼Œè¿™ä¸ªæŒ‡å®šçš„åˆ†å¸ƒæ˜¯é€šè¿‡ç»™å‡ºä¸€æ‰¹å…·ä½“çš„åˆ†å¸ƒæ ·æœ¬ Z=(z1,z2,â€¦,zN) æ¥æè¿°çš„ï¼ˆæ¯”å¦‚ï¼Œç»™å‡ºä¸€æ‰¹æœä»æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œè€Œä¸æ˜¯ç»™å‡ºæ¦‚ç‡å¯†åº¦</strong></p><p>è¿™ä¸ªé—®é¢˜ç›¸å½“ä¸€èˆ¬åŒ–ï¼Œè·Ÿ GAN æ‰€åšçš„äº‹æƒ…ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒGAN ä¹Ÿæ˜¯å¸Œæœ›æŠŠå‡åŒ€çš„éšæœºå™ªå£°æ˜ å°„æˆç‰¹å®šåˆ†å¸ƒï¼Œè¿™ä¸ªç‰¹å®šåˆ†å¸ƒç”±ä¸€ç»„â€œæ­£æ ·æœ¬â€æè¿°ã€‚è¿™æ ·çš„ç†è§£å°±å¯ä»¥å›ç­”æˆ‘ä»¬å¼€å¤´çš„ä¸€ä¸ªå°é—®é¢˜äº†ï¼š<strong>ä¸ºä»€ä¹ˆ GAN å¯ä»¥å°†å™ªå£°å˜æ¢æˆæ­£æ ·æœ¬ï¼Ÿäº‹å®ä¸Š GAN å¹¶ä¸æ˜¯å­¦ä¹ å™ªå£°åˆ°æ­£æ ·æœ¬çš„å˜æ¢ï¼Œè€Œæ˜¯å­¦ä¹ å‡åŒ€åˆ†å¸ƒåˆ°æŒ‡å®šåˆ†å¸ƒçš„å˜æ¢ã€‚</strong>å‡å¦‚å­¦ä¹ æˆåŠŸäº†ï¼Œé‚£ä¹ˆè¾“å…¥ä¸€ä¸ªéšæœºå™ªå£°ï¼Œé‚£ä¹ˆå°±å˜æ¢æˆæŒ‡å®šåˆ†å¸ƒçš„æ•°æ®ï¼Œè€Œé€šå¸¸æ¥è¯´<strong>æˆ‘ä»¬æŒ‡å®šçš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªæ¯”è¾ƒâ€œçª„â€çš„åˆ†å¸ƒ</strong>ï¼ˆæ¯”å¦‚æŒ‡å®šçš„æ­£æ ·æœ¬æ˜¯æŸä¸€ç±»å›¾ç‰‡çš„é›†åˆï¼Œä½†äº‹å®ä¸Šå›¾ç‰‡æ— ç©·æ— å°½ï¼ŒæŸä¸€ç±»çš„å›¾ç‰‡æ˜¯ç›¸å½“çª„çš„ï¼‰ï¼Œæ‰€ä»¥éƒ½ä¼šæ˜ å°„åˆ°æˆ‘ä»¬çœ¼ä¸­çš„â€œæ­£æ ·æœ¬â€å»ã€‚</p><p>å‰é¢æ­£æ€åˆ†å¸ƒçš„ä¾‹å­å·²ç»è¡¨æ˜ï¼Œè¿™ä¸ªæ˜ å°„ f é€šå¸¸éƒ½æ˜¯å¾ˆå¤æ‚çš„ï¼Œå› æ­¤æ²¡å¿…è¦æ±‚å®ƒçš„è§£æè§£ã€‚è¿™æ—¶å€™â€œç¥ç»å¤§æ³•â€å°±ç™»åœºäº†ï¼š<strong>ç†Ÿæ‚‰ç¥ç»ç½‘ç»œçš„è¯»è€…éƒ½çŸ¥é“ï¼Œæˆ‘ä»¬æ€»å¯ä»¥ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆä»»æ„å‡½æ•°ï¼Œå› æ­¤ï¼Œä¸å¦¨ç”¨ä¸€ä¸ªå¸¦æœ‰å¤šä¸ªå‚æ•°çš„ç¥ç»ç½‘ç»œ G(X,Î¸) å»æ‹Ÿåˆå®ƒï¼Ÿåªè¦æŠŠå‚æ•° Î¸ è®­ç»ƒå¥½ï¼Œå°±å¯ä»¥è®¤ä¸º Y=G(X,Î¸) äº†ã€‚</strong></p><p>å¯æ˜¯ï¼Œé—®é¢˜åˆæ¥äº†ï¼šæ‹Ÿåˆä»€ä¹ˆç›®æ ‡å‘¢ï¼Ÿæˆ‘ä»¬æ€ä¹ˆçŸ¥é“ Y=G(X,Î¸) è·ŸæŒ‡å®šçš„åˆ†å¸ƒæ˜¯å¾ˆæ¥è¿‘çš„å‘¢ï¼Ÿ</p><p> <strong>4</strong> <strong>KL è·ç¦»ï¼ŸJS è·ç¦»ï¼Ÿ</strong></p><p>è®©æˆ‘ä»¬æŠŠé—®é¢˜å†ç†æ¸…æ¥šä¸€ä¸‹ï¼š<strong>æˆ‘ä»¬ç°åœ¨æœ‰ä¸€æ‰¹æœä»æŸä¸ªæŒ‡å®šåˆ†å¸ƒçš„æ•°æ® Z=(z1,z2,â€¦,zN)ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªç¥ç»ç½‘ç»œ Y=G(X,Î¸)ï¼Œå°†å‡åŒ€éšæœºæ•° X æ˜ å°„åˆ°è¿™ä¸ªæŒ‡å®šåˆ†å¸ƒä¸­æ¥ã€‚</strong></p><p>éœ€è¦ç‰¹åˆ«æŒ‡å‡ºï¼Œæˆ‘ä»¬æ˜¯è¦<strong>æ¯”è¾ƒä¸¤ä¸ªåˆ†å¸ƒçš„æ¥è¿‘ç¨‹åº¦ï¼Œè€Œä¸æ˜¯æ¯”è¾ƒæ ·æœ¬ä¹‹é—´çš„å·®è·ã€‚</strong>é€šå¸¸æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šç”¨ KL è·ç¦»æ¥æè¿°ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ï¼šè®¾ p1(x),p2(x) æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦ï¼ˆå½“ç„¶ï¼Œè¿˜æœ‰å…¶ä»–è·ç¦»å¯ä»¥é€‰æ‹©ï¼Œæ¯”å¦‚ Wasserstein è·ç¦»ï¼Œä½†è¿™ä¸æ”¹å˜ä¸‹é¢è¦è®¨è®ºçš„å†…å®¹çš„å®è´¨ï¼‰ï¼Œé‚£ä¹ˆï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613394054.webp" alt="img" style="zoom:67%;" /></p><p>å¦‚æœæ˜¯ç¦»æ•£æ¦‚ç‡ï¼Œåˆ™å°†ç§¯åˆ†æ¢æˆæ±‚å’Œå³å¯ã€‚KL è·ç¦»å¹¶éçœŸæ­£çš„åº¦é‡è·ç¦»ï¼Œä½†æ˜¯å®ƒèƒ½å¤Ÿæè¿°ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå½“å®ƒæ˜¯ 0 æ—¶ï¼Œè¡¨æ˜ä¸¤ä¸ªåˆ†å¸ƒä¸€è‡´ã€‚ä½†å› ä¸ºå®ƒä¸æ˜¯å¯¹ç§°çš„ã€‚æœ‰æ—¶å€™å°†å®ƒå¯¹ç§°åŒ–ï¼Œå¾—åˆ° JS è·ç¦»ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613402498.webp" alt="img" style="zoom:67%;" /></p><p>å’¦ï¼Ÿæ€ä¹ˆåˆå›åˆ°æ¦‚ç‡å¯†åº¦äº†ï¼Ÿä¸æ˜¯è¯´æ²¡ç»™å‡ºæ¦‚ç‡å¯†åº¦å—ï¼Ÿæ²¡åŠæ³•ï¼Œå…¬å¼å°±æ˜¯è¿™æ ·ï¼Œåªå¥½ä¼°ç®—ä¸€ä¸‹å’¯ã€‚å‡è®¾æˆ‘ä»¬å¯ä»¥å°†å®æ•°åŸŸåˆ†è‹¥å¹²ä¸ªä¸ç›¸äº¤çš„åŒºé—´ I1,I2,â€¦,IKï¼Œé‚£ä¹ˆå°±å¯ä»¥ä¼°ç®—ä¸€ä¸‹ç»™å®šåˆ†å¸ƒ Z çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654039.webp" alt="img" style="zoom: 50%;" /></p><p>å…¶ä¸­ #(zjâˆˆIi) è¡¨ç¤ºå¦‚æœ zjâˆˆIiï¼Œé‚£ä¹ˆå–å€¼ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼Œä¹Ÿå°±æ˜¯è¯´å¤§å®¶ä¸è¦è¢«å…¬å¼å”¬ä½äº†ï¼Œä¸Šå¼å°±æ˜¯ä¸€ä¸ªç®€å•çš„è®¡æ•°å‡½æ•°ï¼Œç”¨é¢‘ç‡ä¼°è®¡æ¦‚ç‡ç½¢äº†ã€‚</p><p>æ¥ç€æˆ‘ä»¬ç”Ÿæˆ M ä¸ªå‡åŒ€éšæœºæ•° x1,x2,â€¦,xMï¼ˆè¿™é‡Œä¸ä¸€å®šè¦ M=Nï¼Œè¿˜æ˜¯é‚£å¥è¯ï¼Œæˆ‘ä»¬æ¯”è¾ƒçš„æ˜¯åˆ†å¸ƒï¼Œä¸æ˜¯æ ·æœ¬æœ¬èº«ï¼Œå› æ­¤å¤šä¸€ä¸ªå°‘ä¸€ä¸ªæ ·æœ¬ï¼Œå¯¹åˆ†å¸ƒçš„ä¼°ç®—ä¹Ÿå·®ä¸äº†å¤šå°‘ã€‚ï¼‰ï¼Œæ ¹æ® Y=G(X,Î¸) è®¡ç®—å¯¹åº”çš„ y1,y2,â€¦,yMï¼Œç„¶åæ ¹æ®å…¬å¼å¯ä»¥è®¡ç®—ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654040.webp" alt="img" style="zoom: 50%;" /></p><p>ç°åœ¨æœ‰äº† pz(Ii) å’Œ py(Ii)ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ç®—å®ƒä»¬çš„å·®è·äº†ï¼Œæ¯”å¦‚å¯ä»¥é€‰æ‹© JS è·ç¦»ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654040-1577180533050.webp" alt="img" style="zoom: 50%;" /></p><p>æ³¨æ„ yi æ˜¯ç”± G(X,Î¸) ç”Ÿæˆçš„ï¼Œæ‰€ä»¥ py(Ii) æ˜¯å¸¦æœ‰å‚æ•° Î¸ çš„ï¼Œå› æ­¤å¯ä»¥é€šè¿‡æœ€å°åŒ– Loss æ¥å¾—åˆ°å‚æ•° Î¸ çš„æœ€ä¼˜å€¼ï¼Œä»è€Œå†³å®šç½‘ç»œ Y=G(X,Î¸)ã€‚</p><p> <strong>5</strong> <strong>ç¥ç»è·ç¦»</strong></p><p>å‡å¦‚æˆ‘ä»¬åªç ”ç©¶å•å˜é‡æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å˜æ¢ï¼Œé‚£ä¸Šè¿°è¿‡ç¨‹å®Œå…¨å¤Ÿäº†ã€‚ç„¶è€Œï¼Œå¾ˆå¤šçœŸæ­£æœ‰æ„ä¹‰çš„äº‹æƒ…éƒ½æ˜¯å¤šå…ƒçš„ï¼Œæ¯”å¦‚åœ¨ MNIST ä¸Šåšå®éªŒï¼Œæƒ³è¦å°†éšæœºå™ªå£°å˜æ¢æˆæ‰‹å†™æ•°å­—å›¾åƒã€‚è¦æ³¨æ„ MNIST çš„å›¾åƒæ˜¯ 28*28=784 åƒç´ çš„ï¼Œå‡å¦‚æ¯ä¸ªåƒç´ éƒ½æ˜¯éšæœºçš„ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸€ä¸ª 784 å…ƒçš„æ¦‚ç‡åˆ†å¸ƒã€‚æŒ‰ç…§æˆ‘ä»¬å‰é¢åˆ†åŒºé—´æ¥è®¡ç®— KL è·ç¦»æˆ–è€… JS è·ç¦»ï¼Œå“ªæ€•æ¯ä¸ªåƒç´ åªåˆ†ä¸¤ä¸ªåŒºé—´ï¼Œé‚£ä¹ˆå°±æœ‰ 2784â‰ˆ10236 ä¸ªåŒºé—´ï¼Œè¿™æ˜¯ä½•å…¶å·¨å¤§çš„è®¡ç®—é‡ï¼</p><p>ç»ˆäºï¼Œæœ‰äººæ€’äº†ï¼š<strong>â€œè€å­å¹²å˜›è¦ç”¨ä½ é‚£é€—æ¯”çš„ JS è·ç¦»ï¼Œè€å­è‡ªå·±ç”¨ç¥ç»ç½‘ç»œé€ ä¸€ä¸ªè·ç¦»ï¼â€</strong>äºæ˜¯ä»–å†™å‡ºå¸¦å‚æ•° Î˜ çš„ç¥ç»ç½‘ç»œï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654040-1577180535246.webp" alt="img" style="zoom: 50%;" /></p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œç›´æ¥å°†é€ å‡ºæ¥çš„ yi å’ŒçœŸå®çš„ zi éƒ½æ”¾è¿›å»è¿™ä¸ªç¥ç»ç½‘ç»œä¸€ç®—ï¼Œè‡ªåŠ¨å‡ºæ¥è·ç¦»ï¼Œå¤šæ–¹ä¾¿ã€‚<strong>è¿™ä¸ªæ€æƒ³æ˜¯é‡Œç¨‹ç¢‘å¼çš„ï¼Œå®ƒè¿è·ç¦»çš„å®šä¹‰éƒ½ç›´æ¥ç”¨ç¥ç»ç½‘ç»œå­¦äº†ï¼Œè¿˜æœ‰ä»€ä¹ˆä¸å¯èƒ½å­¦çš„å‘¢ï¼Ÿ</strong></p><p>æˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œè¦æ˜¯çœŸæœ‰è¿™ä¹ˆä¸ª L å­˜åœ¨ï¼Œå®ƒåº”è¯¥æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿé¦–å…ˆï¼Œå¯¹äºç‰¹å®šçš„ä»»åŠ¡æ¥è¯´ï¼Œ<img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654359.webp" alt="img" style="zoom:50%;" />æ˜¯ç»™å®šçš„ï¼Œå› æ­¤å®ƒå¹¶éå˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå®ƒå½“åšæ¨¡å‹æœ¬èº«çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤ç®€å†™æˆï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654408.webp" alt="img" style="zoom: 50%;" /></p><p>æ¥ç€ï¼Œåˆ«å¿˜è®°æˆ‘ä»¬æ˜¯<strong>æè¿°åˆ†å¸ƒä¹‹é—´çš„è·ç¦»è€Œä¸æ˜¯æ ·æœ¬çš„è·ç¦»</strong>ï¼Œè€Œåˆ†å¸ƒæœ¬èº«è·Ÿå„ä¸ª yi å‡ºç°çš„é¡ºåºæ˜¯æ²¡æœ‰å…³ç³»çš„ï¼Œå› æ­¤åˆ†å¸ƒä¹‹é—´çš„è·ç¦»è·Ÿå„ä¸ª yi å‡ºç°çš„é¡ºåºæ˜¯æ— å…³çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå°½ç®¡ L æ˜¯å„ä¸ª yi çš„å‡½æ•°ï¼Œä½†å®ƒå¿…é¡»å…¨å¯¹ç§°çš„ï¼è¿™æ˜¯ä¸ªå¾ˆå¼ºçš„çº¦æŸï¼Œå½“ç„¶ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬çš„é€‰æ‹©ä¹Ÿæœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654451.webp" alt="img"></p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å…ˆæ‰¾ä¸€ä¸ªæœ‰åºçš„å‡½æ•° Dï¼Œç„¶åå¯¹æ‰€æœ‰å¯èƒ½çš„åºæ±‚å¹³å‡ï¼Œé‚£ä¹ˆå°±å¾—åˆ°æ— åºçš„å‡½æ•°äº†ã€‚å½“ç„¶ï¼Œè¿™æ ·çš„è®¡ç®—é‡æ˜¯ ğ’ª(M!)ï¼Œæ˜¾ç„¶ä¹Ÿä¸é è°±ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±é€‰æ‹©æœ€ç®€å•çš„ä¸€ç§ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654493.webp" alt="img" style="zoom:50%;" /></p><p>è¿™ä¾¿æ˜¯<strong>æ— åºçš„æœ€ç®€å•å®ç°</strong>ï¼Œå¯ä»¥ç®€å•çš„ç†è§£ä¸ºï¼šåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œç­‰äºå•ä¸ªæ ·æœ¬çš„è·ç¦»çš„å¹³å‡ã€‚</p><p> <strong>6</strong>  <strong>å¯¹æŠ—æ¥äº†</strong></p><p>â€œç­‰ç­‰ï¼Œä½ çš„æ ‡é¢˜æ˜¯ GANï¼Œä½ è®²äº†é‚£ä¹ˆä¸€å¤§é€šï¼Œæˆ‘æ€ä¹ˆæ²¡æ„Ÿè§‰åˆ°åŠç‚¹ GAN çš„å‘³é“å‘€ï¼Ÿå¯¹æŠ—åœ¨å“ªé‡Œï¼Ÿâ€ è¿™ä½çœ‹å®˜æ‚¨åˆ«æ€¥ï¼Œé©¬ä¸Šå°±æœ‰äº†ã€‚</p><p>å‰é¢è¯´åˆ°ï¼Œç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ ä¸€ä¸ªè·ç¦» Lï¼Œæœ€ç»ˆç®€åŒ–ç‰ˆçš„å½¢å¼ï¼Œåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654503.webp" alt="img" style="zoom:50%;" /></p><p>é—®é¢˜æ˜¯ï¼šD(Y,Î˜) æ€ä¹ˆè®­ç»ƒï¼Ÿåˆ«å¿˜äº†ï¼Œä¹‹å‰çš„ G(X,Î¸) è¿˜æ²¡æœ‰è®­ç»ƒå¥½ï¼Œç°åœ¨åˆå¼„ä¸ª D(Y,Î˜) å‡ºæ¥ï¼Œè¶Šæè¶Šå¤æ‚ï¼Œå°å¿ƒè·³åˆ°å‘é‡Œå‡ºä¸æ¥äº†ã€‚</p><p>å¯¹æŠ—ç»ˆäºæ¥äº†â€¦</p><p><strong>å› ä¸º D(Y,Î˜) çš„å‡å€¼ï¼Œä¹Ÿå°±æ˜¯ Lï¼Œæ˜¯åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦ï¼Œè¿™å°±æ„å‘³ç€ï¼ŒL è¦èƒ½å¤Ÿå°†ä¸¤ä¸ªåˆ†å¸ƒåŒºåˆ†å¼€æ¥ï¼Œå³ L è¶Šå¤§è¶Šå¥½ï¼›ä½†æ˜¯æˆ‘ä»¬æœ€ç»ˆçš„ç›®çš„ï¼Œæ˜¯å¸Œæœ›é€šè¿‡å‡åŒ€åˆ†å¸ƒè€Œç”Ÿæˆæˆ‘ä»¬æŒ‡å®šçš„åˆ†å¸ƒï¼Œæ‰€ä»¥ G(X,Î¸) åˆ™å¸Œæœ›ä¸¤ä¸ªåˆ†å¸ƒè¶Šæ¥è¶Šæ¥è¿‘ï¼Œå³ L è¶Šå°è¶Šå¥½ã€‚è¿™æ—¶å€™ï¼Œä¸€ä¸ªå¤©æ‰çš„æƒ³æ³•å‡ºç°äº†ï¼šäº’æ€¼ï¼ä¸è¦æ€‚ï¼Œganï¼</strong></p><p>é¦–å…ˆæˆ‘ä»¬éšæœºåˆå§‹åŒ– $G(X,Î¸)$ï¼Œå›ºå®šå®ƒï¼Œç„¶åç”Ÿæˆä¸€æ‰¹ Yï¼Œè¿™æ—¶å€™æˆ‘ä»¬è¦è®­ç»ƒ $D(Y,Î˜)$ï¼Œæ—¢ç„¶ L ä»£è¡¨çš„æ˜¯â€œä¸æŒ‡å®šæ ·æœ¬ Z çš„å·®å¼‚â€ï¼Œé‚£ä¹ˆï¼Œå¦‚æœå°†æŒ‡å®šæ ·æœ¬ Z ä»£å…¥ Lï¼Œç»“æœåº”è¯¥æ˜¯è¶Šå°è¶Šå¥½ï¼Œè€Œå°† Y ä»£å…¥ Lï¼Œç»“æœåº”è¯¥æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œæ‰€ä»¥ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654537.webp" alt="img" style="zoom:50%;" /></p><p>ç„¶è€Œæœ‰ä¸¤ä¸ªç›®æ ‡å¹¶ä¸å®¹æ˜“å¹³è¡¡ï¼Œæ‰€ä»¥å¹²è„†éƒ½å–åŒæ ·çš„æ ·æœ¬æ•° Bï¼ˆä¸€ä¸ª batchï¼‰ï¼Œç„¶åä¸€èµ·è®­ç»ƒå°±å¥½ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654540.webp" alt="img" style="zoom:50%;" /></p><p>å¾ˆè‡ªç„¶ï¼ŒG(X,Î¸) å¸Œæœ›å®ƒç”Ÿæˆçš„æ ·æœ¬è¶Šæ¥è¿‘çœŸå®æ ·æœ¬è¶Šå¥½ï¼Œå› æ­¤è¿™æ—¶å€™æŠŠ Î˜ å›ºå®šï¼Œåªè®­ç»ƒ Î¸ è®© L è¶Šæ¥è¶Šå°ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654579.webp" alt="img" style="zoom:50%;" /></p><p><strong>è¿™å°±æ˜¯å¤©æ‰çš„å¯¹æŠ—ç½‘ç»œï¼</strong></p><p>éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼š</p><ol><li>è¿™é‡Œçš„ Loss å†™æ³•è·Ÿä¼ ç»Ÿçš„ GAN ç›¸åï¼Œä¹ æƒ¯æ€§çš„åšæ³•æ˜¯è®©çœŸå®æ ·æœ¬çš„LLè¶Šå¤§è¶Šå¥½ï¼Œä½†è¿™åªä¸è¿‡è·Ÿæœ¬æ–‡å·®äº†ä¸ªè´Ÿå·è€Œå·²ï¼Œä¸æ˜¯æœ¬è´¨çš„é—®é¢˜ï¼›</li></ol><ol><li>ä» GAN å¼€å§‹ï¼ŒD è¿™ä¸ªç¥ç»ç½‘ç»œå°±è¢«èµ‹äºˆäº†â€œåˆ¤åˆ«å™¨â€çš„æ„ä¹‰ï¼Œä½†åœ¨è¿™é‡Œ D æœ¬èº«æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼ˆæ­£å¦‚æˆ‘ä»¬ä¸èƒ½è¯´æŸä¸ªæ•°æ˜¯ä¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼‰ï¼Œåªæœ‰ D çš„å¹³å‡å€¼ L æ‰ä»£è¡¨ç€ä¸çœŸå®åˆ†å¸ƒçš„å·®è·ï¼ˆæˆ‘ä»¬åªèƒ½æ ¹æ®ä¸€æ‰¹æ•°æ®æ¥ä¼°è®¡å®ƒæ˜¯å¦æœä»æ­£æ€åˆ†å¸ƒï¼‰ï¼Œæ‰€ä»¥ä»è¿™é‡Œä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒGAN ä¸èƒ½å•ä¸ªæ ·æœ¬åœ°è®­ç»ƒï¼Œè‡³å°‘æˆæ‰¹è®­ç»ƒï¼Œå› ä¸ºæœ‰ä¸€æ‰¹æ ·æœ¬æ‰èƒ½çœ‹å‡ºç»Ÿè®¡ç‰¹å¾ï¼›</li></ol><ol><li>å’‹çœ‹ä¸Šå» D åªæ˜¯ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œè€Œ G åˆ™è¦æŠŠå™ªå£°æ˜ å°„ä¸ºæ­£æ ·æœ¬ï¼Œè²Œä¼¼ D åº”è¯¥æ¯” G è¦ç®€å•å¾—å¤šï¼Ÿäº‹å®å¹¶éå¦‚æ­¤ï¼Œå®ƒä»¬ä¸¤è€…çš„å¤æ‚åº¦è‡³å°‘æ˜¯ç›¸å½“çš„ã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚è€ƒè™‘ä¸€ä¸‹å®ƒä»¬çš„å·¥ä½œåŸç†ï¼šå› ä¸º D çš„å‡å€¼ L ç›´æ¥å°±ç»™å‡ºäº†è¾“å…¥çš„æ•°æ®ä¸æŒ‡å®šåˆ†å¸ƒçš„å·®å¼‚ï¼Œè€Œè¦çœŸçš„åšåˆ°è¿™ä¸€ç‚¹ï¼Œé‚£ä¹ˆ D è¦æŠŠæ‰€æœ‰çš„â€œæ­£æ ·æœ¬â€ï¼ˆåœ¨æŸç§ç¨‹åº¦ä¸Šï¼‰éƒ½â€œè®°ä½â€äº†æ‰è¡Œï¼›è€Œ G è¦ç”Ÿæˆè‰¯å¥½çš„æ­£æ ·æœ¬ï¼ŒåŸºæœ¬ä¸Šä¹Ÿæ˜¯â€œè®°ä½â€äº†æ‰€æœ‰çš„æ­£æ ·æœ¬ï¼Œå¹¶é€šè¿‡éšæœºæ•°æ¥æ’å€¼è¾“å‡ºã€‚å› æ­¤ä¸¤ä¸ªç½‘ç»œçš„å¤æ‚åº¦åº”è¯¥æ˜¯ç›¸å½“çš„ï¼ˆå½“ç„¶è¿™é‡Œçš„â€œè®°ä½â€æ˜¯å½¢è±¡ç†è§£ï¼Œä¸æ˜¯çœŸçš„å¼ºè¡Œè®°ä½äº†ï¼Œä¸ç„¶å°±æ˜¯è¿‡æ‹Ÿåˆäº†ï¼‰ï¼›</li></ol><ol><li>æ—¢ç„¶ L1 æ˜¯çœŸä¼ªæ ·æœ¬çš„åˆ†å¸ƒå·®ï¼Œé‚£ä¹ˆ L1 è¶Šå¤§ï¼Œæ„å‘³ç€â€œä¼ªé€ â€çš„æ ·æœ¬è´¨é‡è¶Šå¥½ï¼Œæ‰€ä»¥ L1 åŒæ—¶ä¹ŸæŒ‡ç¤ºç€ GAN è®­ç»ƒçš„è¿›ç¨‹ï¼ŒL1 è¶Šå¤§ï¼Œè®­ç»ƒå¾—è¶Šå¥½ã€‚ï¼ˆD å¸Œæœ› L1 è¶Šå°è¶Šå¥½ï¼ŒG å¸Œæœ› L1 è¶Šå¤§è¶Šå¥½ï¼Œå½“ç„¶æ˜¯ G å¸Œæœ›çš„ç»“æœï¼Œæ‰æ˜¯æˆ‘ä»¬å¸Œæœ›çš„ã€‚å…¶å®ä¹Ÿå¯ä»¥è¿™æ ·ç†è§£ï¼ŒG çš„æŸå¤± L2ï¼Œå…¶å®å°±ç›¸å½“äº âˆ’L1ï¼Œä½†æ˜¯å› ä¸º D çš„æƒé‡å·²ç»å›ºå®šäº†ï¼Œæ‰€ä»¥æœ‰å…³çœŸå®æ ·æœ¬é‚£ä¸€é¡¹æ˜¯ä¸ªå¸¸æ•°ï¼Œå› æ­¤åªå‰©ä¸‹ä¼ªé€ æ ·æœ¬é‚£ä¸€é¡¹ï¼Œå³ L2ï¼Œä½† L2 æ˜¯ä¸ªç»å¯¹å€¼ï¼Œæˆ‘ä»¬å…³å¿ƒçš„æ˜¯ç›¸å¯¹å€¼ï¼Œæ‰€ä»¥ âˆ’L1 æ˜¯æˆ‘ä»¬å…³å¿ƒçš„ï¼Œå®ƒè¶Šå°è¶Šå¥½ï¼Œç›¸å½“äº L1 è¶Šå¤§è¶Šå¥½ã€‚ï¼‰</li></ol><p> <strong>7</strong> <strong>åˆ«èµ°ï¼Œè¿˜æ²¡å®Œ</strong></p><p>ç¨å¾®æ€è€ƒä¸€ä¸‹ï¼Œæˆ‘ä»¬å°±å‘ç°ï¼Œé—®é¢˜è¿˜æ²¡å®Œã€‚æˆ‘ä»¬ç›®å‰è¿˜æ²¡æœ‰å¯¹ D åšçº¦æŸï¼Œä¸éš¾å‘ç°ï¼Œæ— çº¦æŸçš„è¯ Loss åŸºæœ¬ä¸Šä¼šç›´æ¥è·‘åˆ°è´Ÿæ— ç©·å»äº†ã€‚</p><p>å› æ­¤ï¼Œæœ‰å¿…è¦ç»™ D åŠ ç‚¹æ¡ä»¶ï¼Œä¸€ä¸ªæ¯”è¾ƒå®¹æ˜“æƒ³åˆ°çš„æ–¹æ¡ˆæ˜¯çº¦æŸ D çš„èŒƒå›´ï¼Œæ¯”å¦‚èƒ½ä¸èƒ½ç»™ D æœ€åçš„è¾“å‡ºåŠ ä¸ª Sigmoid æ¿€æ´»å‡½æ•°ï¼Œè®©å®ƒå–å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Ÿäº‹å®ä¸Šè¿™ä¸ªæ–¹æ¡ˆåœ¨ç†è®ºä¸Šæ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œç„¶è€Œè¿™ä¼šé€ æˆè®­ç»ƒçš„å›°éš¾ã€‚å› ä¸º Sigmoid å‡½æ•°å…·æœ‰é¥±å’ŒåŒºï¼Œä¸€æ—¦ D è¿›å…¥äº†é¥±å’ŒåŒºï¼Œå°±å¾ˆéš¾ä¼ å›æ¢¯åº¦æ¥æ›´æ–° G äº†ã€‚</p><p>æœ€å¥½åŠ ä»€ä¹ˆçº¦æŸå‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥å°½å¯èƒ½ä»åŸºæœ¬åŸç†å‡ºå‘æ¥æ‰¾å¯»çº¦æŸï¼Œå°½é‡é¿å…åŠ å…¥äººå·¥å› ç´ ã€‚æˆ‘ä»¬å›åˆ°è·ç¦»çš„ä½œç”¨ä¸Šæ¥çœ‹ï¼šè·ç¦»æ˜¯ä¸ºäº†è¡¨æ˜ä¸¤ä¸ªå¯¹è±¡çš„å·®è·ï¼Œè€Œå¦‚æœå¯¹è±¡äº§ç”Ÿçš„å¾®å°çš„å˜åŒ–ï¼Œé‚£ä¹ˆè·ç¦»çš„æ³¢åŠ¨ä¹Ÿä¸èƒ½å¤ªå¤§ï¼Œè¿™åº”è¯¥æ˜¯å¯¹è·ç¦»åŸºæœ¬çš„ç¨³å®šæ€§è¦æ±‚ï¼Œâ€œå¤±ä¹‹æ¯«å˜ï¼Œè°¬ä»¥åƒé‡Œâ€æ˜¯ä¼šäº§ç”Ÿæµ‘æ²Œçš„ï¼Œæ•°å­¦æ¨¡å‹ä¸åº”è¯¥æ˜¯è¿™æ ·ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œé‚£ä¸ªæ‰€è°“çš„â€œJS è·ç¦»â€ï¼Œæ ¹æ®å°±ä¸æ˜¯è·ç¦»äº†ï¼Œå› ä¸ºå°±ç®—å¯¹äºä¼¯åŠªåˆ©åˆ†å¸ƒ {0:0.1,1:0.9} å’Œ {0:0,1:1}ï¼Œè¿™ä¸¤ä¸ªç›¸ä¼¼çš„åˆ†å¸ƒç®—å‡ºæ¥çš„â€œè·ç¦»â€å±…ç„¶æ˜¯æ— ç©·å¤§ï¼ˆå› ä¸ºå‡ºç°äº† 0.1/0 è¿™ä¸€é¡¹ï¼‰ã€‚</p><p>æ”¾åˆ°æˆ‘ä»¬çš„ D ä¸­ï¼Œè¿™ä¸ªçº¦æŸæˆ‘ä»¬è¯¥æ€ä¹ˆä½“ç°å‘¢ï¼Ÿå‡å¦‚æŸä¸ªæ ·æœ¬ä¸æ˜¯ yi è€Œæ˜¯ yâ€²iï¼Œå‡è®¾ â€–yiâˆ’yâ€²iâ€–ï¼ˆç”¨ä¸¤ç«–è¡¨ç¤ºæ¬§å¼è·ç¦»ï¼Œå› ä¸º y å¯èƒ½æ˜¯ä¸ªå¤šå…ƒå‘é‡ï¼‰å¹¶ä¸æ˜¯ååˆ†å¤§ï¼Œé‚£ä¹ˆä¼šå¯¹åˆ†å¸ƒé€ æˆä¸€å®šçš„å½±å“ã€‚è¿™ä¸ªå½±å“æœ‰å¤šå¤§å‘¢ï¼Ÿæ˜¾ç„¶ä¸ä¼šå¤§ï¼Œå› ä¸ºåˆ†å¸ƒæ˜¯ä¸€æ‰¹æ ·æœ¬çš„ç»Ÿè®¡ç‰¹å¾ï¼Œå¦‚æœåªæ˜¯ç¨å¾®æ”¹å˜äº†ä¸€ä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆåˆ†å¸ƒçš„å˜åŒ–æ˜¾ç„¶ä¸èƒ½å¤§çš„ã€‚è€Œæˆ‘ä»¬çŸ¥é“ï¼Œåˆ†å¸ƒçš„è·ç¦»ç”¨ D çš„å‡å€¼ L æ¥æè¿°ï¼Œåªæ”¹å˜ä¸€ä¸ª yiï¼Œæ‰€é€ æˆçš„åˆ†å¸ƒå·®æ­£æ¯”äºï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654621.webp" alt="img" style="zoom:50%;" /></p><p>æˆ‘ä»¬å¸Œæœ› yiâ€²â†’yi æ—¶ï¼Œè‡ªç„¶åœ°å°±æœ‰<img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654745.webp" alt="img" style="zoom:67%;" />ï¼Œæ€ä¹ˆå®ç°è¿™ä¸€ç‚¹å‘¢ï¼Ÿä¸€ä¸ªç®€å•çš„æ–¹æ¡ˆæ˜¯ D æ»¡è¶³ä»¥ä¸‹çº¦æŸï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654647.webp" alt="img" style="zoom:50%;" /></p><p>è¿™é‡Œ $Î±&gt;0$ï¼Œè€Œæœ€ç®€å•çš„æ–¹æ¡ˆå°±æ˜¯ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654662.webp" alt="img" style="zoom:50%;" /></p><p>è¿™å°±æ˜¯æ•°å­¦ä¸­å¸¸è§çš„ <strong>Lipschitz çº¦æŸ</strong>ã€‚å¦‚æœèƒ½å¤Ÿæ»¡è¶³è¿™ä¸ªçº¦æŸï¼Œé‚£ä¹ˆè·ç¦»å°±èƒ½æ»¡è¶³ç¨³å®šæ€§è¦æ±‚ã€‚æ³¨æ„è¿™æ˜¯ä¸ªå……åˆ†æ¡ä»¶ï¼Œä¸æ˜¯å¿…è¦æ¡ä»¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ¡ˆã€‚ä½†ä¸å¾—ä¸è¯´ï¼Œè¿™æ˜¯ä¸ªç®€å•æ˜äº†çš„æ–¹æ¡ˆã€‚è€Œä½¿å¾—å‡½æ•° D æ»¡è¶³ Lipschitz çº¦æŸçš„ä¸€ä¸ªå……åˆ†æ¡ä»¶å°±æ˜¯ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654697.webp" alt="img" style="zoom:50%;" /></p><p> <strong>8</strong> <strong>â€œç½šâ€å‡ºæ¥çš„æˆæœ</strong></p><p>æ€ä¹ˆæŠŠè¿™ä¸ªçº¦æŸåŠ å…¥åˆ°æ¨¡å‹ä¸­å»å‘¢ï¼ŸåŠ å…¥ä¸ªæƒ©ç½šé¡¹å°±å¥½ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654741.webp" alt="img" style="zoom: 67%;" /></p><p>å½“ç„¶æƒ©ç½šæ˜¯â€œè½¯çº¦æŸâ€ï¼Œæœ€ç»ˆçš„ç»“æœä¸ä¸€å®šæ»¡è¶³è¿™ä¸ªçº¦æŸï¼Œä½†å´ä¼šåœ¨çº¦æŸä¸Šä¸‹æ³¢åŠ¨ã€‚ä¹Ÿå°±æ˜¯è¯´è™½ç„¶æˆ‘ä»¬æŒ‡å®šäº† C=1ï¼Œä½†æœ€ç»ˆçš„ C å´ä¸ä¸€å®šç­‰äº 1ï¼Œä¸è¿‡ä¼šåœ¨ 1 ä¸Šä¸‹æ³¢åŠ¨ï¼Œè€Œè¿™ä¹Ÿä¸è¿‡æ˜¯ä¸€ä¸ªæ›´å®½æ¾çš„ Lipschitz çº¦æŸè€Œå·²ï¼Œæˆ‘ä»¬ä¸åœ¨ä¹ C çš„å…·ä½“å¤§å°ï¼Œåªè¦ C æœ‰ä¸Šç•Œå°±å¥½ã€‚å¦å¤–ï¼Œçº¦æŸçš„åŠ æ³•ä¸æ˜¯å”¯ä¸€çš„ï¼ŒWGAN çš„ä½œè€… Martin Arjovsky åœ¨ä»–çš„è®ºæ–‡ä¸­æå‡ºçš„åŠ æ³•ä¸ºï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654782.webp" alt="img" style="zoom:67%;" /></p><p>å“ªä¸ªå¥½ï¼Ÿå®éªŒç»“æœå¥½åƒéƒ½å·®ä¸å¤šã€‚</p><p>ä¸è¿‡ï¼Œä¸Šé¢çš„æƒ©ç½šé¡¹éƒ½æ˜¯å½¢å¼è€Œå·²ï¼Œæˆ‘ä»¬è¿˜æ²¡ç»™å‡ºå…·ä½“çš„è®¡ç®—æ–¹æ³•ã€‚ç†è®ºä¸Šæœ€å¥½èƒ½å¤Ÿå¯¹æ‰€æœ‰çš„ yï¼ˆå…¨ç©ºé—´ï¼‰éƒ½ç®—ä¸€é <img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654793.webp" alt="img" style="zoom:67%;" /> ç„¶åå–å¹³å‡ï¼Œæ˜¾ç„¶è¿™æ˜¯åšä¸åˆ°çš„ã€‚é‚£ä¹ˆåªå¥½ç”¨ä¸€ä¸ªé€€è€Œæ±‚å…¶æ¬¡çš„æ–¹æ¡ˆï¼šåªå¯¹çœŸå®æ ·æœ¬ zi å’Œç”Ÿæˆæ ·æœ¬ yi ç®—ã€‚ä½†è¿™æ ·çº¦æŸèŒƒå›´è²Œä¼¼ä¹Ÿå¤ªå°äº†ï¼Œæ‰€ä»¥å¹²è„†åœ¨çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ä¹‹é—´éšæœºæ’å€¼ï¼Œå¸Œæœ›è¿™ä¸ªçº¦æŸå¯ä»¥â€œå¸ƒæ»¡â€çœŸå®æ ·æœ¬å’Œç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„ç©ºé—´ï¼Œå³ï¼š</p><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="img"></p><p>ä»¥åŠï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654814.webp" alt="img" style="zoom:67%;" /></p><p>è¿™é‡Œçš„ Îµi æ˜¯ U[0,1] çš„éšæœºæ•°ï¼Œè¿™åº”è¯¥å·²ç»æ˜¯æˆ‘ä»¬â€œåŠ›æ‰€èƒ½åŠâ€çš„æœ€ä¼˜çš„æ–¹æ¡ˆäº†ã€‚åé¢è¿™ä¸ªå°±æ˜¯ Martin Arjovsky æå‡ºçš„æœ€æ–°çš„ Lipschitz çº¦æŸçš„æ–¹æ¡ˆï¼Œè€Œå®éªŒç»“æœè¡¨æ˜å‰ä¸€ä¸ªæ–¹æ¡ˆæ•ˆæœä¹Ÿä¸é”™ã€‚ç›®å‰å®ƒä»¬çš„å¤§åå«â€œWGAN-GPâ€ï¼Œå…¨ç§° Wasserstein Generative Adversarial Nets - Gradient Penaltyã€‚</p><p>æœ€åï¼Œæœ‰äººä¼šåé©³ï¼Œæ¢¯åº¦æœ‰ä¸Šç•Œï¼Œåªä¸è¿‡æ˜¯ Lipschitz çº¦æŸçš„å……åˆ†æ¡ä»¶ï¼Œ<strong>ä¸ºå•¥ä¸ç›´æ¥å°† Lipschitz çº¦æŸä»¥å·®åˆ†å½¢å¼åŠ å…¥åˆ°æƒ©ç½šä¸­å»å‘¢ï¼Ÿ</strong>ï¼ˆå…¶å®æœ‰è¿™ä¸ªç–‘é—®çš„æœ€ä¸»è¦çš„åŸå› ï¼Œæ˜¯å¾ˆå¤šæ·±åº¦å­¦ä¹ æ¡†æ¶å¹¶æ²¡æœ‰æä¾›æ¢¯åº¦å‡½æ•°ï¼›å¦å¤–ï¼Œå°½ç®¡ tensorflow æä¾›äº†æ¢¯åº¦å‡½æ•°ï¼Œä½†å¦‚æœåˆ¤åˆ«å™¨ç”¨çš„æ˜¯ RNNï¼Œé‚£ä¹ˆæ¢¯åº¦å‡½æ•°ä¹Ÿæ˜¯ä¸å¯ç”¨çš„ã€‚ï¼‰äº‹å®ä¸Šï¼Œè¿™æ ·åšæŸç§æ„ä¹‰ä¸Šæ›´åŠ åˆç†ï¼Œæˆ‘è§‰å¾— Martin Arjovsky ç›´æ¥ç”¨æ¢¯åº¦ï¼Œä¸è¿‡æ˜¯æƒ³å†™å¾—ç®€å•ä¸€ç‚¹ï¼Œè¿™æ—¶å€™æƒ©ç½šæ˜¯ï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654845.webp" alt="img" style="zoom: 80%;" /></p><p>ä»¥åŠï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654847.webp" alt="img"></p><p>è¿™é‡Œ yi,j=Îµi,jyi+(1âˆ’Îµi,j)ziï¼Œä¹Ÿå°±æ˜¯æ¯æ­¥æ’å€¼ä¸¤æ¬¡ï¼Œç„¶åç”¨æ’å€¼çš„ç»“æœç®—å·®åˆ†ã€‚</p><p> <strong>9</strong> <strong>ç„¶åå‘¢ï¼Ÿ</strong></p><p>æš‚æ—¶æ²¡æœ‰ç„¶åäº†ï¼Œç»ˆäºå†™å®Œäº†ã€‚è¿™ä¾¿æ˜¯æˆ‘ç†è§£çš„ GANã€‚ </p><p>é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ°”å‘µæˆåœ°ç›´è¾¾ WGAN-GPï¼Œè€Œä¸éœ€è¦å¾ˆå¤šçš„å†å²çŸ¥è¯†å’Œæ•°å­¦çŸ¥è¯†ã€‚æœ‰è¶£çš„æ˜¯ï¼Œ<strong>æˆ‘ä»¬çš„æ¨å¯¼è¿‡ç¨‹è¡¨æ˜ï¼ŒWGAN-GP å…¶å®è·Ÿ Wasserstein è·ç¦»æ²¡æœ‰ç›´æ¥çš„è”ç³»</strong>ï¼Œå°½ç®¡å½“åˆ WGAN çš„ä½œè€…æ˜¯ä» Wasserstein è·ç¦»å°†å®ƒä»¬æ¨å¯¼å‡ºæ¥çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒWGAN è·Ÿ W æ²¡å•¥å…³ç³»ï¼Œè¿™å°±å°´å°¬äº†ã€‚å¦å¤–ï¼Œæœ‰äººæé—®â€œWGAN ç›¸æ¯”åŸå§‹çš„ GAN æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿâ€ï¼Œå¦‚æœæ ¹æ®æœ¬æ–‡çš„ç†è®ºæ¨å¯¼ï¼Œé‚£ä¹ˆåŸå§‹çš„ GAN æ ¹æœ¬å°±ä¸æ˜¯ GANï¼Œå› ä¸ºå®ƒä¸èƒ½æ”¹å†™ä¸ºæœ¬æ–‡çš„æŸä¸ªç‰¹ä¾‹ã€‚ï¼ˆåŸå› åœ¨äºï¼Œæœ¬æ–‡çš„æ¨å¯¼åŸºäºåˆ†å¸ƒçš„æ‹Ÿåˆï¼Œè€ŒåŸå§‹ GAN çš„æ¨å¯¼åŸºäºåšå¼ˆè®ºï¼Œå‡ºå‘ç‚¹ä¸åŒã€‚ï¼‰ </p><p>è¿™ä¸ª Loss è¿˜æœ‰ä¸€å®šçš„æ”¹è¿›ç©ºé—´ï¼Œæ¯”å¦‚ Loss Sensitive GANï¼ˆLS-GANï¼‰ï¼Œè¿˜æœ‰æ›´å¹¿ä¹‰çš„ CLS-GANï¼ˆå°† LS-GAN å’Œ WGAN ç»Ÿä¸€èµ·æ¥äº†ï¼‰ï¼Œè¿™äº›æ¨å¹¿æˆ‘ä»¬å°±ä¸è®¨è®ºäº†ã€‚ä¸è¿‡è¿™äº›æ¨å¹¿éƒ½å»ºç«‹åœ¨ Lipschitz çº¦æŸä¹‹ä¸Šï¼Œåªä¸è¿‡å¾®è°ƒäº† Lossï¼Œä¹Ÿè®¸æœªæ¥ä¼šæœ‰äººå‘ç°æ¯” Lipschitz çº¦æŸæ›´å¥½çš„å¯¹ D çš„çº¦æŸã€‚</p><p> <strong>10</strong>  <strong>WGAN-GP çš„ä¾‹å­</strong></p><p>æœ€åï¼Œåˆ†äº«ä¸€ä¸ª WGAN-GP çš„å®ç°ï¼Œä»¥ MNISTä¸º æ•°æ®é›†ï¼Œè¯»è€…å¯ä»¥è‡ªå·±æ”¹ç€ç©ï¼š</p><p><em><a href="https://github.com/bojone/gan/" target="_blank" rel="noopener">https://github.com/bojone/gan/</a></em></p><p>è®­ç»ƒè¿›åº¦æ˜¾ç¤ºï¼š</p><p><img src="/images/001-%E4%BA%92%E6%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-%E4%BB%8E%E9%9B%B6%E7%9B%B4%E8%BE%BE%20WGAN-GP/640-1572613654894.webp" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="/001-li-jie-lstm/"/>
      <url>/001-li-jie-lstm/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒèµ„æ–™ï¼š<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><hr><h2 id="å¾ªç¯ç¥ç»ç½‘ç»œ"><a href="#å¾ªç¯ç¥ç»ç½‘ç»œ" class="headerlink" title="å¾ªç¯ç¥ç»ç½‘ç»œ"></a>å¾ªç¯ç¥ç»ç½‘ç»œ</h2><p>Humans donâ€™t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You donâ€™t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p><p>Traditional neural networks canâ€™t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. Itâ€™s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p><p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/RNN-rolled.png" alt="img" style="zoom:36%;" /></p><p>â€‹                                <strong>Recurrent Neural Networks have loops.</strong></p><p>In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.</p><p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they arenâ€™t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/RNN-unrolled.png" alt="An unrolled recurrent neural network."></p><p>â€‹                        <strong>An unrolled recurrent neural network.</strong></p><p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. Theyâ€™re the natural architecture of neural network to use for such data.</p><p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioningâ€¦ The list goes on. Iâ€™ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathyâ€™s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.</p><p>Essential to these successes is the use of â€œLSTMs,â€ a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. Itâ€™s these LSTMs that this essay will explore.</p><h2 id="RNNçš„é•¿ç¨‹ä¾èµ–é—®é¢˜"><a href="#RNNçš„é•¿ç¨‹ä¾èµ–é—®é¢˜" class="headerlink" title="RNNçš„é•¿ç¨‹ä¾èµ–é—®é¢˜"></a>RNNçš„é•¿ç¨‹ä¾èµ–é—®é¢˜</h2><p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, theyâ€™d be extremely useful. But can they? It depends.</p><p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in â€œthe clouds are in the <em>sky</em>,â€ we donâ€™t need any further context â€“ itâ€™s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that itâ€™s needed is small, RNNs can learn to use the past information.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/RNN-shorttermdepdencies.png" alt="img" style="zoom:36%;" /></p><p>But there are also cases where we need more context. Consider trying to predict the last word in the text â€œI grew up in Franceâ€¦ I speak fluent <em>French</em>.â€ Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. Itâ€™s entirely possible for the gap between the relevant information and the point where it is needed to become very large.</p><p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies." style="zoom:36%;" /></p><p>In theory, RNNs are absolutely capable of handling such â€œlong-term dependencies.â€ A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs donâ€™t seem to be able to learn them. The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p><p>Thankfully, LSTMs donâ€™t have this problem!</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long Short Term Memory networks â€“ usually just called â€œLSTMsâ€ â€“ are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1" target="_blank" rel="noopener">1</a> They work tremendously well on a large variety of problems, and are now widely used.</p><p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-SimpleRNN.png" alt="img" style="zoom:36%;" /></p><p>â€‹    <strong>The repeating module in a standard RNN contains a single layer.</strong></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-chain.png" alt="A LSTM neural network." style="zoom:36%;" /></p><p> <strong>The repeating module in an LSTM contains four interacting layers.</strong></p><p>Donâ€™t worry about the details of whatâ€™s going on. Weâ€™ll walk through the LSTM diagram step by step later. For now, letâ€™s just try to get comfortable with the notation weâ€™ll be using.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM2-notation.png" alt="img"></p><p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p><h2 id="LSTMèƒŒåçš„æ ¸å¿ƒæ€æƒ³"><a href="#LSTMèƒŒåçš„æ ¸å¿ƒæ€æƒ³" class="headerlink" title="LSTMèƒŒåçš„æ ¸å¿ƒæ€æƒ³"></a>LSTMèƒŒåçš„æ ¸å¿ƒæ€æƒ³</h2><p>The key to LSTMs is the <strong>cell state</strong>, the horizontal line running through the top of the diagram.</p><p>The cell state is kind of like a conveyor belt(ä¼ è¾“å¸¦). It runs straight down the entire chain, with only some minor linear interactions. Itâ€™s very easy for information to just flow along it unchanged.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-C-line.png" alt="img" style="zoom:36%;" /></p><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated(æ§åˆ¶ï¼Œç®¡ç†) by structures called <strong>gates</strong>.</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-gate.png" alt="img" style="zoom:50%;" /></p><p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means â€œlet nothing through,â€ while a value of one means â€œlet everything through!â€</p><p>An LSTM has three of these gates, to protect and control the <strong>cell state</strong>.</p><h2 id="é€æ­¥ç†è§£LSTM"><a href="#é€æ­¥ç†è§£LSTM" class="headerlink" title="é€æ­¥ç†è§£LSTM"></a>é€æ­¥ç†è§£LSTM</h2><p>The first step in our LSTM is to decide what information weâ€™re going to throw away from the cell state. This decision is made by a sigmoid layer called the â€œforget gate layer.â€ It looks at $h_{tâˆ’1}$ and $x_t$, and outputs a number between $0$ and $1$ for each number in the cell state $C_{tâˆ’1}$. A $1$ represents â€œcompletely keep thisâ€ while a $0$ represents â€œcompletely get rid of this.â€</p><p>Letâ€™s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-focus-f.png" alt="img"></p><p>The next step is to decide what new information weâ€™re going to store in the cell state. This has two parts. First, a <code>sigmoid</code> layer called the â€œinput gate layerâ€ decides which values weâ€™ll update. Next, a <code>tanh</code> layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state. In the next step, weâ€™ll combine these two to create an update to the state.</p><p>In the example of our language model, weâ€™d want to add the gender of the new subject to the cell state, to replace the old one weâ€™re forgetting.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-focus-i.png" alt="img"></p><p>Itâ€™s now time to update the old cell state, $C_{tâˆ’1}$, into the new cell state $C_t$. The previous steps already decided what to do, we just need to actually do it.</p><p>We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t*\tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.</p><p>In the case of the language model, this is where weâ€™d actually drop the information about the old subjectâ€™s gender and add the new information, as we decided in the previous steps.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-focus-C.png" alt="img"></p><p>Finally, we need to decide what weâ€™re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state weâ€™re going to output. Then, we put the cell state through <code>tanh</code> (to push the values to be between $âˆ’1$ and $1$) and multiply it by the output of the <code>sigmoid</code> gate, so that we only output the parts we decided to.</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case thatâ€™s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if thatâ€™s what follows next.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-focus-o.png" alt="img"></p><h2 id="LSTMçš„å˜å¼‚ä½“"><a href="#LSTMçš„å˜å¼‚ä½“" class="headerlink" title="LSTMçš„å˜å¼‚ä½“"></a>LSTMçš„å˜å¼‚ä½“</h2><p>What Iâ€™ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but itâ€™s worth mentioning some of them.</p><p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a>, is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-var-peepholes.png" alt="img"></p><p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p><p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when weâ€™re going to input something in its place. We only input new values to the state when we forget something older.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-var-tied.png" alt="img" style="zoom:36%;" /></p><p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a>. It combines the forget and input gates into a single â€œupdate gate.â€ It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p><p><img src="/images/001-%E7%90%86%E8%A7%A3LSTM/LSTM3-var-GRU.png" alt="A gated recurrent unit neural network." style="zoom:36%;" /></p><p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="noopener">Yao, et al. (2015)</a>. Thereâ€™s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="noopener">Koutnik, et al. (2014)</a>.</p><p>Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that theyâ€™re all about the same. <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz, et al. (2015)</a> tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p><p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p><p>LSTMs were a big step in what we can accomplish with RNNs. Itâ€™s natural to wonder: is there another big step? A common opinion among researchers is: â€œYes! There is a next step and itâ€™s attention!â€ The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, <em>et al.</em> (2015)</a> do exactly this â€“ it might be a fun starting point if you want to explore attention! Thereâ€™s been a number of really exciting results using attention, and it seems like a lot more are around the cornerâ€¦</p><p>Attention isnâ€™t the only exciting thread in RNN research. For example, Grid LSTMs by <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, <em>et al.</em> (2015)</a> seem extremely promising. Work using RNNs in generative models â€“ such as <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, <em>et al.</em> (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, <em>et al.</em> (2015)</a>, or <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a> â€“ also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ç†è§£å’Œä½¿ç”¨Pytorchæ­å»ºGAN</title>
      <link href="/009-li-jie-he-shi-yong-pytorch-da-jian-gan/"/>
      <url>/009-li-jie-he-shi-yong-pytorch-da-jian-gan/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25" target="_blank" rel="noopener">https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25</a></p><p>åŸæ–‡æ ‡é¢˜ï¼šUnderstanding and building Generative Adversarial Networks(GANs)- Deep Learning with PyTorch</p><hr><blockquote><p><em>Weâ€™ll be building a Generative Adversarial Network that will be able to generate images of birds that never actually existed in the real world.</em></p></blockquote><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_OshLCyKhDM6bo-MXqfrM9w.jpeg" alt="img"></p><p>-These bird images are purely generated by the Deep Learning Model(GAN)-</p><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_RiLLyBMOagYJktyv5eodIQ.png" alt="img"></p><p>Before we actually start building a GAN, let us first talk about the idea behind GANs. GANs were invented by Ian Goodfellow, he obtained his <a href="https://en.wikipedia.org/wiki/Bachelor_of_Science" target="_blank" rel="noopener">B.S.</a> and <a href="https://en.wikipedia.org/wiki/Master_of_Science" target="_blank" rel="noopener">M.S.</a> in computer science from <a href="https://en.wikipedia.org/wiki/Stanford_University" target="_blank" rel="noopener">Stanford University</a> and his Ph.D. in machine learning from the <a href="https://en.wikipedia.org/wiki/UniversitÃ©_de_MontrÃ©al" target="_blank" rel="noopener">UniversitÃ© de MontrÃ©al</a>,. This is the new big thing in the field of Deep Learning right now. Yann LeCun, the director of Facebook AI said :</p><blockquote><p><em>â€œGenerative Adversarial Networks is the most interesting idea in the last ten years in Machine Learning.â€</em></p></blockquote><h2 id="ä½•è°“GANs-å®ƒæœ‰ä½•ç”¨"><a href="#ä½•è°“GANs-å®ƒæœ‰ä½•ç”¨" class="headerlink" title="ä½•è°“GANs ? å®ƒæœ‰ä½•ç”¨ ?"></a>ä½•è°“GANs ? å®ƒæœ‰ä½•ç”¨ ?</h2><p>Neural Networks are good at classifying and predicting things, and AI Researchers wanted to make the neural net more human in nature by allowing it to CREATE rather than just letting it see things, and turns out that Ian Goodfellow was successful in inventing a class of Deep Learning Model which could do that.</p><h2 id="GANså¦‚ä½•å·¥ä½œ"><a href="#GANså¦‚ä½•å·¥ä½œ" class="headerlink" title="GANså¦‚ä½•å·¥ä½œ ?"></a>GANså¦‚ä½•å·¥ä½œ ?</h2><p>GANs contain two separate neural networks. Let us call one neural network as â€œGâ€, which stands for <strong>Generator </strong>and the other neural network as â€œDâ€, which is a <strong>Discriminator</strong>. The Generator first generates random images and a Discriminator sees those images and tells the Generator how real the generated images are.</p><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_YH3b1fARO-bf6gU3kyzT4A.jpeg" alt="img"></p><h2 id="ç”Ÿæˆå™¨"><a href="#ç”Ÿæˆå™¨" class="headerlink" title="ç”Ÿæˆå™¨ :"></a>ç”Ÿæˆå™¨ :</h2><p>In the starting phase, a Generator model takes <code>random noise signals</code> as input and generates a random noisy image as the output, gradually with the help of the Discriminator, it starts generating images of a particular class that look real.</p><h2 id="åˆ¤åˆ«å™¨"><a href="#åˆ¤åˆ«å™¨" class="headerlink" title="åˆ¤åˆ«å™¨ :"></a>åˆ¤åˆ«å™¨ :</h2><p>The Discriminator which will be the opponent of Generator is fed with both the generated images as well as a certain class of images at the same time, allowing it to tell the generated how the real image looks like.</p><p>After reaching a certain point, the Discriminator will be unable to tell if the generate image is a real or a fake image, and that is when we can see images of a certain class(class that the discriminator is trained with.) being generated by out Generator that never actually existed before.</p><h2 id="GANçš„åº”ç”¨"><a href="#GANçš„åº”ç”¨" class="headerlink" title="GANçš„åº”ç”¨ :"></a>GANçš„åº”ç”¨ :</h2><ul><li><p>è¶…åˆ†è¾¨ç‡ï¼ˆSuper Resolutionï¼‰.</p><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_CjlvvAEa800e3asqLWkFpQ.png" alt="img"></p></li></ul><ul><li>Assisting Artists.</li></ul><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_xW7HOxpzO_ZNGyYXX4MMPQ.jpeg" alt="img"></p><ul><li><p>Element Abstraction.</p><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_iRyCj-s28wuupEZjOIcknw.png" alt="img"></p><h1 id="ä¸Šä»£ç "><a href="#ä¸Šä»£ç " class="headerlink" title="ä¸Šä»£ç "></a>ä¸Šä»£ç </h1><p>NOTE : The below explanation of the code is not prepared for a novice deep learning programmer , i expect you to be comfortable with the deep learning accent in python.</p><p><strong>L</strong>et us start by importing all the required python libraries for building our GAN. Please make sure PyTorch is installed in your computer before you start.</p><pre class=" language-lang-python"><code class="language-lang-python">#importing required librariesfrom __future__ import print_functionimport torchimport torch.nn as nnimport torch.nn.parallelimport torch.optim as optimimport torch.utils.dataimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torchvision.utils as vutilsfrom torch.autograd import Variable</code></pre><p><strong>N</strong>ow let us set the hyper-parameters which will be the <strong>batch-size</strong> and <strong>image-size</strong> in this case :</p><pre class=" language-lang-python"><code class="language-lang-python"># Setting hyperparametersbatchSize = 64 imageSize = 64</code></pre></li></ul><p>  In the first line, we have set the size of the batch to 64. And in the second line we have set the size of the images generated by the generator to 64 x 64 resolution.</p><hr><p>  <strong>T</strong>hen we are going to create an object to perform image transformations as given below :</p><pre class=" language-lang-python"><code class="language-lang-python"># Creating the transformationstransform = transforms.Compose([transforms.Scale(imageSize),                                 transforms.ToTensor(),                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])</code></pre><p>The above transformations are necessary to make the image compatible as an input to the neural network of the discriminator.</p><hr><p>NOTE : In order to get the dataset, click here and you will be directed to <a href="https://github.com/venkateshtata/GAN_Medium.git" target="_blank" rel="noopener">https://github.com/venkateshtata/GAN_Medium.git</a> , clone that repository into your local system and replace the <code>dcgan.py</code> file with the python file your writing to. the <code>data</code> folder contains the dataset.</p><hr><p><strong>N</strong>ow lets load our dataset from a respective directory. The type of dataset we are going to be using here is a <code>CIFAR-10</code> dataset. We are going to load them in batches, and make sure that the python file you are writing to is in the same directory for less complexity while importing the dataset.</p><pre class=" language-lang-python"><code class="language-lang-python"># Loading the datasetdataset = dset.CIFAR10(root = './data', download = True, transform = transform)dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)</code></pre><p>We download the training set in the <code>./data</code> folder and we apply the previous transformations on each image. Then use <code>dataLoader</code> to get the images of the training set batch by batch. Almost every element of the above code is self explanatory, the value of <code>num_workers</code> defines the number of threads that must be used to carry out the process of loading the training data.</p><hr><p><strong>A</strong>s we will be dealing with multiple(2) neural networks here, we will be defining a universal function to initialise the weights of a given neural network by calling the function and passing the NN(Neural Network) into it.</p><pre class=" language-lang-python"><code class="language-lang-python">def weights_init(m):    classname = m.__class__.__name__    if classname.find('Conv') != -1:        m.weight.data.normal_(0.0, 0.02)    elif classname.find('BatchNorm') != -1:        m.weight.data.normal_(1.0, 0.02)        m.bias.data.fill_(0)</code></pre><p>The above <code>weights_init</code> function takes as input a neural network <code>m</code> and will initialise all its weights. This function will be called for each iteration during the training process.</p><hr><p><img src="009-%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BAGAN/1_NFO8IogPJRf_eGKBZnd-Fg.png" alt="img"></p><p><strong>O</strong>ur first big step will be to define a class for our <code>Generator neural network</code>. Weâ€™ll start by creating a class that will be holding the architecture of the Generator, which will basically contain a sequence of layers that each input undergoes.</p><pre class=" language-lang-python"><code class="language-lang-python">class G(nn.Module):    def __init__(self):            super(G, self).__init__()            self.main = nn.Sequential(                            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),                            nn.BatchNorm2d(512),                            nn.ReLU(True),                            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),                            nn.BatchNorm2d(256),                            nn.ReLU(True),                            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),                            nn.BatchNorm2d(128),                            nn.ReLU(True),                            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),                            nn.BatchNorm2d(64),                            nn.ReLU(True),                            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),                            nn.Tanh()                        )</code></pre><p><strong>B</strong>reaking down the above code :</p><ul><li>We have created a class â€˜Gâ€™, referring to the Generator neural network, and inheriting from <code>nn.module</code> which contains all the tools required for building neural networks, which help us is placing different applications and and connections inside a given neural network.</li><li>Then we create a meta module of a neural network that will contain a sequence of modules such as convolutions, full connections, etc.</li><li>A great thing to observe from the above Fig 1.0 is that the structures of neural networks of both Generator and the Discriminator are inverse to each other, which basically means that <strong>in Generator, the Convolution must be in an inverse way</strong>, where the the input will be random noise vectors. Hence we start with an inverse convolution using <code>ConvTranspose2d</code>.</li><li>Then we normalize all the features along the dimension of the batch and apply a <code>ReLU</code> rectification to break the linearity. Click <a href="http://pytorch.org/docs/master/nn.html" target="_blank" rel="noopener">here</a> for more detailed explanation of parameters used in the above functions.</li><li>We repeat the above operations again while changing the input nodes from â€˜100â€™ to â€˜512â€™, the number of feature maps from <code>512</code> to <code>256</code> and keeping the bias as False. [ Note: The values i am choosing in the above code are choices of researchers. ]</li><li>In the final <code>ConvTranspose2d</code> we will be outputting 3 filters as the output image of the generator is going to be a 3 channel(RGB) and we apply a <code>Tanh</code> rectification to break the linearity and stay between -1 and +1.</li></ul><hr><p><strong>N</strong>ow we need to create a tool which will be a forward function to propogate the signal inside the Generator.</p><pre class=" language-lang-python"><code class="language-lang-python">def forward(self, input):        output = self.main(input)        return output</code></pre><p>The input of the above function will be some random vector of size <code>100</code> as defined inside the <code>G</code> class. It returns the output containing the generated images. The initial image is made up random vectors.</p><hr><p><strong>C</strong>reating the generator :</p><pre class=" language-lang-python"><code class="language-lang-python">netG = G() netG.apply(weights_init)</code></pre><p>Here we are creating a generator object and initialising all the weights of the input neural network.</p><hr><p><strong>N</strong>ow, lets start defining our <strong>Discriminator</strong> class that will be holding the architecture of a Discriminator.</p><pre class=" language-lang-python"><code class="language-lang-python">class D(nn.Module):def __init__(self):        super(D, self).__init__()        self.main = nn.Sequential(            nn.Conv2d(3, 64, 4, 2, 1, bias = False),            nn.LeakyReLU(0.2, inplace = True),            nn.Conv2d(64, 128, 4, 2, 1, bias = False),            nn.BatchNorm2d(128),            nn.LeakyReLU(0.2, inplace = True),            nn.Conv2d(128, 256, 4, 2, 1, bias = False),            nn.BatchNorm2d(256),            nn.LeakyReLU(0.2, inplace = True),            nn.Conv2d(256, 512, 4, 2, 1, bias = False),            nn.BatchNorm2d(512),            nn.LeakyReLU(0.2, inplace = True),            nn.Conv2d(512, 1, 4, 1, 0, bias = False),            nn.Sigmoid()        )</code></pre><p><strong>B</strong>reaking down the Discriminator :</p><ul><li>Similar to the G class, the <code>D</code> Discriminator class is inheriting from the <code>nn.module</code>. The input of the <strong>Discriminator</strong> will be the image generated by the <strong>Generator</strong>, to which the <strong>Discriminator</strong> will be returning a number between 0 and 1 as output.</li><li>Since it takes a generated image of the generator, the first operation is going to be a convolution, hence we start with a convolution and apply <code>LeakyReLU</code>.</li><li>Observe that unlike the what we did in <code>G</code> class, we are using <code>LeakyReLU</code> here, which will take the negative slope till <code>0.2</code>, and this comes from frequent experimentation, which i didnâ€™t do, but researchers choice.</li><li>We use <code>BatchNorm2d</code> to normalize all the features along the dimension of the batch.</li><li>And at the end, we are using the classic old fashioned function, which is the <code>sigmoid</code> function to break the linearity and stay between 0 and 1.</li></ul><hr><p><strong>N</strong>ow, in order to forward propagate the signal into the <strong>Discriminator</strong>, we need to define a <code>Forward</code> class, which is going to carry the output of the generator to the <strong>Discriminator</strong> :</p><pre class=" language-lang-python"><code class="language-lang-python">def forward(self, input):        output = self.main(input)        return output.view(-1)</code></pre><p>In the final line we return the output which will be a value between 0 and 1, because we need to flatten passed NN to make sure the vectors are in the same dimension.</p><hr><p><strong>C</strong>reating the Discriminator :</p><pre class=" language-lang-python"><code class="language-lang-python">netD = D() netD.apply(weights_init)</code></pre><p>We create the discriminator object of the above class <code>D</code> and initialize all the weights of its neural network.</p><hr><p><strong>N</strong>ow its time we train our Generative Adversarial Network. But before that we need to start by getting a criteria that will measure the error of prediction given by the discriminator. In order to achieve that, we are going to use <strong>BCE Loss</strong>(where BCE means Binary Cross Entropy.), which is perfect for Adversarial Neural Networks. Hence we need optimisers for both the generator as well as the discriminator.</p><pre class=" language-lang-python"><code class="language-lang-python">criterion = nn.BCELoss()optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))</code></pre><p>We start by creating a criterion object that will measure the error between the prediction and the target. Then we create optimisers for objects of both discriminator and the generator.</p><p>We are using <code>Adam</code> optimiser from the <code>optim</code> module, which is a highly advance optimal for stochastic gradient descent.</p><hr><p><strong>W</strong>eâ€™ll be training our neural nets for <code>25</code> epochs, hence :</p><pre class=" language-lang-python"><code class="language-lang-python">for epoch in range(25):</code></pre><p>Then we need to iterate over the images within the dataset, hence :</p><pre class=" language-lang-python"><code class="language-lang-python">for i, data in enumerate(dataloader, 0):</code></pre><p>First step is to update the weights of the neural network of the discriminator, hence we initialise the gradients of the discriminator to 0 with respect to the weights :</p><pre class=" language-lang-python"><code class="language-lang-python">netD.zero_grad()</code></pre><p><strong>A</strong>s we know that our discriminator must be trained with both the real and fake images at a time. Hence we will train the discriminator with a real image of the dataset first :</p><pre class=" language-lang-python"><code class="language-lang-python">real, _ = data        input = Variable(real)        target = Variable(torch.ones(input.size()[0]))        output = netD(input)        errD_real = criterion(output, target)</code></pre><p>We get a real image of the dataset which will be used to train the discriminator, and then wrap it in a variable. Then we forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1) and compute the loss between the predictions (output) and the target (equal to 1).</p><hr><p><strong>N</strong>ow, training the discriminator with a fake image generated by the generator :</p><pre class=" language-lang-python"><code class="language-lang-python">noise = Variable(torch.randn(input.size()[0], 100, 1, 1))        fake = netG(noise)        target = Variable(torch.zeros(input.size()[0]))        output = netD(fake.detach())        errD_fake = criterion(output, target)</code></pre><p>Here, first we are making a random input vector (noise) of the generator and forward propagate this random input vector into the neural network of the generator to get some fake generated images. Then we forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1) and compute the loss between the prediction (output) and the target (equal to 0).</p><hr><p><strong>B</strong>ack-propagating the total error :</p><pre class=" language-lang-python"><code class="language-lang-python">errD = errD_real + errD_fake        errD.backward()        optimizerD.step()</code></pre><p>Here we are computing the total error of the discriminator and backpropagating the loss error by computing the gradients of the total error with respect to the weights of the discriminator. At the end we apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.</p><hr><p><strong>N</strong>ext step is to update the weights of the neural network of the generator :</p><pre class=" language-lang-python"><code class="language-lang-python">netG.zero_grad()        target = Variable(torch.ones(input.size()[0]))        output = netD(fake)        errG = criterion(output, target)        errG.backward()        optimizerG.step()</code></pre><p><strong>A</strong>s done previously , first we are initialising the gradients of the generator to 0 with respect to the weights. Getting the target. Forward propagating the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1) and then computing the loss between the prediction (output between 0 and 1) and the target (equal to 1). Then back-propagating the loss error by computing the gradients of the total error with respect to the weights of the generator and applying the optimizer to update the weights according to how much they are responsible for the loss error of the generator.</p><hr><p><strong>N</strong>ow, our final step is to print the losses and save the real images and the generated images of the mini batch every 100 steps. Which is done as followed :</p><pre class=" language-lang-python"><code class="language-lang-python">print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))        if i % 100 == 0:            vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True)            fake = netG(noise)            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True)</code></pre><h2 id="å®Œæ•´ä»£ç "><a href="#å®Œæ•´ä»£ç " class="headerlink" title="å®Œæ•´ä»£ç  :"></a>å®Œæ•´ä»£ç  :</h2><pre class=" language-lang-python"><code class="language-lang-python">from __future__ import print_functionimport torchimport torch.nn as nnimport torch.nn.parallelimport torch.optim as optimimport torch.utils.dataimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torchvision.utils as vutilsfrom torch.autograd import VariablebatchSize = 64 imageSize = 64transform = transforms.Compose([transforms.Scale(imageSize),                                 transforms.ToTensor(),                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.dataset = dset.CIFAR10(root = './data',                        download = True,                        transform = transform) dataloader = torch.utils.data.DataLoader(dataset,                                          batch_size = batchSize,                                          shuffle = True, num_workers = 2) def weights_init(m):    classname = m.__class__.__name__    if classname.find('Conv') != -1:        m.weight.data.normal_(0.0, 0.02)    elif classname.find('BatchNorm') != -1:        m.weight.data.normal_(1.0, 0.02)        m.bias.data.fill_(0)# Generatorclass G(nn.Module):    def __init__(self):            super(G, self).__init__()            self.main = nn.Sequential(                nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),                nn.BatchNorm2d(512),                nn.ReLU(True),                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),                nn.BatchNorm2d(256),                nn.ReLU(True),                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),                nn.BatchNorm2d(128),                nn.ReLU(True),                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),                nn.BatchNorm2d(64),                nn.ReLU(True),                nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),                nn.Tanh()            )    def forward(self, input):            output = self.main(input)            return outputnetG = G()netG.apply(weights_init)# Discriminatorclass D(nn.Module):    def __init__(self):            super(D, self).__init__()            self.main = nn.Sequential(                nn.Conv2d(3, 64, 4, 2, 1, bias = False),                nn.LeakyReLU(0.2, inplace = True),                nn.Conv2d(64, 128, 4, 2, 1, bias = False),                nn.BatchNorm2d(128),                nn.LeakyReLU(0.2, inplace = True),                nn.Conv2d(128, 256, 4, 2, 1, bias = False),                nn.BatchNorm2d(256),                nn.LeakyReLU(0.2, inplace = True),                nn.Conv2d(256, 512, 4, 2, 1, bias = False),                nn.BatchNorm2d(512),                nn.LeakyReLU(0.2, inplace = True),                nn.Conv2d(512, 1, 4, 1, 0, bias = False),                nn.Sigmoid()            )    def forward(self, input):            output = self.main(input)            return output.view(-1)netD = D()netD.apply(weights_init)# Create criterioncriterion = nn.BCELoss()optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))# Batch Trainingfor epoch in range(25):    for i, data in enumerate(dataloader, 0):            netD.zero_grad()            real, _ = data            input = Variable(real)            target = Variable(torch.ones(input.size()[0]))            output = netD(input)            errD_real = criterion(output, target)            noise = Variable(torch.randn(input.size()[0], 100, 1, 1))            fake = netG(noise)            target = Variable(torch.zeros(input.size()[0]))            output = netD(fake.detach())            errD_fake = criterion(output, target)            errD = errD_real + errD_fake            errD.backward()            optimizerD.step()            netG.zero_grad()            target = Variable(torch.ones(input.size()[0]))            output = netD(fake)            errG = criterion(output, target)            errG.backward()            optimizerG.step()            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i,     len(dataloader), errD.data[0], errG.data[0]))            if i % 100 == 0:                vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True)                fake = netG(noise)                vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True)</code></pre><p>ä»£ç åº“åœ¨æ­¤ : <a href="https://github.com/venkateshtata/GAN_Medium" target="_blank" rel="noopener">https://github.com/venkateshtata/GAN_Medium</a></p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>å˜åˆ†è‡ªç¼–ç å™¨VAEæ˜¯è¿™ä¹ˆä¸€å›äº‹</title>
      <link href="/vae-bian-fen-zi-bian-ma-qi-vae-yuan-lai-shi-zhe-me-yi-hui-shi/"/>
      <url>/vae-bian-fen-zi-bian-ma-qi-vae-yuan-lai-shi-zhe-me-yi-hui-shi/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34998569</a></p><hr><p>è¿‡å»è™½ç„¶æ²¡æœ‰ç»†çœ‹ï¼Œä½†å°è±¡é‡Œä¸€ç›´è§‰å¾—<strong>å˜åˆ†è‡ªç¼–ç å™¨</strong>ï¼ˆVariational Auto-Encoderï¼ŒVAEï¼‰æ˜¯ä¸ªå¥½ä¸œè¥¿ã€‚è¶ç€æœ€è¿‘çœ‹æ¦‚ç‡å›¾æ¨¡å‹çš„ä¸‰åˆ†é’Ÿçƒ­åº¦ï¼Œæˆ‘å†³å®šä¹Ÿäº‰å–æŠŠ VAE ææ‡‚ã€‚</p><p>äºæ˜¯ä¹ç…§æ ·ç¿»äº†ç½‘ä¸Šå¾ˆå¤šèµ„æ–™ï¼Œæ— ä¸€ä¾‹å¤–å‘ç°éƒ½å¾ˆå«ç³Šï¼Œä¸»è¦çš„æ„Ÿè§‰æ˜¯å…¬å¼å†™äº†ä¸€å¤§é€šï¼Œè¿˜æ˜¯è¿·è¿·ç³Šç³Šçš„ï¼Œæœ€åå¥½ä¸å®¹æ˜“è§‰å¾—çœ‹æ‡‚äº†ï¼Œå†å»çœ‹çœ‹å®ç°çš„ä»£ç ï¼Œåˆæ„Ÿè§‰å®ç°ä»£ç è·Ÿç†è®ºå®Œå…¨ä¸æ˜¯ä¸€å›äº‹å•Šã€‚</p><p>ç»ˆäºï¼Œä¸œæ‹¼è¥¿å‡‘å†åŠ ä¸Šæˆ‘è¿™æ®µæ—¶é—´å¯¹æ¦‚ç‡æ¨¡å‹çš„ä¸€äº›ç§¯ç´¯ï¼Œå¹¶åå¤å¯¹æ¯”åŸè®ºæ–‡ <em>Auto-Encoding Variational Bayes</em>ï¼Œæœ€åæˆ‘è§‰å¾—æˆ‘åº”è¯¥æ˜¯æƒ³æ˜ç™½äº†ã€‚</p><p>å…¶å®çœŸæ­£çš„ VAEï¼Œè·Ÿå¾ˆå¤šæ•™ç¨‹è¯´çš„çš„è¿˜çœŸä¸å¤§ä¸€æ ·ï¼Œå¾ˆå¤šæ•™ç¨‹å†™äº†ä¸€å¤§é€šï¼Œéƒ½æ²¡æœ‰æŠŠæ¨¡å‹çš„è¦ç‚¹å†™å‡ºæ¥ã€‚äºæ˜¯å†™äº†è¿™ç¯‡ä¸œè¥¿ï¼Œå¸Œæœ›é€šè¿‡ä¸‹é¢çš„æ–‡å­—ï¼Œèƒ½æŠŠ VAE åˆæ­¥è®²æ¸…æ¥šã€‚</p><h2 id="åˆ†å¸ƒå˜æ¢"><a href="#åˆ†å¸ƒå˜æ¢" class="headerlink" title="åˆ†å¸ƒå˜æ¢"></a><strong>åˆ†å¸ƒå˜æ¢</strong></h2><p>é€šå¸¸æˆ‘ä»¬ä¼šæ‹¿ VAE è·Ÿ GAN æ¯”è¾ƒï¼Œçš„ç¡®ï¼Œå®ƒä»¬ä¸¤ä¸ªçš„ç›®æ ‡åŸºæœ¬æ˜¯ä¸€è‡´çš„â€”â€”å¸Œæœ›<strong>æ„å»ºä¸€ä¸ªä»éšå˜é‡ <em>Z</em> ç”Ÿæˆç›®æ ‡æ•°æ® <em>X</em> çš„æ¨¡å‹</strong>ï¼Œä½†æ˜¯å®ç°ä¸Šæœ‰æ‰€ä¸åŒã€‚</p><p>æ›´å‡†ç¡®åœ°è®²ï¼Œå®ƒä»¬æ˜¯å‡è®¾äº†æœä»æŸäº›å¸¸è§çš„åˆ†å¸ƒï¼ˆæ¯”å¦‚æ­£æ€åˆ†å¸ƒæˆ–å‡åŒ€åˆ†å¸ƒï¼‰ï¼Œç„¶åå¸Œæœ›è®­ç»ƒä¸€ä¸ªæ¨¡å‹ <em>X</em>=<em>g</em>(<em>Z</em>)ï¼Œè¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿ==å°†åŸæ¥çš„æ¦‚ç‡åˆ†å¸ƒæ˜ å°„åˆ°è®­ç»ƒé›†çš„æ¦‚ç‡åˆ†å¸ƒ==ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>å®ƒä»¬çš„ç›®çš„éƒ½æ˜¯è¿›è¡Œåˆ†å¸ƒä¹‹é—´çš„å˜æ¢</strong>ã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-d7a52f6f211594a0853993365f51cb82_hd.jpg" alt="img"></p><p>ç”Ÿæˆæ¨¡å‹çš„éš¾é¢˜å°±æ˜¯åˆ¤æ–­ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„ç›¸ä¼¼åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªçŸ¥é“ä¸¤è€…çš„é‡‡æ ·ç»“æœï¼Œä¸çŸ¥é“å®ƒä»¬çš„åˆ†å¸ƒè¡¨è¾¾å¼ã€‚</p><p>é‚£ç°åœ¨å‡è®¾æœä»æ ‡å‡†çš„æ­£æ€åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘å°±å¯ä»¥ä»ä¸­é‡‡æ ·å¾—åˆ°è‹¥å¹²ä¸ª $Z_1$,$Z_2$,â€¦,$Z_n$ï¼Œç„¶åå¯¹å®ƒåšå˜æ¢å¾—åˆ° XÌ‚1=g(Z1),XÌ‚2=g(Z2),â€¦,XÌ‚n=g(Zn)ï¼Œ<strong>æˆ‘ä»¬æ€ä¹ˆåˆ¤æ–­è¿™ä¸ªé€šè¿‡ f æ„é€ å‡ºæ¥çš„æ•°æ®é›†ï¼Œå®ƒçš„åˆ†å¸ƒè·Ÿæˆ‘ä»¬ç›®æ ‡çš„æ•°æ®é›†åˆ†å¸ƒæ˜¯ä¸æ˜¯ä¸€æ ·çš„å‘¢ï¼Ÿ</strong></p><p>æœ‰è¯»è€…è¯´ä¸æ˜¯æœ‰ KL æ•£åº¦å—ï¼Ÿå½“ç„¶ä¸è¡Œï¼Œå› ä¸º KL æ•£åº¦æ˜¯æ ¹æ®ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„<strong>è¡¨è¾¾å¼</strong>æ¥ç®—å®ƒä»¬çš„ç›¸ä¼¼åº¦çš„ï¼Œç„¶è€Œç›®å‰æˆ‘ä»¬å¹¶ä¸çŸ¥é“å®ƒä»¬çš„æ¦‚ç‡åˆ†å¸ƒçš„è¡¨è¾¾å¼ã€‚</p><p>æˆ‘ä»¬åªæœ‰ä¸€æ‰¹ä»æ„é€ çš„åˆ†å¸ƒé‡‡æ ·è€Œæ¥çš„æ•°æ® {XÌ‚1,XÌ‚2,â€¦,XÌ‚n}ï¼Œè¿˜æœ‰ä¸€æ‰¹ä»çœŸå®çš„åˆ†å¸ƒé‡‡æ ·è€Œæ¥çš„æ•°æ® {X1,X2,â€¦,Xn}ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„è®­ç»ƒé›†ï¼‰ã€‚æˆ‘ä»¬åªæœ‰æ ·æœ¬æœ¬èº«ï¼Œæ²¡æœ‰åˆ†å¸ƒè¡¨è¾¾å¼ï¼Œå½“ç„¶ä¹Ÿå°±æ²¡æœ‰æ–¹æ³•ç®— KL æ•£åº¦ã€‚</p><p>è™½ç„¶é‡åˆ°å›°éš¾ï¼Œä½†è¿˜æ˜¯è¦æƒ³åŠæ³•è§£å†³çš„ã€‚<strong>GAN çš„æ€è·¯å¾ˆç›´æ¥ç²—çŠ·ï¼šæ—¢ç„¶æ²¡æœ‰åˆé€‚çš„åº¦é‡ï¼Œé‚£æˆ‘å¹²è„†æŠŠè¿™ä¸ªåº¦é‡ä¹Ÿç”¨ç¥ç»ç½‘ç»œè®­ç»ƒå‡ºæ¥å§</strong>ã€‚</p><p>å°±è¿™æ ·ï¼Œ<strong>WGAN</strong> å°±è¯ç”Ÿäº†ï¼Œè¯¦ç»†è¿‡ç¨‹è¯·å‚è€ƒ<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwMTc4ODE0Mw%3D%3D%26mid%3D2247484880%26idx%3D1%26sn%3D4b2e976cc715c9fe2d022ff6923879a8%26chksm%3D96e9da50a19e5346307b54f5ce172e355ccaba890aa157ce50fda68eeaccba6ea05425f6ad76%26scene%3D21%23wechat_redirect">äº’æ€¼çš„è‰ºæœ¯ï¼šä»é›¶ç›´è¾¾ WGAN-GP</a>ã€‚è€Œ VAE åˆ™ä½¿ç”¨äº†ä¸€ä¸ªç²¾è‡´è¿‚å›çš„æŠ€å·§ã€‚</p><h2 id="VAEæ…¢è°ˆ"><a href="#VAEæ…¢è°ˆ" class="headerlink" title="VAEæ…¢è°ˆ"></a><strong>VAEæ…¢è°ˆ</strong></h2><p>è¿™ä¸€éƒ¨åˆ†æˆ‘ä»¬å…ˆå›é¡¾ä¸€èˆ¬æ•™ç¨‹æ˜¯æ€ä¹ˆä»‹ç» VAE çš„ï¼Œç„¶åå†æ¢ç©¶æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œæ¥ç€å°±è‡ªç„¶åœ°å‘ç°äº† VAE çœŸæ­£çš„é¢ç›®ã€‚</p><p><strong>ç»å…¸å›é¡¾</strong></p><p>é¦–å…ˆæˆ‘ä»¬æœ‰ä¸€æ‰¹æ•°æ®æ ·æœ¬ {<em>X</em>1,â€¦,<em>X</em>n}ï¼Œå…¶æ•´ä½“ç”¨ <em>X</em> æ¥æè¿°ï¼Œæˆ‘ä»¬æœ¬æƒ³æ ¹æ® {<em>X</em>1,â€¦,<em>X</em>n} å¾—åˆ° <em>X</em> çš„åˆ†å¸ƒ <em>p</em>(<em>X</em>)ï¼Œå¦‚æœèƒ½å¾—åˆ°çš„è¯ï¼Œé‚£æˆ‘ç›´æ¥æ ¹æ® <em>p</em>(<em>X</em>) æ¥é‡‡æ ·ï¼Œå°±å¯ä»¥å¾—åˆ°æ‰€æœ‰å¯èƒ½çš„ <em>X</em> äº†ï¼ˆåŒ…æ‹¬ {<em>X</em>1,â€¦,<em>X</em>n} ä»¥å¤–çš„ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»ˆæç†æƒ³çš„ç”Ÿæˆæ¨¡å‹äº†ã€‚</p><p>å½“ç„¶ï¼Œè¿™ä¸ªç†æƒ³å¾ˆéš¾å®ç°ï¼Œäºæ˜¯æˆ‘ä»¬å°†åˆ†å¸ƒæ”¹ä¸€æ”¹ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-6d99b17711df7d1fe8efbee2dbce28ff_hd.jpg" alt="img"></p><p>è¿™é‡Œæˆ‘ä»¬å°±ä¸åŒºåˆ†æ±‚å’Œè¿˜æ˜¯æ±‚ç§¯åˆ†äº†ï¼Œæ„æ€å¯¹äº†å°±è¡Œã€‚æ­¤æ—¶ <em>p</em>(<em>X</em>|<em>Z</em>) å°±æè¿°äº†ä¸€ä¸ªç”± <em>Z</em> æ¥ç”Ÿæˆ <em>X</em>çš„æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬å‡è®¾ <em>Z</em> æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯ <em>p</em>(<em>Z</em>)=<em>N</em>(0,<em>I</em>)ã€‚<strong>å¦‚æœè¿™ä¸ªç†æƒ³èƒ½å®ç°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å…ˆä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ª</strong> <strong>Zï¼Œç„¶åæ ¹æ®</strong> <strong><em>Z</em></strong> <strong>æ¥ç®—ä¸€ä¸ª</strong> <strong>Xï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆæ£’çš„ç”Ÿæˆæ¨¡å‹</strong>ã€‚</p><p>æ¥ä¸‹æ¥å°±æ˜¯ç»“åˆè‡ªç¼–ç å™¨æ¥å®ç°é‡æ„ï¼Œä¿è¯æœ‰æ•ˆä¿¡æ¯æ²¡æœ‰ä¸¢å¤±ï¼Œå†åŠ ä¸Šä¸€ç³»åˆ—çš„æ¨å¯¼ï¼Œæœ€åæŠŠæ¨¡å‹å®ç°ã€‚æ¡†æ¶çš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-25f257b89b46996fdfaaf3935a9bfb48_hd.jpg" alt="img"></p><p><strong>â–²</strong> VAEçš„ä¼ ç»Ÿç†è§£</p><p>çœ‹å‡ºäº†ä»€ä¹ˆé—®é¢˜äº†å—ï¼Ÿå¦‚æœåƒè¿™ä¸ªå›¾çš„è¯ï¼Œæˆ‘ä»¬å…¶å®å®Œå…¨ä¸æ¸…æ¥šï¼š<strong>ç©¶ç«Ÿç»è¿‡é‡æ–°é‡‡æ ·å‡ºæ¥çš„Zkï¼Œæ˜¯ä¸æ˜¯è¿˜å¯¹åº”ç€åŸæ¥çš„</strong> <strong>Xkï¼Œæ‰€ä»¥æˆ‘ä»¬å¦‚æœç›´æ¥æœ€å°åŒ–</strong> <strong>D(XÌ‚k,Xk)^2ï¼ˆè¿™é‡Œ</strong> <strong>D ä»£è¡¨æŸç§è·ç¦»å‡½æ•°ï¼‰æ˜¯å¾ˆä¸ç§‘å­¦çš„ï¼Œè€Œäº‹å®ä¸Šä½ çœ‹ä»£ç ä¹Ÿä¼šå‘ç°æ ¹æœ¬ä¸æ˜¯è¿™æ ·å®ç°çš„</strong>ã€‚</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå¾ˆå¤šæ•™ç¨‹è¯´äº†ä¸€å¤§é€šå¤´å¤´æ˜¯é“çš„è¯ï¼Œç„¶åå†™ä»£ç æ—¶å´ä¸æ˜¯æŒ‰ç…§æ‰€å†™çš„æ–‡å­—æ¥å†™ï¼Œå¯æ˜¯ä»–ä»¬ä¹Ÿä¸è§‰å¾—è¿™æ ·ä¼šæœ‰çŸ›ç›¾ã€‚</p><p><strong>VAEåˆç°</strong></p><p>å…¶å®ï¼Œ<strong>åœ¨æ•´ä¸ª VAE æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å»ä½¿ç”¨</strong> <strong>p(Z)ï¼ˆå…ˆéªŒåˆ†å¸ƒï¼‰æ˜¯æ­£æ€åˆ†å¸ƒçš„å‡è®¾ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯å‡è®¾</strong> <strong>p(Z|X)ï¼ˆåéªŒåˆ†å¸ƒï¼‰æ˜¯æ­£æ€åˆ†å¸ƒ</strong>ã€‚</p><p>å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªçœŸå®æ ·æœ¬ <em>Xk</em>ï¼Œæˆ‘ä»¬å‡è®¾å­˜åœ¨<strong>ä¸€ä¸ªä¸“å±äº</strong> <strong><em>Xk</em></strong> <strong>çš„åˆ†å¸ƒ</strong> <strong>p(Z|Xk)</strong>ï¼ˆå­¦åå«åéªŒåˆ†å¸ƒï¼‰ï¼Œå¹¶è¿›ä¸€æ­¥å‡è®¾è¿™ä¸ªåˆ†å¸ƒæ˜¯ï¼ˆç‹¬ç«‹çš„ã€å¤šå…ƒçš„ï¼‰æ­£æ€åˆ†å¸ƒã€‚</p><p>ä¸ºä»€ä¹ˆè¦å¼ºè°ƒâ€œä¸“å±â€å‘¢ï¼Ÿå› ä¸ºæˆ‘ä»¬åé¢è¦è®­ç»ƒä¸€ä¸ªç”Ÿæˆå™¨ <em>X</em>=<em>g</em>(<em>Z</em>)ï¼Œå¸Œæœ›èƒ½å¤ŸæŠŠä»åˆ†å¸ƒ <em>p</em>(<em>Z</em>|<em>Xk</em>) é‡‡æ ·å‡ºæ¥çš„ä¸€ä¸ª <em>Zk</em> è¿˜åŸä¸º <em>Xk</em>ã€‚</p><p>å¦‚æœå‡è®¾ <em>p</em>(<em>Z</em>) æ˜¯æ­£æ€åˆ†å¸ƒï¼Œç„¶åä» <em>p</em>(<em>Z</em>) ä¸­é‡‡æ ·ä¸€ä¸ª <em>Z</em>ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆçŸ¥é“è¿™ä¸ª <em>Z</em> å¯¹åº”äºå“ªä¸ªçœŸå®çš„ <em>X</em> å‘¢ï¼Ÿ<strong>ç°åœ¨</strong> <strong>p(Z|Xk) ä¸“å±äº</strong> <strong>Xkï¼Œæˆ‘ä»¬æœ‰ç†ç”±è¯´ä»è¿™ä¸ªåˆ†å¸ƒé‡‡æ ·å‡ºæ¥çš„</strong> <strong><em>Z</em></strong> <strong>åº”è¯¥è¦è¿˜åŸåˆ°Xk</strong> <strong>ä¸­å»</strong>ã€‚</p><p>äº‹å®ä¸Šï¼Œåœ¨è®ºæ–‡ <em>Auto-Encoding Variational Bayes</em> çš„åº”ç”¨éƒ¨åˆ†ï¼Œä¹Ÿç‰¹åˆ«å¼ºè°ƒäº†è¿™ä¸€ç‚¹ï¼š</p><blockquote><p><em>In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:</em></p></blockquote><p><img src="../img/008-å˜åˆ†è‡ªç¼–ç å™¨VAE-åŸæ¥æ˜¯è¿™ä¹ˆä¸€å›äº‹/v2-241c1d29c3c8ca890bb27fa7abb98ed6_hd.jpg" alt="img"></p><p>è®ºæ–‡ä¸­çš„å¼ (9) æ˜¯å®ç°æ•´ä¸ªæ¨¡å‹çš„å…³é”®ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆå¾ˆå¤šæ•™ç¨‹åœ¨ä»‹ç» VAE æ—¶éƒ½æ²¡æœ‰æŠŠå®ƒå‡¸æ˜¾å‡ºæ¥ã€‚å°½ç®¡è®ºæ–‡ä¹Ÿæåˆ° <em>p</em>(<em>Z</em>) æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œç„¶è€Œé‚£å…¶å®å¹¶ä¸æ˜¯æœ¬è´¨é‡è¦çš„ã€‚</p><p>å†æ¬¡å¼ºè°ƒï¼Œè¿™æ—¶å€™æ¯ä¸€ä¸ª $X_k$ éƒ½é…ä¸Šäº†ä¸€ä¸ªä¸“å±çš„æ­£æ€åˆ†å¸ƒï¼Œæ‰æ–¹ä¾¿åé¢çš„ç”Ÿæˆå™¨åšè¿˜åŸã€‚ä½†è¿™æ ·æœ‰å¤šå°‘ä¸ª <em>X</em> å°±æœ‰å¤šå°‘ä¸ªæ­£æ€åˆ†å¸ƒäº†ã€‚æˆ‘ä»¬çŸ¥é“æ­£æ€åˆ†å¸ƒæœ‰ä¸¤ç»„å‚æ•°ï¼šå‡å€¼ $Î¼$ å’Œæ–¹å·® $Ïƒ^2$ï¼ˆå¤šå…ƒçš„è¯ï¼Œå®ƒä»¬éƒ½æ˜¯å‘é‡ï¼‰ã€‚</p><p><strong>é‚£æˆ‘æ€ä¹ˆæ‰¾å‡ºä¸“å±äº</strong> $X_k$ <strong>çš„æ­£æ€åˆ†å¸ƒ</strong> $p(Z|X_k) $çš„å‡å€¼å’Œæ–¹å·®å‘¢ï¼Ÿå¥½åƒå¹¶æ²¡æœ‰ä»€ä¹ˆç›´æ¥çš„æ€è·¯ã€‚</p><p>é‚£å¥½å§ï¼Œ<strong>æˆ‘å°±ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆå‡ºæ¥</strong>ã€‚è¿™å°±æ˜¯ç¥ç»ç½‘ç»œæ—¶ä»£çš„å“²å­¦ï¼šéš¾ç®—çš„æˆ‘ä»¬éƒ½ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆï¼Œåœ¨ WGAN é‚£é‡Œæˆ‘ä»¬å·²ç»ä½“éªŒè¿‡ä¸€æ¬¡äº†ï¼Œç°åœ¨å†æ¬¡ä½“éªŒåˆ°äº†ã€‚</p><p>äºæ˜¯æˆ‘ä»¬æ„å»ºä¸¤ä¸ªç¥ç»ç½‘ç»œ $Î¼_k=f_1(X_k)$ï¼Œ$logÏƒ^2=f_2(X_k)$ æ¥ç®—å®ƒä»¬äº†ã€‚æˆ‘ä»¬é€‰æ‹©æ‹Ÿåˆ$ logÏƒ^2$ è€Œä¸æ˜¯ç›´æ¥æ‹Ÿåˆ $Ïƒ^2$ï¼Œæ˜¯å› ä¸º $Ïƒ^2$ æ€»æ˜¯éè´Ÿçš„ï¼Œéœ€è¦åŠ æ¿€æ´»å‡½æ•°å¤„ç†ï¼Œè€Œæ‹Ÿåˆ $logÏƒ^2$ ä¸éœ€è¦åŠ æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºå®ƒå¯æ­£å¯è´Ÿã€‚</p><p>åˆ°è¿™é‡Œï¼Œæˆ‘èƒ½çŸ¥é“ä¸“å±äº $ X_k$ çš„å‡å€¼å’Œæ–¹å·®äº†ï¼Œä¹Ÿå°±çŸ¥é“å®ƒçš„æ­£æ€åˆ†å¸ƒé•¿ä»€ä¹ˆæ ·äº†ï¼Œç„¶åä»è¿™ä¸ªä¸“å±åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ª $Z_k$ å‡ºæ¥ï¼Œç„¶åç»è¿‡ä¸€ä¸ªç”Ÿæˆå™¨å¾—åˆ° <em>XÌ‚k</em>=<em>g</em>(<em>Zk</em>)ã€‚</p><p>ç°åœ¨æˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°æœ€å°åŒ– <em>D</em>(<em>XÌ‚k</em>,<em>Xk</em>)^2ï¼Œå› ä¸º <em>Zk</em> æ˜¯ä»ä¸“å± <em>Xk</em> çš„åˆ†å¸ƒä¸­é‡‡æ ·å‡ºæ¥çš„ï¼Œè¿™ä¸ªç”Ÿæˆå™¨åº”è¯¥è¦æŠŠå¼€å§‹çš„ <em>Xk</em> è¿˜åŸå›æ¥ã€‚<strong>äºæ˜¯å¯ä»¥ç”»å‡º VAE çš„ç¤ºæ„å›¾ï¼š</strong></p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-36c7da0b2fe37bd021699532a2cff1e8_hd.jpg" alt="img"></p><p>äº‹å®ä¸Šï¼ŒVAE æ˜¯ä¸ºæ¯ä¸ªæ ·æœ¬æ„é€ ä¸“å±çš„æ­£æ€åˆ†å¸ƒï¼Œç„¶åé‡‡æ ·æ¥é‡æ„ã€‚</p><p><strong>åˆ†å¸ƒæ ‡å‡†åŒ–</strong></p><p>è®©æˆ‘ä»¬æ¥æ€è€ƒä¸€ä¸‹ï¼Œæ ¹æ®ä¸Šå›¾çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæœ€ç»ˆä¼šå¾—åˆ°ä»€ä¹ˆç»“æœã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬å¸Œæœ›é‡æ„ <em>X</em>ï¼Œä¹Ÿå°±æ˜¯æœ€å°åŒ– <em>D</em>(<em>XÌ‚k</em>,<em>Xk</em>)^2ï¼Œä½†æ˜¯è¿™ä¸ªé‡æ„è¿‡ç¨‹å—åˆ°å™ªå£°çš„å½±å“ï¼Œå› ä¸º<em>Zk</em> æ˜¯é€šè¿‡é‡æ–°é‡‡æ ·è¿‡çš„ï¼Œä¸æ˜¯ç›´æ¥ç”± encoder ç®—å‡ºæ¥çš„ã€‚</p><p>æ˜¾ç„¶å™ªå£°ä¼šå¢åŠ é‡æ„çš„éš¾åº¦ï¼Œä¸è¿‡å¥½åœ¨è¿™ä¸ªå™ªå£°å¼ºåº¦ï¼ˆä¹Ÿå°±æ˜¯æ–¹å·®ï¼‰é€šè¿‡ä¸€ä¸ªç¥ç»ç½‘ç»œç®—å‡ºæ¥çš„ï¼Œæ‰€ä»¥æœ€ç»ˆæ¨¡å‹ä¸ºäº†é‡æ„å¾—æ›´å¥½ï¼Œè‚¯å®šä¼šæƒ³å°½åŠæ³•è®©æ–¹å·®ä¸º0ã€‚</p><p>è€Œæ–¹å·®ä¸º 0 çš„è¯ï¼Œä¹Ÿå°±æ²¡æœ‰éšæœºæ€§äº†ï¼Œæ‰€ä»¥ä¸ç®¡æ€ä¹ˆé‡‡æ ·å…¶å®éƒ½åªæ˜¯å¾—åˆ°ç¡®å®šçš„ç»“æœï¼ˆä¹Ÿå°±æ˜¯å‡å€¼ï¼‰ï¼Œåªæ‹Ÿåˆä¸€ä¸ªå½“ç„¶æ¯”æ‹Ÿåˆå¤šä¸ªè¦å®¹æ˜“ï¼Œè€Œå‡å€¼æ˜¯é€šè¿‡å¦å¤–ä¸€ä¸ªç¥ç»ç½‘ç»œç®—å‡ºæ¥çš„ã€‚</p><p>è¯´ç™½äº†ï¼Œ<strong>æ¨¡å‹ä¼šæ…¢æ…¢é€€åŒ–æˆæ™®é€šçš„ AutoEncoderï¼Œå™ªå£°ä¸å†èµ·ä½œç”¨</strong>ã€‚</p><p>è¿™æ ·ä¸å°±ç™½è´¹åŠ›æ°”äº†å—ï¼Ÿè¯´å¥½çš„ç”Ÿæˆæ¨¡å‹å‘¢ï¼Ÿ</p><p>åˆ«æ€¥åˆ«æ€¥ï¼Œ<strong>å…¶å® VAE è¿˜è®©æ‰€æœ‰çš„</strong> <strong>p(Z|X) éƒ½å‘æ ‡å‡†æ­£æ€åˆ†å¸ƒçœ‹é½</strong>ï¼Œè¿™æ ·å°±é˜²æ­¢äº†å™ªå£°ä¸ºé›¶ï¼ŒåŒæ—¶ä¿è¯äº†æ¨¡å‹å…·æœ‰ç”Ÿæˆèƒ½åŠ›ã€‚</p><p>æ€ä¹ˆç†è§£â€œä¿è¯äº†ç”Ÿæˆèƒ½åŠ›â€å‘¢ï¼Ÿå¦‚æœæ‰€æœ‰çš„ <em>p</em>(<em>Z</em>|<em>X</em>) éƒ½å¾ˆæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ <em>N</em>(0,<em>I</em>)ï¼Œé‚£ä¹ˆæ ¹æ®å®šä¹‰ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-e162ff5ea26838c18314a351ca166427_hd.jpg" alt="img"></p><p>è¿™æ ·æˆ‘ä»¬å°±èƒ½è¾¾åˆ°æˆ‘ä»¬çš„å…ˆéªŒå‡è®¾ï¼š$p(Z)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚ç„¶åæˆ‘ä»¬å°±å¯ä»¥æ”¾å¿ƒåœ°ä» $N(0,I)$ ä¸­é‡‡æ ·æ¥ç”Ÿæˆå›¾åƒäº†ã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-a3f264a40db57e010b7ebf0253198726_hd.jpg" alt="img"></p><p>ä¸ºäº†ä½¿æ¨¡å‹å…·æœ‰ç”Ÿæˆèƒ½åŠ›ï¼ŒVAE è¦æ±‚æ¯ä¸ª p(Z_X) éƒ½å‘æ­£æ€åˆ†å¸ƒçœ‹é½ã€‚</p><p>é‚£æ€ä¹ˆè®©æ‰€æœ‰çš„ <em>p</em>(<em>Z</em>|<em>X</em>) éƒ½å‘ <em>N</em>(0,<em>I</em>) çœ‹é½å‘¢ï¼Ÿå¦‚æœæ²¡æœ‰å¤–éƒ¨çŸ¥è¯†çš„è¯ï¼Œå…¶å®æœ€ç›´æ¥çš„æ–¹æ³•åº”è¯¥æ˜¯åœ¨é‡æ„è¯¯å·®çš„åŸºç¡€ä¸Šä¸­åŠ å…¥é¢å¤–çš„ lossï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-18ab0eeeeaf3d1adffa235e995521255_hd.jpg" alt="img"></p><p>å› ä¸ºå®ƒä»¬åˆ†åˆ«ä»£è¡¨äº†å‡å€¼ <em>Î¼k</em> å’Œæ–¹å·®çš„å¯¹æ•° log<em>Ïƒ</em>^2ï¼Œè¾¾åˆ° <em>N</em>(0,<em>I</em>) å°±æ˜¯å¸Œæœ›äºŒè€…å°½é‡æ¥è¿‘äº 0 äº†ã€‚ä¸è¿‡ï¼Œè¿™åˆä¼šé¢ä¸´ç€è¿™ä¸¤ä¸ªæŸå¤±çš„æ¯”ä¾‹è¦æ€ä¹ˆé€‰å–çš„é—®é¢˜ï¼Œé€‰å–å¾—ä¸å¥½ï¼Œç”Ÿæˆçš„å›¾åƒä¼šæ¯”è¾ƒæ¨¡ç³Šã€‚</p><p>æ‰€ä»¥ï¼ŒåŸè®ºæ–‡ç›´æ¥ç®—äº†ä¸€èˆ¬ï¼ˆå„åˆ†é‡ç‹¬ç«‹çš„ï¼‰æ­£æ€åˆ†å¸ƒä¸æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ KL æ•£åº¦<em>KL</em>(<em>N</em>(<em>Î¼</em>,<em>Ïƒ</em>^2)â€–<em>N</em>(0,<em>I</em>))ä½œä¸ºè¿™ä¸ªé¢å¤–çš„ lossï¼Œè®¡ç®—ç»“æœä¸ºï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-08091fc4fa1460c9fa611cfcf0608105_hd.jpg" alt="img"></p><p>è¿™é‡Œçš„ <em>d</em> æ˜¯éšå˜é‡ <em>Z</em> çš„ç»´åº¦ï¼Œè€Œ <em>Î¼</em>(<em>i</em>) å’Œ Ïƒ_{(i)}^{2} åˆ†åˆ«ä»£è¡¨ä¸€èˆ¬æ­£æ€åˆ†å¸ƒçš„å‡å€¼å‘é‡å’Œæ–¹å·®å‘é‡çš„ç¬¬ <em>i</em> ä¸ªåˆ†é‡ã€‚ç›´æ¥ç”¨è¿™ä¸ªå¼å­åšè¡¥å…… lossï¼Œå°±ä¸ç”¨è€ƒè™‘å‡å€¼æŸå¤±å’Œæ–¹å·®æŸå¤±çš„ç›¸å¯¹æ¯”ä¾‹é—®é¢˜äº†ã€‚</p><p>æ˜¾ç„¶ï¼Œè¿™ä¸ª loss ä¹Ÿå¯ä»¥åˆ†ä¸¤éƒ¨åˆ†ç†è§£ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-af1049578e84eddf1c817422aa8a3bbf_hd.jpg" alt="img"></p><p><strong>æ¨å¯¼</strong></p><p>ç”±äºæˆ‘ä»¬è€ƒè™‘çš„æ˜¯å„åˆ†é‡ç‹¬ç«‹çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼Œå› æ­¤åªéœ€è¦æ¨å¯¼ä¸€å…ƒæ­£æ€åˆ†å¸ƒçš„æƒ…å½¢å³å¯ï¼Œæ ¹æ®å®šä¹‰æˆ‘ä»¬å¯ä»¥å†™å‡ºï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-7a3c7ea64e7f11c475cf35cd44fa3ca2_hd.jpg" alt="img"></p><p>æ•´ä¸ªç»“æœåˆ†ä¸ºä¸‰é¡¹ç§¯åˆ†ï¼Œç¬¬ä¸€é¡¹å®é™…ä¸Šå°±æ˜¯ âˆ’log<em>Ïƒ^</em>2 ä¹˜ä»¥æ¦‚ç‡å¯†åº¦çš„ç§¯åˆ†ï¼ˆä¹Ÿå°±æ˜¯ 1ï¼‰ï¼Œæ‰€ä»¥ç»“æœæ˜¯ âˆ’log<em>Ïƒ^</em>2ï¼›ç¬¬äºŒé¡¹å®é™…æ˜¯æ­£æ€åˆ†å¸ƒçš„äºŒé˜¶çŸ©ï¼Œç†Ÿæ‚‰æ­£æ€åˆ†å¸ƒçš„æœ‹å‹åº”è¯¥éƒ½æ¸…æ¥šæ­£æ€åˆ†å¸ƒçš„äºŒé˜¶çŸ©ä¸º <em>Î¼</em>^2+<em>Ïƒ</em>^2ï¼›è€Œæ ¹æ®å®šä¹‰ï¼Œç¬¬ä¸‰é¡¹å®é™…ä¸Šå°±æ˜¯â€œ-æ–¹å·®é™¤ä»¥æ–¹å·®=-1â€ã€‚æ‰€ä»¥æ€»ç»“æœå°±æ˜¯ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-603cd66d01ad6bac42ac1d2a38bad61f_hd.jpg" alt="img"></p><p><strong>é‡å‚æ•°æŠ€å·§</strong></p><p>æœ€åæ˜¯å®ç°æ¨¡å‹çš„ä¸€ä¸ªæŠ€å·§ï¼Œè‹±æ–‡åæ˜¯ Reparameterization Trickï¼Œæˆ‘è¿™é‡Œå«å®ƒåšé‡å‚æ•°å§ã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-2952f780b506319c0911ab4297440079_hd.jpg" alt="img"></p><p><strong>â–²</strong> é‡å‚æ•°æŠ€å·§</p><p>å…¶å®å¾ˆç®€å•ï¼Œå°±æ˜¯æˆ‘ä»¬è¦ä» <em>p</em>(<em>Z</em>|<em>Xk</em>) ä¸­é‡‡æ ·ä¸€ä¸ª <em>Zk</em> å‡ºæ¥ï¼Œå°½ç®¡æˆ‘ä»¬çŸ¥é“äº† <em>p</em>(<em>Z</em>|<em>Xk</em>) æ˜¯æ­£æ€åˆ†å¸ƒï¼Œä½†æ˜¯å‡å€¼æ–¹å·®éƒ½æ˜¯é æ¨¡å‹ç®—å‡ºæ¥çš„ï¼Œæˆ‘ä»¬è¦é è¿™ä¸ªè¿‡ç¨‹åè¿‡æ¥ä¼˜åŒ–å‡å€¼æ–¹å·®çš„æ¨¡å‹ï¼Œä½†æ˜¯â€œé‡‡æ ·â€è¿™ä¸ªæ“ä½œæ˜¯ä¸å¯å¯¼çš„ï¼Œè€Œé‡‡æ ·çš„ç»“æœæ˜¯å¯å¯¼çš„ï¼Œäºæ˜¯æˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªäº‹å®ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-39d484abe79242a398d6f57ee3d7dc04_hd.jpg" alt="img"></p><p>æ‰€ä»¥ï¼Œæˆ‘ä»¬å°†ä» <em>N</em>(<em>Î¼</em>,<em>Ïƒ</em>^2) é‡‡æ ·å˜æˆäº†ä» <em>N</em>(<em>Î¼</em>,<em>Ïƒ</em>^2) ä¸­é‡‡æ ·ï¼Œç„¶åé€šè¿‡å‚æ•°å˜æ¢å¾—åˆ°ä»<em>N</em>(<em>Î¼</em>,<em>Ïƒ</em>^2) ä¸­é‡‡æ ·çš„ç»“æœã€‚è¿™æ ·ä¸€æ¥ï¼Œâ€œé‡‡æ ·â€è¿™ä¸ªæ“ä½œå°±ä¸ç”¨å‚ä¸æ¢¯åº¦ä¸‹é™äº†ï¼Œæ”¹ä¸ºé‡‡æ ·çš„ç»“æœå‚ä¸ï¼Œä½¿å¾—æ•´ä¸ªæ¨¡å‹å¯è®­ç»ƒäº†ã€‚</p><p>å…·ä½“æ€ä¹ˆå®ç°ï¼Œå¤§å®¶æŠŠä¸Šè¿°æ–‡å­—å¯¹ç…§ç€ä»£ç çœ‹ä¸€ä¸‹ï¼Œä¸€ä¸‹å­å°±æ˜ç™½äº†ã€‚</p><h2 id="åç»­åˆ†æ"><a href="#åç»­åˆ†æ" class="headerlink" title="åç»­åˆ†æ"></a><strong>åç»­åˆ†æ</strong></h2><p>å³ä¾¿æŠŠä¸Šé¢çš„æ‰€æœ‰å†…å®¹éƒ½ææ¸…æ¥šäº†ï¼Œé¢å¯¹ VAEï¼Œæˆ‘ä»¬å¯èƒ½è¿˜å­˜æœ‰å¾ˆå¤šç–‘é—®ã€‚</p><p><strong>æœ¬è´¨æ˜¯ä»€ä¹ˆ</strong></p><p>VAE çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼ŸVAE è™½ç„¶ä¹Ÿç§°æ˜¯ AEï¼ˆAutoEncoderï¼‰çš„ä¸€ç§ï¼Œä½†å®ƒçš„åšæ³•ï¼ˆæˆ–è€…è¯´å®ƒå¯¹ç½‘ç»œçš„è¯ é‡Šï¼‰æ˜¯åˆ«å…·ä¸€æ ¼çš„ã€‚</p><p>åœ¨ VAE ä¸­ï¼Œå®ƒçš„ Encoder æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªç”¨æ¥è®¡ç®—å‡å€¼ï¼Œä¸€ä¸ªç”¨æ¥è®¡ç®—æ–¹å·®ï¼Œè¿™å·²ç»è®©äººæ„å¤–äº†ï¼šEncoder ä¸æ˜¯ç”¨æ¥ Encode çš„ï¼Œæ˜¯ç”¨æ¥ç®—å‡å€¼å’Œæ–¹å·®çš„ï¼Œè¿™çœŸæ˜¯å¤§æ–°é—»äº†ï¼Œè¿˜æœ‰å‡å€¼å’Œæ–¹å·®ä¸éƒ½æ˜¯ç»Ÿè®¡é‡å—ï¼Œæ€ä¹ˆæ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥ç®—çš„ï¼Ÿ</p><p>äº‹å®ä¸Šï¼Œæˆ‘è§‰å¾— <strong>VAE ä»è®©æ™®é€šäººæœ›è€Œç”Ÿç•çš„å˜åˆ†å’Œè´å¶æ–¯ç†è®ºå‡ºå‘ï¼Œæœ€åè½åœ°åˆ°ä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ä¸­</strong>ï¼Œè™½ç„¶èµ°äº†æ¯”è¾ƒé•¿çš„ä¸€æ®µè·¯ï¼Œä½†æœ€ç»ˆçš„æ¨¡å‹å…¶å®æ˜¯å¾ˆæ¥åœ°æ°”çš„ã€‚</p><p><strong>å®ƒæœ¬è´¨ä¸Šå°±æ˜¯åœ¨æˆ‘ä»¬å¸¸è§„çš„è‡ªç¼–ç å™¨çš„åŸºç¡€ä¸Šï¼Œå¯¹ encoder çš„ç»“æœï¼ˆåœ¨VAEä¸­å¯¹åº”ç€è®¡ç®—å‡å€¼çš„ç½‘ç»œï¼‰åŠ ä¸Šäº†â€œé«˜æ–¯å™ªå£°â€ï¼Œä½¿å¾—ç»“æœ decoder èƒ½å¤Ÿå¯¹å™ªå£°æœ‰é²æ£’æ€§ï¼›è€Œé‚£ä¸ªé¢å¤–çš„ KL lossï¼ˆç›®çš„æ˜¯è®©å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼‰ï¼Œäº‹å®ä¸Šå°±æ˜¯ç›¸å½“äºå¯¹ encoder çš„ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œå¸Œæœ› encoder å‡ºæ¥çš„ä¸œè¥¿å‡æœ‰é›¶å‡å€¼ã€‚</strong></p><p>é‚£å¦å¤–ä¸€ä¸ª encoderï¼ˆå¯¹åº”ç€è®¡ç®—æ–¹å·®çš„ç½‘ç»œï¼‰çš„ä½œç”¨å‘¢ï¼Ÿå®ƒæ˜¯ç”¨æ¥<strong>åŠ¨æ€è°ƒèŠ‚å™ªå£°çš„å¼ºåº¦</strong>çš„ã€‚</p><p>ç›´è§‰ä¸Šæ¥æƒ³ï¼Œ<strong>å½“ decoder è¿˜æ²¡æœ‰è®­ç»ƒå¥½æ—¶ï¼ˆé‡æ„è¯¯å·®è¿œå¤§äº KL lossï¼‰ï¼Œå°±ä¼šé€‚å½“é™ä½å™ªå£°ï¼ˆKL loss å¢åŠ ï¼‰ï¼Œä½¿å¾—æ‹Ÿåˆèµ·æ¥å®¹æ˜“ä¸€äº›ï¼ˆé‡æ„è¯¯å·®å¼€å§‹ä¸‹é™ï¼‰</strong>ã€‚</p><p>åä¹‹ï¼Œ<strong>å¦‚æœ decoder è®­ç»ƒå¾—è¿˜ä¸é”™æ—¶ï¼ˆé‡æ„è¯¯å·®å°äº KL lossï¼‰ï¼Œè¿™æ—¶å€™å™ªå£°å°±ä¼šå¢åŠ ï¼ˆKL loss å‡å°‘ï¼‰ï¼Œä½¿å¾—æ‹Ÿåˆæ›´åŠ å›°éš¾äº†ï¼ˆé‡æ„è¯¯å·®åˆå¼€å§‹å¢åŠ ï¼‰ï¼Œè¿™æ—¶å€™ decoder å°±è¦æƒ³åŠæ³•æé«˜å®ƒçš„ç”Ÿæˆèƒ½åŠ›äº†</strong>ã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-784891edddff506ea1670c81767e993c_hd.jpg" alt="img"></p><p><strong>â–²</strong> VAEçš„æœ¬è´¨ç»“æ„</p><p>è¯´ç™½äº†ï¼Œ<strong>é‡æ„çš„è¿‡ç¨‹æ˜¯å¸Œæœ›æ²¡å™ªå£°çš„ï¼Œè€Œ KL loss åˆ™å¸Œæœ›æœ‰é«˜æ–¯å™ªå£°çš„ï¼Œä¸¤è€…æ˜¯å¯¹ç«‹çš„ã€‚æ‰€ä»¥ï¼ŒVAE è·Ÿ GAN ä¸€æ ·ï¼Œå†…éƒ¨å…¶å®æ˜¯åŒ…å«äº†ä¸€ä¸ªå¯¹æŠ—çš„è¿‡ç¨‹ï¼Œåªä¸è¿‡å®ƒä»¬ä¸¤è€…æ˜¯æ··åˆèµ·æ¥ï¼Œå…±åŒè¿›åŒ–çš„</strong>ã€‚</p><p>ä»è¿™ä¸ªè§’åº¦çœ‹ï¼ŒVAE çš„æ€æƒ³ä¼¼ä¹è¿˜é«˜æ˜ä¸€äº›ï¼Œå› ä¸ºåœ¨ GAN ä¸­ï¼Œé€ å‡è€…åœ¨è¿›åŒ–æ—¶ï¼Œé‰´åˆ«è€…æ˜¯å®‰ç„¶ä¸åŠ¨çš„ï¼Œåä¹‹äº¦ç„¶ã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯ä¸€ä¸ªä¾§é¢ï¼Œä¸èƒ½è¯´æ˜ VAE å°±æ¯” GAN å¥½ã€‚</p><p>GAN çœŸæ­£é«˜æ˜çš„åœ°æ–¹æ˜¯ï¼šå®ƒè¿åº¦é‡éƒ½ç›´æ¥è®­ç»ƒå‡ºæ¥äº†ï¼Œè€Œä¸”è¿™ä¸ªåº¦é‡å¾€å¾€æ¯”æˆ‘ä»¬äººå·¥æƒ³çš„è¦å¥½ï¼ˆç„¶è€Œ GAN æœ¬èº«ä¹Ÿæœ‰å„ç§é—®é¢˜ï¼Œè¿™å°±ä¸å±•å¼€äº†ï¼‰ã€‚</p><p><strong>æ­£æ€åˆ†å¸ƒï¼Ÿ</strong></p><p>å¯¹äº <em>p</em>(<em>Z</em>|<em>X</em>) çš„åˆ†å¸ƒï¼Œè¯»è€…å¯èƒ½ä¼šæœ‰ç–‘æƒ‘ï¼šæ˜¯ä¸æ˜¯å¿…é¡»é€‰æ‹©æ­£æ€åˆ†å¸ƒï¼Ÿå¯ä»¥é€‰æ‹©å‡åŒ€åˆ†å¸ƒå—ï¼Ÿ</p><p>é¦–å…ˆï¼Œè¿™ä¸ªæœ¬èº«æ˜¯ä¸€ä¸ªå®éªŒé—®é¢˜ï¼Œä¸¤ç§åˆ†å¸ƒéƒ½è¯•ä¸€ä¸‹å°±çŸ¥é“äº†ã€‚ä½†æ˜¯ä»ç›´è§‰ä¸Šæ¥è®²ï¼Œæ­£æ€åˆ†å¸ƒè¦æ¯”å‡åŒ€åˆ†å¸ƒæ›´åŠ åˆç†ï¼Œå› ä¸ºæ­£æ€åˆ†å¸ƒæœ‰ä¸¤ç»„ç‹¬ç«‹çš„å‚æ•°ï¼šå‡å€¼å’Œæ–¹å·®ï¼Œè€Œå‡åŒ€åˆ†å¸ƒåªæœ‰ä¸€ç»„ã€‚</p><p>å‰é¢æˆ‘ä»¬è¯´ï¼Œ<strong>åœ¨ VAE ä¸­ï¼Œé‡æ„è·Ÿå™ªå£°æ˜¯ç›¸äº’å¯¹æŠ—çš„ï¼Œé‡æ„è¯¯å·®è·Ÿå™ªå£°å¼ºåº¦æ˜¯ä¸¤ä¸ªç›¸äº’å¯¹æŠ—çš„æŒ‡æ ‡ï¼Œè€Œåœ¨æ”¹å˜å™ªå£°å¼ºåº¦æ—¶åŸåˆ™ä¸Šéœ€è¦æœ‰ä¿æŒå‡å€¼ä¸å˜çš„èƒ½åŠ›ï¼Œä¸ç„¶æˆ‘ä»¬å¾ˆéš¾ç¡®å®šé‡æ„è¯¯å·®å¢å¤§äº†ï¼Œç©¶ç«Ÿæ˜¯å‡å€¼å˜åŒ–äº†ï¼ˆencoderçš„é”…ï¼‰è¿˜æ˜¯æ–¹å·®å˜å¤§äº†ï¼ˆå™ªå£°çš„é”…ï¼‰</strong>ã€‚</p><p>è€Œå‡åŒ€åˆ†å¸ƒä¸èƒ½åšåˆ°ä¿æŒå‡å€¼ä¸å˜çš„æƒ…å†µä¸‹æ”¹å˜æ–¹å·®ï¼Œæ‰€ä»¥æ­£æ€åˆ†å¸ƒåº”è¯¥æ›´åŠ åˆç†ã€‚</p><p><strong>å˜åˆ†åœ¨å“ªé‡Œ</strong></p><p>è¿˜æœ‰ä¸€ä¸ªæœ‰æ„æ€ï¼ˆä½†ä¸å¤§é‡è¦ï¼‰çš„é—®é¢˜æ˜¯ï¼šVAE å«åšâ€œå˜åˆ†è‡ªç¼–ç å™¨â€ï¼Œå®ƒè·Ÿå˜åˆ†æ³•æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿåœ¨VAE çš„è®ºæ–‡å’Œç›¸å…³è§£è¯»ä¸­ï¼Œå¥½åƒä¹Ÿæ²¡çœ‹åˆ°å˜åˆ†æ³•çš„å­˜åœ¨ï¼Ÿ</p><p>å…¶å®å¦‚æœè¯»è€…å·²ç»æ‰¿è®¤äº† KL æ•£åº¦çš„è¯ï¼Œé‚£ VAE å¥½åƒçœŸçš„è·Ÿå˜åˆ†æ²¡å¤šå¤§å…³ç³»äº†ï¼Œå› ä¸º KL æ•£åº¦çš„å®šä¹‰æ˜¯ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-5499a0f15ebef578e25122650a510f2c_hd.jpg" alt="img"></p><p>å¦‚æœæ˜¯ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒå°±è¦å†™æˆæ±‚å’Œï¼Œæˆ‘ä»¬è¦è¯æ˜ï¼š<strong>å·²æ¦‚ç‡åˆ†å¸ƒ</strong> <strong>p(x)ï¼ˆæˆ–å›ºå®šq(x)ï¼‰çš„æƒ…å†µä¸‹ï¼Œå¯¹äºä»»æ„çš„æ¦‚ç‡åˆ†å¸ƒ q(x)ï¼ˆæˆ–</strong> <strong>p(x)ï¼‰ï¼Œéƒ½æœ‰</strong> <strong>KLp(x)â€–q(x))â‰¥0ï¼Œè€Œä¸”åªæœ‰å½“p(x)=q(x)æ—¶æ‰ç­‰äºé›¶</strong>ã€‚</p><p>å› ä¸º <em>KL</em>(<em>p</em>(<em>x</em>)â€–<em>q</em>(<em>x</em>))å®é™…ä¸Šæ˜¯ä¸€ä¸ªæ³›å‡½ï¼Œè¦å¯¹æ³›å‡½æ±‚æå€¼å°±è¦ç”¨åˆ°å˜åˆ†æ³•ï¼Œå½“ç„¶ï¼Œè¿™é‡Œçš„å˜åˆ†æ³•åªæ˜¯æ™®é€šå¾®ç§¯åˆ†çš„å¹³è¡Œæ¨å¹¿ï¼Œè¿˜æ²¡æ¶‰åŠåˆ°çœŸæ­£å¤æ‚çš„å˜åˆ†æ³•ã€‚è€Œ VAE çš„å˜åˆ†ä¸‹ç•Œï¼Œæ˜¯ç›´æ¥åŸºäº KL æ•£åº¦å°±å¾—åˆ°çš„ã€‚æ‰€ä»¥ç›´æ¥æ‰¿è®¤äº† KL æ•£åº¦çš„è¯ï¼Œå°±æ²¡æœ‰å˜åˆ†çš„ä»€ä¹ˆäº‹äº†ã€‚</p><p>ä¸€å¥è¯ï¼ŒVAE çš„åå­—ä¸­â€œå˜åˆ†â€ï¼Œæ˜¯å› ä¸ºå®ƒçš„æ¨å¯¼è¿‡ç¨‹ç”¨åˆ°äº† KL æ•£åº¦åŠå…¶æ€§è´¨ã€‚</p><p><strong>æ¡ä»¶VAE</strong></p><p>æœ€åï¼Œå› ä¸ºç›®å‰çš„ VAE æ˜¯æ— ç›‘ç£è®­ç»ƒçš„ï¼Œå› æ­¤å¾ˆè‡ªç„¶æƒ³åˆ°ï¼šå¦‚æœæœ‰æ ‡ç­¾æ•°æ®ï¼Œé‚£ä¹ˆèƒ½ä¸èƒ½æŠŠæ ‡ç­¾ä¿¡æ¯åŠ è¿›å»è¾…åŠ©ç”Ÿæˆæ ·æœ¬å‘¢ï¼Ÿ</p><p>è¿™ä¸ªé—®é¢˜çš„æ„å›¾ï¼Œå¾€å¾€æ˜¯å¸Œæœ›èƒ½å¤Ÿå®ç°æ§åˆ¶æŸä¸ªå˜é‡æ¥å®ç°ç”ŸæˆæŸä¸€ç±»å›¾åƒã€‚å½“ç„¶ï¼Œè¿™æ˜¯è‚¯å®šå¯ä»¥çš„ï¼Œæˆ‘ä»¬æŠŠè¿™ç§æƒ…å†µå«åš <strong>Conditional VAE</strong>ï¼Œæˆ–è€…å« CVAEï¼ˆç›¸åº”åœ°ï¼Œåœ¨ GAN ä¸­æˆ‘ä»¬ä¹Ÿæœ‰ä¸ª CGANï¼‰ã€‚</p><p>ä½†æ˜¯ï¼ŒCVAE ä¸æ˜¯ä¸€ä¸ªç‰¹å®šçš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ç±»æ¨¡å‹ï¼Œæ€»ä¹‹å°±æ˜¯æŠŠæ ‡ç­¾ä¿¡æ¯èå…¥åˆ° VAE ä¸­çš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œç›®çš„ä¹Ÿä¸ä¸€æ ·ã€‚è¿™é‡ŒåŸºäºå‰é¢çš„è®¨è®ºï¼Œç»™å‡ºä¸€ç§éå¸¸ç®€å•çš„ VAEã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-d148bf520bb386c0c4bb11756e4798ee_hd.jpg" alt="img"></p><p><strong>â–²</strong> ä¸€ä¸ªç®€å•çš„CVAEç»“æ„</p><p>åœ¨å‰é¢çš„è®¨è®ºä¸­ï¼Œæˆ‘ä»¬å¸Œæœ› <em>X</em> ç»è¿‡ç¼–ç åï¼Œ<em>Z</em> çš„åˆ†å¸ƒéƒ½å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œè¿™ä¸ªâ€œå¸Œæœ›â€æ˜¯é€šè¿‡åŠ å…¥äº† KL loss æ¥å®ç°çš„ã€‚</p><p>å¦‚æœç°åœ¨å¤šäº†ç±»åˆ«ä¿¡æ¯ <em>Y</em>ï¼Œ<strong>æˆ‘ä»¬å¯ä»¥å¸Œæœ›åŒä¸€ä¸ªç±»çš„æ ·æœ¬éƒ½æœ‰ä¸€ä¸ªä¸“å±çš„å‡å€¼</strong> <strong>Î¼^Yï¼ˆæ–¹å·®ä¸å˜ï¼Œè¿˜æ˜¯å•ä½æ–¹å·®ï¼‰ï¼Œè¿™ä¸ª</strong> <strong><em>Î¼^Y</em></strong> <strong>è®©æ¨¡å‹è‡ªå·±è®­ç»ƒå‡ºæ¥</strong>ã€‚</p><p>è¿™æ ·çš„è¯ï¼Œæœ‰å¤šå°‘ä¸ªç±»å°±æœ‰å¤šå°‘ä¸ªæ­£æ€åˆ†å¸ƒï¼Œè€Œåœ¨ç”Ÿæˆçš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±å¯ä»¥<strong>é€šè¿‡æ§åˆ¶å‡å€¼æ¥æ§åˆ¶ç”Ÿæˆå›¾åƒçš„ç±»åˆ«</strong>ã€‚</p><p>äº‹å®ä¸Šï¼Œè¿™æ ·å¯èƒ½ä¹Ÿæ˜¯åœ¨ VAE çš„åŸºç¡€ä¸ŠåŠ å…¥æœ€å°‘çš„ä»£ç æ¥å®ç° CVAE çš„æ–¹æ¡ˆäº†ï¼Œå› ä¸ºè¿™ä¸ªâ€œæ–°å¸Œæœ›â€ä¹Ÿåªéœ€é€šè¿‡ä¿®æ”¹ KL loss å®ç°ï¼š</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-6cfa68ced2a4a089f8db3da7236f7129_hd.jpg" alt="img"></p><p>ä¸‹å›¾æ˜¾ç¤ºè¿™ä¸ªç®€å•çš„ CVAE æ˜¯æœ‰ä¸€å®šçš„æ•ˆæœçš„ï¼Œä¸è¿‡å› ä¸º encoder å’Œ decoder éƒ½æ¯”è¾ƒç®€å•ï¼ˆçº¯ MLPï¼‰ï¼Œæ‰€ä»¥æ§åˆ¶ç”Ÿæˆçš„æ•ˆæœä¸å°½å®Œç¾ã€‚</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE-%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E4%B8%80%E5%9B%9E%E4%BA%8B/v2-55a120bb7af51c4d99c315f402cb9fb1_hd.jpg" alt="img"></p><p>ç”¨è¿™ä¸ª CVAE æ§åˆ¶ç”Ÿæˆæ•°å­— 9ï¼Œå¯ä»¥å‘ç°ç”Ÿæˆäº†å¤šç§æ ·å¼çš„ 9ï¼Œå¹¶ä¸”æ…¢æ…¢å‘ 7 è¿‡æ¸¡ï¼Œæ‰€ä»¥åˆæ­¥è§‚å¯Ÿè¿™ç§ CVAE æ˜¯æœ‰æ•ˆçš„ã€‚</p><p>æ›´å®Œå¤‡çš„ CVAE è¯·è¯»è€…è‡ªè¡Œå­¦ä¹ äº†ï¼Œæœ€è¿‘è¿˜å‡ºæ¥äº† CVAE ä¸ GAN ç»“åˆçš„å·¥ä½œ <em>CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training</em>ï¼Œæ¨¡å‹å¥—è·¯åƒå˜ä¸‡åŒ–ã€‚</p><h2 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a><strong>ä»£ç </strong></h2><p>æˆ‘æŠŠ Keras å®˜æ–¹çš„ VAE ä»£ç å¤åˆ¶äº†ä¸€ä»½ï¼Œç„¶åå¾®è°ƒå¹¶æ ¹æ®å‰æ–‡å†…å®¹æ·»åŠ äº†ä¸­æ–‡æ³¨é‡Šï¼Œä¹ŸæŠŠæœ€åè¯´åˆ°çš„ç®€å•çš„ CVAE å®ç°äº†ä¸€ä¸‹ï¼Œä¾›è¯»è€…å‚è€ƒã€‚</p><p>ä»£ç ï¼š<a href="https://link.zhihu.com/?target=https%3A//github.com/bojone/vae">https://github.com/bojone/vae</a></p><h2 id="ç»ˆç‚¹ç«™"><a href="#ç»ˆç‚¹ç«™" class="headerlink" title="ç»ˆç‚¹ç«™"></a><strong>ç»ˆç‚¹ç«™</strong></h2><p>ç£•ç£•ç¢°ç¢°ï¼Œåˆåˆ°äº†æ–‡ç« çš„ç»ˆç‚¹äº†ã€‚ä¸çŸ¥é“è®²æ¸…æ¥šäº†æ²¡ï¼Œå¸Œæœ›å¤§å®¶å¤šæç‚¹æ„è§ã€‚</p><p>æ€»çš„æ¥è¯´ï¼ŒVAE çš„æ€è·¯è¿˜æ˜¯å¾ˆæ¼‚äº®çš„ã€‚å€’ä¸æ˜¯è¯´å®ƒæä¾›äº†ä¸€ä¸ªå¤šä¹ˆå¥½çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå› ä¸ºäº‹å®ä¸Šå®ƒç”Ÿæˆçš„å›¾åƒå¹¶ä¸ç®—å¥½ï¼Œåæ¨¡ç³Šï¼‰ï¼Œè€Œæ˜¯å®ƒæä¾›äº†ä¸€ä¸ªå°†æ¦‚ç‡å›¾è·Ÿæ·±åº¦å­¦ä¹ ç»“åˆèµ·æ¥çš„ä¸€ä¸ªéå¸¸æ£’çš„æ¡ˆä¾‹ï¼Œè¿™ä¸ªæ¡ˆä¾‹æœ‰è¯¸å¤šå€¼å¾—æ€è€ƒå›å‘³çš„åœ°æ–¹ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>å˜åˆ†è‡ªç¼–ç å™¨èƒŒåçš„ç›´è§‰</title>
      <link href="/vae-bian-fen-zi-dong-bian-ma-qi-bei-hou-de-zhi-jue-yu-shi-xian/"/>
      <url>/vae-bian-fen-zi-dong-bian-ma-qi-bei-hou-de-zhi-jue-yu-shi-xian/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/</a></p><hr><p>There are two generative models facing neck to neck in the data generation business right now: <a href="https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/" target="_blank" rel="noopener">Generative Adversarial Nets (GAN)</a> and Variational Autoencoder (VAE). These two models have different take on how the models are trained. GAN is rooted in game theory, its objective is to find the Nash Equilibrium between discriminator net and generator net. On the other hand, VAE is rooted in bayesian inference, i.e. it wants to model the underlying probability distribution of data so that it could sample new data from that distribution.</p><p>In this post, we will look at the intuition of VAE model and its implementation in Keras.</p><h2 id="VAEçš„å…¬å¼ä¸ç›´è§‰"><a href="#VAEçš„å…¬å¼ä¸ç›´è§‰" class="headerlink" title="VAEçš„å…¬å¼ä¸ç›´è§‰"></a>VAEçš„å…¬å¼ä¸ç›´è§‰</h2><p>Suppose we want to generate a data. Good way to do it is first to decide what kind of data we want to generate, then actually generate the data. For example, say, we want to generate an animal. First, we imagine the animal: it must have four legs, and it must be able to swim. Having those criteria, we could then actually generate the animal by sampling from the animal kingdom. ç§ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªé¸­å˜´å…½!</p><p>From the story above, our imagination is analogous to <strong>latent variable</strong>. It is often useful to decide the latent variable first in generative models, as latent variable could describe our data. Without latent variable, it is as if we just generate data blindly. And this is the difference between GAN and VAE: VAE uses latent variable, hence itâ€™s an expressive model.</p><p>å¥½å§ï¼Œæ‰¯èµ·æ¥å¾ˆå¯¹ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚ä½•å»ºæ¨¡å‘¢? è®©æˆ‘ä»¬å…ˆä»æ¦‚ç‡åˆ†å¸ƒè°ˆèµ·ã€‚</p><p>å®šä¹‰ä¸€äº›ç¬¦å·ï¼š</p><ol><li>$X$: æˆ‘ä»¬æƒ³è¦å»ºæ¨¡çš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯åŠ¨ç‰©</li><li>$z$: æ½œå˜é‡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æƒ³è±¡ a.k.a our imagination</li><li>$P(X)$:æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ,ä¹Ÿå°±æ˜¯åŠ¨ç‰©ç‹å›½</li><li>$P(z)$: æ½œå˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ, ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¤§è„‘çš„è‡ªåŠ¨è„‘è¡¥æƒ³è±¡</li><li>$P(X|z)$: æŒ‡å®šæ½œå˜é‡ä¸‹ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒ, å°†æƒ³è±¡è½¬åŒ–ä¸ºçœŸæ˜¯åŠ¨ç‰©</li></ol><p>Our objective here is to model the data, hence we want to find $P(X)$. Using the law of probability, we could find it in relation with $z$ as follows:</p><script type="math/tex; mode=display">P(X) = \int P(X \vert z) P(z) dz</script><p>that is, we marginalize out $z$ from the joint probability distribution $P(X,z)$.</p><p>è¦æ˜¯æˆ‘ä»¬çŸ¥é“ $P(X,z)$å°±å¥½äº†, æˆ–è€…ç­‰ä»·çš„, $P(X|z)$ å’Œ $P(z)$â€¦</p><p>The idea of VAE is to infer $P(z)$ using $P(z|X)$. This is make a lot of sense if we think about it: we want to make our latent variable likely under our data. Talking in term of our fable example, we want to limit our imagination only on animal kingdom domain, so we shouldnâ€™t imagine about things like root, leaf, tyre, glass, GPU, refrigerator, doormat, â€¦ as itâ€™s unlikely that those things have anything to do with things that come from the animal kingdom. Right?</p><p>But the problem is, we have to infer that distribution $P(z|X)$, as we donâ€™t know it yet. In VAE, as it name suggests, we infer $P(z|X)$ using a method called <strong>Variational Inference (VI)</strong>. VI is one of the popular choice of method in bayesian inference, the other one being MCMC method. å˜åˆ†æ¨æ–­çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å°†å…¶ä½œä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜æ¥è¿›è¡Œæ¨ç†. How? By modeling the true distribution $P(z|X)$ using simpler distribution that is easy to evaluate, e.g. Gaussian, and minimize the difference between those two distribution using <a href="https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence" target="_blank" rel="noopener">KL divergence</a> metric, which tells us how difference it is $P$ and $Q$.</p><p>å¥½é¸Ÿ, ç°åœ¨æ¯”æ–¹è¯´æˆ‘ä»¬è¦ç”¨ $Q(z|X)$æ¨æ–­$P(z|X)$ . é‚£ä¹ˆï¼Œä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦å®šä¹‰ä¸º:</p><script type="math/tex; mode=display">\begin{align}D_{KL}[Q(z \vert X) \Vert P(z \vert X)] &= \sum_z Q(z \vert X) \, \log \frac{Q(z \vert X)}{P(z \vert X)} \\[10pt]                            &= E \left[ \log \frac{Q(z \vert X)}{P(z \vert X)} \right] \\[10pt]                            &= E[\log Q(z \vert X) - \log P(z \vert X)]\end{align} %]]></script><p>Recall the notations above, there are two things that we havenâ€™t use, namely $P(X)$, $P(X|z)$, and $P(z)$. But, with Bayesâ€™ rule, we could make it appear in the equation:</p><script type="math/tex; mode=display">\begin{align}D_{KL}[Q(z \vert X) \Vert P(z \vert X)] &= E \left[ \log Q(z \vert X) - \log \frac{P(X \vert z) P(z)}{P(X)} \right] \\[10pt]                                        &= E[\log Q(z \vert X) - (\log P(X \vert z) + \log P(z) - \log P(X))] \\[10pt]                                        &= E[\log Q(z \vert X) - \log P(X \vert z) - \log P(z) + \log P(X)]\end{align} %]]></script><p>Notice that the expectation is over $z$ and $P(X)$ doesnâ€™t depend on $z$, so we could move it outside of the expectation.</p><script type="math/tex; mode=display">\begin{align}D_{KL}[Q(z \vert X) \Vert P(z \vert X)] &= E[\log Q(z \vert X) - \log P(X \vert z) - \log P(z)] + \log P(X) \\[10pt]D_{KL}[Q(z \vert X) \Vert P(z \vert X)] - \log P(X) &= E[\log Q(z \vert X) - \log P(X \vert z) - \log P(z)]\end{align} %]]></script><p>And this is it, the VAE objective function:</p><script type="math/tex; mode=display">\log P(X) - D_{KL}[Q(z \vert X) \Vert P(z \vert X)] = E[\log P(X \vert z)] - D_{KL}[Q(z \vert X) \Vert P(z)]</script><p>è‡³æ­¤ï¼Œæˆ‘ä»¬éƒ½æœ‰äº†ä»€ä¹ˆï¼Œåˆ—ä¸¾ä¸€ä¸‹ï¼š</p><ol><li>$Q(z|X)$ that project our data $X$ into latent variable space</li><li>$z$, the latent variable</li><li>$P(X|z)$ that generate data given latent variable</li></ol><p>We might feel familiar with this kind of structure. And guess what, itâ€™s the same structure as seen in <a href="https://wiseodd.github.io/techblog/2016/12/03/autoencoders/" target="_blank" rel="noopener">Autoencoder</a>! That is, $Q(z|X)$ is the encoder net, $z$ is the encoded representation, and $P(X|z)$ is the decoder net! Well, well, no wonder the name of this model is Variational Autoencoder!</p><h2 id="å‰–æç›®æ ‡"><a href="#å‰–æç›®æ ‡" class="headerlink" title="å‰–æç›®æ ‡"></a>å‰–æç›®æ ‡</h2><p>It turns out, VAE objective function has a very nice interpretation. That is, we want to model our data, which described by $logâ¡P(X)$, under some error $D_{KL}[Q(z|X)âˆ¥P(z|X)]$. In other words, VAE tries to find the lower bound of $logâ¡P(X)$, which in practice is good enough as trying to find the exact distribution is often untractable[éš¾ä»¥å¤„ç†].</p><p>That model then could be found by maximazing over some mapping from latent variable to data $logP(X|z)$ and minimizing the difference between our simple distribution $Q(z|X)$ and the true latent distribution $P(z)$.</p><p>As we might already know, maximizing $E[logP(X|z)]$ is a maximum likelihood estimation. We basically see it all the time in discriminative supervised model, for example Logistic Regression, SVM, or Linear Regression. In the other words, given an input $z$ and an output $X$, we want to maximize the conditional distribution $P(X|z)$ under some model parameters. So we could implement it by using any classifier with input $z$ and output $X$, then optimize the objective function by using for example log loss or regression loss.</p><p>What about $D_{KL}[Q(z \vert X) \Vert P(z)]$? Here, $P(z)$is the latent variable distribution. We might want to sample $P(z)$ later, so the easiest choice is $N(0,1)$. Hence, we want to make $Q(z|X)$ to be as close as possible to $N(0,1)$ so that we could sample it easily.</p><p>Having $P(z)=N(0,1)$ also add another benefit. Letâ€™s say we also want $Q(z|X)$ to be Gaussian with parameters $Î¼(X)$ and $Î£(X)$, i.e. the mean and variance <strong>given</strong> $X$. Then, the KL divergence between those two distribution could be computed in closed form!</p><script type="math/tex; mode=display">D_{KL}[N(\mu(X), \Sigma(X)) \Vert N(0, 1)] = \frac{1}{2} \, \left( \textrm{tr}(\Sigma(X)) + \mu(X)^T\mu(X) - k - \log \, \det(\Sigma(X)) \right)</script><p>Above, $k$ is the dimension of our Gaussian. $tr(X)$ is trace function, i.e. sum of the diagonal of matrix $X$. The determinant of a diagonal matrix could be computed as product of its diagonal. So really, we could implement $Î£(X)$ as just a vector as itâ€™s a diagonal matrix:</p><script type="math/tex; mode=display">\begin{align}D_{KL}[N(\mu(X), \Sigma(X)) \Vert N(0, 1)] &= \frac{1}{2} \, \left( \sum_k \Sigma(X) + \sum_k \mu^2(X) - \sum_k 1 - \log \, \prod_k \Sigma(X) \right) \\[10pt]                                      &= \frac{1}{2} \, \left( \sum_k \Sigma(X) + \sum_k \mu^2(X) - \sum_k 1 - \sum_k \log \Sigma(X) \right) \\[10pt]                                      &= \frac{1}{2} \, \sum_k \left( \Sigma(X) + \mu^2(X) - 1 - \log \Sigma(X) \right)\end{align} %]]></script><p>In practice, however, itâ€™s better to model $Î£(X)$ as $logÎ£(X)$, as it is more numerically stable to take exponent compared to computing <code>log</code>. Hence, our final KL divergence term is:</p><script type="math/tex; mode=display">D_{KL}[N(\mu(X), \Sigma(X)) \Vert N(0, 1)] = \frac{1}{2} \sum_k \left( \exp(\Sigma(X)) + \mu^2(X) - 1 - \Sigma(X) \right)</script><h2 id="ä½¿ç”¨Keraså®ç°VAE"><a href="#ä½¿ç”¨Keraså®ç°VAE" class="headerlink" title="ä½¿ç”¨Keraså®ç°VAE"></a>ä½¿ç”¨Keraså®ç°VAE</h2><p>First, letâ€™s implement the encoder net $Q(z|X)$, which takes input $X$ and outputting two things: $Î¼(X)$ and $ Î£(X)$, the parameters of the Gaussian.</p><pre class=" language-lang-python"><code class="language-lang-python">from tensorflow.examples.tutorials.mnist import input_datafrom keras.layers import Input, Dense, Lamdafrom keras.models import Modelfrom keras.objectives import binary_crossentropyfrom keras.callbacks import LearningRateSchedulerimport numpy as npimport matplotlib.pyplot as pltimport keras.backend as Kimport tensorflow as tfm = 50n_z = 2n_epoch =10# Q(z|X) -- encoder inputs = Input(shape=(784,))h_q = Dense(512, activation='relu')(inputs)mu = Dense(n_z, activation='linear')(h_q)log_sigma = Dense(n_z, activation='linear')(h_q)</code></pre><p>That is, our $Q(z|X)$ is a neural net with one hidden layer. In this implementation, our latent variable is two dimensional, so that we could easily visualize it. In practice though, more dimension in latent variable should be better.</p><p>However, we are now facing a problem. How do we get $z$ from the encoder outputs? Obviously we could sample $z$ from a Gaussian which parameters are the outputs of the encoder. Alas, sampling directly wonâ€™t do, if we want to train VAE with gradient descent as the sampling operation doesnâ€™t have gradient!</p><p>There is, however a trick called <strong>reparameterization</strong>(é‡æ–°å‚æ•°åŒ–) trick, which makes the network differentiable. Reparameterization trick basically divert the non-differentiable operation out of the network, so that, even though we still involve a thing that is non-differentiable, at least it is out of the network, hence the network could still be trained.</p><p>The reparameterization trick is as follows. Recall, if we have $xâˆ¼N(Î¼,Î£)$ and then standardize it so that $ Î¼=0,Î£=1$, we could revert it back to the original distribution by reverting the standardization process. Hence, we have this equation:</p><script type="math/tex; mode=display">x = \mu + \Sigma^{\frac{1}{2}} x_{std}</script><p>With that in mind, we could extend it. If we sample from a standard normal distribution, we could convert it to any Gaussian we want if we know the mean and the variance. Hence we could implement our sampling operation of $z$ by:</p><script type="math/tex; mode=display">z = \mu(X) + \Sigma^{\frac{1}{2}}(X) \, \epsilon</script><p>where $Ïµâˆ¼N(0,1)$.</p><p>Now, during backpropagation, we donâ€™t care anymore with the sampling process, as it is now outside of the network, i.e. doesnâ€™t depend on anything in the net, hence the gradient wonâ€™t flow through it.</p><pre class=" language-lang-python"><code class="language-lang-python">def sample_z(args):    mu, log_sigma = args    eps = K.random_normal(shape=(m, n_z), mean=0., std=1.)    return mu + K.exp(log_sigma / 2) * eps# Sample z ~ Q(z|X) z = Lambda(sample_z)([mu, log_sigma])</code></pre><p>Now we create the decoder net $P(X|z)$:</p><pre class=" language-lang-python"><code class="language-lang-python"># P(X|z) -- decoder decoder_hidden = Dense(512, activation='relu')decoder_out = Dense(784, activation='sigmoid')h_p = decoder_hidden(z)outputs = decoder_out(h_p)</code></pre><p>Lastly, from this model, we can do three things: reconstruct inputs, encode inputs into latent variables, and generate data from latent variable. So, we have three Keras models:</p><pre class=" language-lang-python"><code class="language-lang-python"># Overall VAE model, for reconstruction and training vae = Model(inputs, outputs)# Encoder model, to encode input into latent variable # We use the mean as the output as it is the center point, the representative of the gaussian encoder = Model(inputs, mu)# Generator model, generate new data given latent variable z d_in = Input(shape=(n_z,))d_h = decoder_hidden(d_in)d_out = decoder_out(d_h)decoder = Model(d_in, d_out)</code></pre><p>Then, we need to translate our loss into Keras code:</p><pre class=" language-lang-python"><code class="language-lang-python">def vae_loss(y_true, y_pred):    """ Calculate loss = reconstruction loss + KL loss for each data in minibatch """    # E[log P(X|z)]         recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian         kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)    return recon + kl</code></pre><p>and then train it:</p><pre class=" language-lang-python"><code class="language-lang-python">vae.compile(optimizer='adam', loss=vae_loss)vae.fit(X_train, X_train, batch_size=m, nb_epoch=n_epoch)</code></pre><p>And thatâ€™s it, the implementation of VAE in Keras!</p><h2 id="MNISTæ•°æ®é›†å®è·µ"><a href="#MNISTæ•°æ®é›†å®è·µ" class="headerlink" title="MNISTæ•°æ®é›†å®è·µ"></a>MNISTæ•°æ®é›†å®è·µ</h2><p>We could use any dataset really, but like always, we will use MNIST as an example.</p><p>After we trained our VAE model, we then could visualize the latent variable space $Q(z|X)$:</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E8%83%8C%E5%90%8E%E7%9A%84%E7%9B%B4%E8%A7%89%E4%B8%8E%E5%AE%9E%E7%8E%B0%20/z_dist.png" alt="img"></p><p>As we could see, in the latent space, the representation of our data that have the same characteristic, e.g. same label, are close to each other. Notice that in the training phase, we never provide any information regarding the data.</p><p>We could also look at the data reconstruction by running through the data into overall VAE net:</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E8%83%8C%E5%90%8E%E7%9A%84%E7%9B%B4%E8%A7%89%E4%B8%8E%E5%AE%9E%E7%8E%B0%20/reconstruction.png" alt="img"></p><p>Lastly, we could generate new sample by first sample $zâˆ¼N(0,1)$ and feed it into our decoder net:</p><p><img src="VAE%20-%20%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E8%83%8C%E5%90%8E%E7%9A%84%E7%9B%B4%E8%A7%89%E4%B8%8E%E5%AE%9E%E7%8E%B0%20/generation.png" alt="img"></p><p>If we look closely on the reconstructed and generated data, we would notice that some of the data are ambiguous. For example the digit 5 looks like 3 or 8. Thatâ€™s because our latent variable space is a continous distribution (i.e. $N(0,1)$), hence there bound to be some smooth transition on the edge of the clusters. And also, the cluster of digits are close to each other if they are somewhat similar. Thatâ€™s why in the latent space, 5 is close to 3.</p><h2 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><p>In this post we looked at the intuition behind Variational Autoencoder (VAE), its formulation, and its implementation in Keras.</p><p>We also saw the difference between VAE and GAN, the two most popular generative models nowadays.</p><p>For more math on VAE, be sure to hit the original paper by Kingma et al., 2014. There is also an excellent tutorial on VAE by Carl Doersch. Check out the references section below.</p><p>The full code is available in my repo: <a href="https://github.com/wiseodd/generative-models" target="_blank" rel="noopener">https://github.com/wiseodd/generative-models</a></p><h2 id="å‚è€ƒæ–‡çŒ®"><a href="#å‚è€ƒæ–‡çŒ®" class="headerlink" title="å‚è€ƒæ–‡çŒ®"></a>å‚è€ƒæ–‡çŒ®</h2><ol><li>Doersch, Carl. â€œTutorial on variational autoencoders.â€ arXiv preprint arXiv:1606.05908 (2016).</li><li>Kingma, Diederik P., and Max Welling. â€œAuto-encoding variational bayes.â€ arXiv preprint arXiv:1312.6114 (2013).</li><li><a href="https://blog.keras.io/building-autoencoders-in-keras.html" target="_blank" rel="noopener">https://blog.keras.io/building-autoencoders-in-keras.html</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>è¯åµŒå…¥</title>
      <link href="/ci-qian-ru/"/>
      <url>/ci-qian-ru/</url>
      
        <content type="html"><![CDATA[<h2 id="1-ä¸ºä½•ä¸é‡‡ç”¨-one-hot-å‘é‡"><a href="#1-ä¸ºä½•ä¸é‡‡ç”¨-one-hot-å‘é‡" class="headerlink" title="1. ä¸ºä½•ä¸é‡‡ç”¨ one-hot å‘é‡"></a>1. ä¸ºä½•ä¸é‡‡ç”¨ one-hot å‘é‡</h2><p>å‡è®¾è¯å…¸ä¸­ä¸åŒè¯çš„æ•°é‡ï¼ˆè¯å…¸å¤§å°ï¼‰ä¸º <code>N</code>ï¼Œæ¯ä¸ªè¯å¯ä»¥å’Œä» <code>0</code> åˆ°<code>Nâˆ’1</code>çš„è¿ç»­æ•´æ•°ä¸€ä¸€å¯¹åº”ã€‚è¿™äº›ä¸è¯å¯¹åº”çš„æ•´æ•°å«åš<strong>è¯çš„ç´¢å¼•</strong>ã€‚ å‡è®¾ä¸€ä¸ªè¯çš„ç´¢å¼•ä¸º<code>i</code>ï¼Œä¸ºäº†å¾—åˆ°è¯¥è¯çš„ one-hot å‘é‡è¡¨ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå…¨ 0 çš„é•¿ä¸º <code>N</code>çš„å‘é‡ï¼Œå¹¶å°†å…¶ç¬¬ <code>i</code>ä½è®¾æˆ <code>1</code>ã€‚è¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªè¯å°±è¡¨ç¤ºæˆäº†ä¸€ä¸ªé•¿åº¦ä¸º Nçš„å‘é‡ï¼Œå¯ä»¥ç›´æ¥è¢«ç¥ç»ç½‘ç»œä½¿ç”¨ã€‚</p><p>è™½ç„¶ one-hot è¯å‘é‡æ„é€ èµ·æ¥å¾ˆå®¹æ˜“ï¼Œä½†é€šå¸¸å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½é€‰æ‹©ã€‚ä¸€ä¸ªä¸»è¦çš„åŸå› æ˜¯ï¼Œ<strong>one-hot è¯å‘é‡æ— æ³•å‡†ç¡®è¡¨è¾¾ä¸åŒè¯ä¹‹é—´çš„ç›¸ä¼¼åº¦</strong>ï¼Œä¾‹å¦‚æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å¯¹äºå‘é‡ x,yâˆˆRdï¼Œå®ƒä»¬çš„ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯å®ƒä»¬ä¹‹é—´å¤¹è§’çš„ä½™å¼¦å€¼ã€‚</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-cdb3732e0bbeeed9.webp" alt="img"></p><p>ç”±äºä»»ä½•ä¸¤ä¸ªä¸åŒè¯çš„ one-hot å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦éƒ½ä¸º 0ï¼Œå¤šä¸ªä¸åŒè¯ä¹‹é—´çš„ç›¸ä¼¼åº¦éš¾ä»¥é€šè¿‡ one-hot å‘é‡å‡†ç¡®åœ°ä½“ç°å‡ºæ¥ã€‚</p><p><strong>Word2vec</strong> å·¥å…·çš„æå‡ºæ­£æ˜¯ä¸ºäº†è§£å†³ä¸Šé¢è¿™ä¸ªé—®é¢˜ [1]ã€‚<strong>å®ƒå°†æ¯ä¸ªè¯è¡¨ç¤ºæˆä¸€ä¸ªå®šé•¿çš„å‘é‡</strong>ï¼Œå¹¶ä½¿å¾—è¿™äº›å‘é‡èƒ½è¾ƒå¥½åœ°è¡¨è¾¾ä¸åŒè¯ä¹‹é—´çš„ç›¸ä¼¼å’Œç±»æ¯”å…³ç³»ã€‚Word2vec å·¥å…·åŒ…å«äº†ä¸¤ä¸ªæ¨¡å‹ï¼šè·³å­—æ¨¡å‹ï¼ˆskip-gramï¼‰[2] å’Œè¿ç»­è¯è¢‹æ¨¡å‹ï¼ˆcontinuous bag of wordsï¼Œç®€ç§° CBOWï¼‰[3]ã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬åˆ†åˆ«ä»‹ç»è¿™ä¸¤ä¸ªæ¨¡å‹ä»¥åŠå®ƒä»¬çš„è®­ç»ƒæ–¹æ³•ã€‚</p><h2 id="2-è·³å­—æ¨¡å‹-skip-gram"><a href="#2-è·³å­—æ¨¡å‹-skip-gram" class="headerlink" title="2. è·³å­—æ¨¡å‹(skip gram)"></a>2. è·³å­—æ¨¡å‹(skip gram)</h2><h3 id="2-1-è·³å­—æ¨¡å‹ä»‹ç»"><a href="#2-1-è·³å­—æ¨¡å‹ä»‹ç»" class="headerlink" title="2.1 è·³å­—æ¨¡å‹ä»‹ç»"></a>2.1 è·³å­—æ¨¡å‹ä»‹ç»</h3><p>è·³å­—æ¨¡å‹å‡è®¾åŸºäºæŸä¸ªè¯æ¥ç”Ÿæˆå®ƒåœ¨æ–‡æœ¬åºåˆ—å‘¨å›´çš„è¯ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾æ–‡æœ¬åºåˆ—æ˜¯â€œtheâ€ã€â€œmanâ€ã€â€œlovesâ€ã€â€œhisâ€å’Œâ€œsonâ€ã€‚ä»¥â€œlovesâ€ä½œä¸ºä¸­å¿ƒè¯ï¼Œè®¾èƒŒæ™¯çª—å£å¤§å°ä¸º 2ã€‚å¦‚å›¾ 10.1 æ‰€ç¤ºï¼Œè·³å­—æ¨¡å‹æ‰€å…³å¿ƒçš„æ˜¯ï¼Œç»™å®šä¸­å¿ƒè¯â€œlovesâ€ï¼Œç”Ÿæˆä¸å®ƒè·ç¦»ä¸è¶…è¿‡ 2 ä¸ªè¯çš„èƒŒæ™¯è¯â€œtheâ€ã€â€œmanâ€ã€â€œhisâ€å’Œâ€œsonâ€çš„æ¡ä»¶æ¦‚ç‡ï¼Œå³</p><p>P(<code>the&quot;,</code>manâ€,<code>his&quot;,</code>sonâ€âˆ£``lovesâ€)</p><p>å‡è®¾ç»™å®šä¸­å¿ƒè¯çš„æƒ…å†µä¸‹ï¼ŒèƒŒæ™¯è¯çš„ç”Ÿæˆæ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆä¸Šå¼å¯ä»¥æ”¹å†™æˆ</p><p>P(<code>the&quot;âˆ£</code>lovesâ€)â‹…P(<code>man&quot;âˆ£</code>lovesâ€)â‹…P(<code>his&quot;âˆ£</code>lovesâ€)â‹…P(<code>son&quot;âˆ£</code>lovesâ€)</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-a3e4af76d81a84a4.webp" alt="img"></p><blockquote id="fn_ footnote"><sup> footnote</sup>. è·³å­—æ¨¡å‹å…³å¿ƒç»™å®šä¸­å¿ƒè¯ç”ŸæˆèƒŒæ™¯è¯çš„æ¡ä»¶æ¦‚ç‡<a href="#reffn_ footnote" title="Jump back to footnote [ footnote] in the text."> &#8617;</a></blockquote><p>åœ¨è·³å­—æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªè¯è¢«è¡¨ç¤ºæˆä¸¤ä¸ª dç»´å‘é‡ç”¨æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ã€‚å‡è®¾è¿™ä¸ªè¯åœ¨è¯å…¸ä¸­ç´¢å¼•ä¸º iï¼Œå½“å®ƒä¸ºä¸­å¿ƒè¯æ—¶å‘é‡è¡¨ç¤ºä¸º viâˆˆRdï¼Œè€Œä¸ºèƒŒæ™¯è¯æ—¶å‘é‡è¡¨ç¤ºä¸º uiâˆˆRdã€‚è®¾ä¸­å¿ƒè¯ wcåœ¨è¯å…¸ä¸­ç´¢å¼•ä¸º cï¼ŒèƒŒæ™¯è¯ woåœ¨è¯å…¸ä¸­ç´¢å¼•ä¸º oï¼Œç»™å®šä¸­å¿ƒè¯ç”ŸæˆèƒŒæ™¯è¯çš„æ¡ä»¶æ¦‚ç‡å¯ä»¥é€šè¿‡å¯¹å‘é‡å†…ç§¯åš softmax è¿ç®—è€Œå¾—åˆ°ï¼š</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-a907cb2e53de64c6.webp" alt="img"></p><p>å…¶ä¸­è¯å…¸ç´¢å¼•é›† V={0,1,â€¦,|V|âˆ’1}ã€‚å‡è®¾ç»™å®šä¸€ä¸ªé•¿åº¦ä¸º Tçš„æ–‡æœ¬åºåˆ—ï¼Œè®¾æ—¶é—´æ­¥ tçš„è¯ä¸º w(t)ã€‚å‡è®¾ç»™å®šä¸­å¿ƒè¯çš„æƒ…å†µä¸‹èƒŒæ™¯è¯çš„ç”Ÿæˆç›¸äº’ç‹¬ç«‹ï¼Œå½“èƒŒæ™¯çª—å£å¤§å°ä¸º mæ—¶ï¼Œè·³å­—æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•°å³ç»™å®šä»»ä¸€ä¸­å¿ƒè¯ç”Ÿæˆæ‰€æœ‰èƒŒæ™¯è¯çš„æ¦‚ç‡</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-2052ced03e044dad.webp" alt="img"></p><p>è¿™é‡Œå°äº 1 å’Œå¤§äº Tçš„æ—¶é—´æ­¥å¯ä»¥å¿½ç•¥ã€‚</p><h3 id="2-2-è·³å­—æ¨¡å‹è®­ç»ƒ"><a href="#2-2-è·³å­—æ¨¡å‹è®­ç»ƒ" class="headerlink" title="2.2 è·³å­—æ¨¡å‹è®­ç»ƒ"></a>2.2 è·³å­—æ¨¡å‹è®­ç»ƒ</h3><p>è·³å­—æ¨¡å‹çš„å‚æ•°æ˜¯æ¯ä¸ªè¯æ‰€å¯¹åº”çš„ä¸­å¿ƒè¯å‘é‡å’ŒèƒŒæ™¯è¯å‘é‡ã€‚è®­ç»ƒä¸­æˆ‘ä»¬é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°æ¥å­¦ä¹ æ¨¡å‹å‚æ•°ï¼Œå³æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚è¿™ç­‰ä»·äºæœ€å°åŒ–ä»¥ä¸‹æŸå¤±å‡½æ•°ï¼š</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-4f7cb4944c503930.webp" alt="img"></p><p>å¦‚æœä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼Œé‚£ä¹ˆåœ¨æ¯ä¸€æ¬¡è¿­ä»£é‡Œæˆ‘ä»¬éšæœºé‡‡æ ·ä¸€ä¸ªè¾ƒçŸ­çš„å­åºåˆ—æ¥è®¡ç®—æœ‰å…³è¯¥å­åºåˆ—çš„æŸå¤±ï¼Œç„¶åè®¡ç®—æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ¢¯åº¦è®¡ç®—çš„å…³é”®æ˜¯å¯¹æ•°æ¡ä»¶æ¦‚ç‡æœ‰å…³ä¸­å¿ƒè¯å‘é‡å’ŒèƒŒæ™¯è¯å‘é‡çš„æ¢¯åº¦ã€‚æ ¹æ®å®šä¹‰ï¼Œé¦–å…ˆçœ‹åˆ°</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-b27041346b051d61.webp" alt="img"></p><p>é€šè¿‡å¾®åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸Šå¼ä¸­ vcçš„æ¢¯åº¦</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-753bb313a7e509a0.webp" alt="img"></p><p>å®ƒçš„è®¡ç®—éœ€è¦è¯å…¸ä¸­æ‰€æœ‰è¯ä»¥ wcä¸ºä¸­å¿ƒè¯çš„æ¡ä»¶æ¦‚ç‡ã€‚æœ‰å…³å…¶ä»–è¯å‘é‡çš„æ¢¯åº¦åŒç†å¯å¾—ã€‚</p><p>è®­ç»ƒç»“æŸåï¼Œå¯¹äºè¯å…¸ä¸­çš„ä»»ä¸€ç´¢å¼•ä¸º içš„è¯ï¼Œæˆ‘ä»¬å‡å¾—åˆ°è¯¥è¯ä½œä¸ºä¸­å¿ƒè¯å’ŒèƒŒæ™¯è¯çš„ä¸¤ç»„è¯å‘é‡ viå’Œ uiã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­ï¼Œä¸€èˆ¬ä½¿ç”¨è·³å­—æ¨¡å‹çš„ä¸­å¿ƒè¯å‘é‡ä½œä¸ºè¯çš„è¡¨å¾å‘é‡ã€‚</p><h2 id="3-è¿ç»­è¯è¢‹æ¨¡å‹"><a href="#3-è¿ç»­è¯è¢‹æ¨¡å‹" class="headerlink" title="3 è¿ç»­è¯è¢‹æ¨¡å‹"></a>3 è¿ç»­è¯è¢‹æ¨¡å‹</h2><h3 id="3-1-è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ"><a href="#3-1-è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ" class="headerlink" title="3.1 è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ"></a>3.1 è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ</h3><p>è¿ç»­è¯è¢‹æ¨¡å‹ä¸è·³å­—æ¨¡å‹ç±»ä¼¼ã€‚ä¸è·³å­—æ¨¡å‹æœ€å¤§çš„ä¸åŒåœ¨äºï¼Œè¿ç»­è¯è¢‹æ¨¡å‹å‡è®¾åŸºäºæŸä¸­å¿ƒè¯åœ¨æ–‡æœ¬åºåˆ—å‰åçš„èƒŒæ™¯è¯æ¥ç”Ÿæˆè¯¥ä¸­å¿ƒè¯ã€‚åœ¨åŒæ ·çš„æ–‡æœ¬åºåˆ—â€œtheâ€ã€ â€œmanâ€ã€â€œlovesâ€ã€â€œhisâ€å’Œâ€œsonâ€é‡Œï¼Œä»¥â€œlovesâ€ä½œä¸ºä¸­å¿ƒè¯ï¼Œä¸”èƒŒæ™¯çª—å£å¤§å°ä¸º 2 æ—¶ï¼Œè¿ç»­è¯è¢‹æ¨¡å‹å…³å¿ƒçš„æ˜¯ï¼Œç»™å®šèƒŒæ™¯è¯â€œtheâ€ã€â€œmanâ€ã€â€œhisâ€å’Œâ€œsonâ€ç”Ÿæˆä¸­å¿ƒè¯â€œlovesâ€çš„æ¡ä»¶æ¦‚ç‡ï¼ˆå¦‚å›¾ 10.2 æ‰€ç¤ºï¼‰ï¼Œä¹Ÿå°±æ˜¯</p><p>P(<code>loves&quot;âˆ£</code>theâ€,<code>man&quot;,</code>hisâ€,``sonâ€)</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-1b0f798de6d8259c.webp" alt="img"></p><p>å› ä¸ºè¿ç»­è¯è¢‹æ¨¡å‹çš„èƒŒæ™¯è¯æœ‰å¤šä¸ªï¼Œæˆ‘ä»¬å°†è¿™äº›èƒŒæ™¯è¯å‘é‡å–å¹³å‡ï¼Œç„¶åä½¿ç”¨å’Œè·³å­—æ¨¡å‹ä¸€æ ·çš„æ–¹æ³•æ¥è®¡ç®—æ¡ä»¶æ¦‚ç‡ã€‚è®¾ viâˆˆRdå’Œ uiâˆˆRdåˆ†åˆ«è¡¨ç¤ºè¯å…¸ä¸­ç´¢å¼•ä¸º içš„è¯ä½œä¸ºèƒŒæ™¯è¯å’Œä¸­å¿ƒè¯çš„å‘é‡ï¼ˆæ³¨æ„ç¬¦å·å’Œè·³å­—æ¨¡å‹ä¸­æ˜¯ç›¸åçš„ï¼‰ã€‚è®¾ä¸­å¿ƒè¯ wcåœ¨è¯å…¸ä¸­ç´¢å¼•ä¸º cï¼ŒèƒŒæ™¯è¯ wo1,â€¦,wo2måœ¨è¯å…¸ä¸­ç´¢å¼•ä¸º o1,â€¦,o2mï¼Œé‚£ä¹ˆç»™å®šèƒŒæ™¯è¯ç”Ÿæˆä¸­å¿ƒè¯çš„æ¡ä»¶æ¦‚ç‡</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-da3b99673087cc2d.webp" alt="img"></p><p>ç»™å®šä¸€ä¸ªé•¿åº¦ä¸º Tçš„æ–‡æœ¬åºåˆ—ï¼Œè®¾æ—¶é—´æ­¥ tçš„è¯ä¸º w(t)ï¼ŒèƒŒæ™¯çª—å£å¤§å°ä¸º mã€‚è¿ç»­è¯è¢‹æ¨¡å‹çš„ä¼¼ç„¶å‡½æ•°ä¸ºç”±èƒŒæ™¯è¯ç”Ÿæˆä»»ä¸€ä¸­å¿ƒè¯çš„æ¦‚ç‡</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-e5c7e9069b39dd92.webp" alt="img"></p><h3 id="3-2-è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ"><a href="#3-2-è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ" class="headerlink" title="3.2 è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ"></a>3.2 è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒ</h3><p>è¿ç»­è¯è¢‹æ¨¡å‹è®­ç»ƒåŒè·³å­—æ¨¡å‹è®­ç»ƒåŸºæœ¬ä¸€è‡´ã€‚è¿ç»­è¯è¢‹æ¨¡å‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·äºæœ€å°åŒ–æŸå¤±å‡½æ•°</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-98b86334af8c0d74.webp" alt="img"></p><p>æ³¨æ„åˆ°</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-35d91654a81ee286.webp" alt="img"></p><p>é€šè¿‡å¾®åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºä¸Šå¼ä¸­æ¡ä»¶æ¦‚ç‡çš„å¯¹æ•°æœ‰å…³ä»»ä¸€èƒŒæ™¯è¯å‘é‡ voiï¼ˆi=1,â€¦,2mï¼‰çš„æ¢¯åº¦</p><p><img src="%E8%AF%8D%E5%B5%8C%E5%85%A5/10947003-beed3548ece6302e.webp" alt="img"></p><p>æœ‰å…³å…¶ä»–è¯å‘é‡çš„æ¢¯åº¦åŒç†å¯å¾—ã€‚åŒè·³å­—æ¨¡å‹ä¸ä¸€æ ·çš„ä¸€ç‚¹åœ¨äºï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨è¿ç»­è¯è¢‹æ¨¡å‹çš„èƒŒæ™¯è¯å‘é‡ä½œä¸ºè¯çš„è¡¨å¾å‘é‡ã€‚</p><h2 id="4-word2vecå®ç°"><a href="#4-word2vecå®ç°" class="headerlink" title="4. word2vecå®ç°"></a>4. word2vecå®ç°</h2><h3 id="4-1-python-gensim"><a href="#4-1-python-gensim" class="headerlink" title="4.1 python-gensim"></a>4.1 python-gensim</h3><pre class=" language-lang-python"><code class="language-lang-python">from gensim.models import word2vec  sentences = word2vec.Text8Corpus("C:/traindataw2v.txt")  # åŠ è½½è¯­æ–™  model = word2vec.Word2Vec(sentences, size=200)  # è®­ç»ƒskip-gramæ¨¡å‹; é»˜è®¤window=5  #è·å–â€œå­¦ä¹ â€çš„è¯å‘é‡  print("å­¦ä¹ ï¼š" + model["å­¦ä¹ "])  # è®¡ç®—ä¸¤ä¸ªè¯çš„ç›¸ä¼¼åº¦/ç›¸å…³ç¨‹åº¦  y1 = model.similarity("ä¸é”™", "å¥½")  # è®¡ç®—æŸä¸ªè¯çš„ç›¸å…³è¯åˆ—è¡¨  y2 = model.most_similar("ä¹¦", topn=20)  # 20ä¸ªæœ€ç›¸å…³çš„  # å¯»æ‰¾å¯¹åº”å…³ç³»  print("ä¹¦-ä¸é”™ï¼Œè´¨é‡-")  y3 = model.most_similar(['è´¨é‡', 'ä¸é”™'], ['ä¹¦'], topn=3)  # å¯»æ‰¾ä¸åˆç¾¤çš„è¯  y4 = model.doesnt_match("ä¹¦ ä¹¦ç± æ•™æ å¾ˆ".split())  # ä¿å­˜æ¨¡å‹ï¼Œä»¥ä¾¿é‡ç”¨  model.save("db.model")  # å¯¹åº”çš„åŠ è½½æ–¹å¼  model = word2vec.Word2Vec.load("db.model")</code></pre><p>é»˜è®¤å‚æ•°å¦‚ä¸‹ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">sentences=None  size=100  alpha=0.025  window=5  min_count=5  max_vocab_size=None  sample=1e-3  seed=1  workers=3  min_alpha=0.0001  sg=0  hs=0  negative=5  cbow_mean=1  hashfxn=hash  iter=5  null_word=0  trim_rule=None  sorted_vocab=1  batch_words=MAX_WORDS_IN_BATCH</code></pre><p>å„ä¸ªå‚æ•°çš„æ„ä¹‰:</p><p>sentencesï¼šå°±æ˜¯æ¯ä¸€è¡Œæ¯ä¸€è¡Œçš„å¥å­ï¼Œä½†æ˜¯å¥å­é•¿åº¦ä¸è¦è¿‡å¤§ï¼Œç®€å•çš„è¯´å°±æ˜¯ä¸Šå›¾çš„æ ·å­</p><p>sgï¼šè¿™ä¸ªæ˜¯è®­ç»ƒæ—¶ç”¨çš„ç®—æ³•ï¼Œå½“ä¸º0æ—¶é‡‡ç”¨çš„æ˜¯CBOWç®—æ³•ï¼Œå½“ä¸º1æ—¶ä¼šé‡‡ç”¨skip-gram</p><p>sizeï¼šè¿™ä¸ªæ˜¯å®šä¹‰è®­ç»ƒçš„å‘é‡çš„é•¿åº¦</p><p>windowï¼šæ˜¯åœ¨ä¸€ä¸ªå¥å­ä¸­ï¼Œå½“å‰è¯å’Œé¢„æµ‹è¯çš„æœ€å¤§è·ç¦»</p><p>alphaï¼šæ˜¯å­¦ä¹ ç‡ï¼Œæ˜¯æ§åˆ¶æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ä¸‹é™é€Ÿåº¦çš„</p><p>seedï¼šç”¨äºéšæœºæ•°å‘ç”Ÿå™¨ã€‚ä¸åˆå§‹åŒ–è¯å‘é‡æœ‰å…³</p><p>min_countï¼š å­—å…¸æˆªæ–­.ï¼Œè¯é¢‘å°‘äºmin_countæ¬¡æ•°çš„å•è¯ä¼šè¢«ä¸¢å¼ƒæ‰</p><p>max_vocab_sizeï¼šè¯å‘é‡æ„å»ºæœŸé—´çš„RAMé™åˆ¶ã€‚å¦‚æœæ‰€æœ‰ä¸é‡å¤å•è¯ä¸ªæ•°è¶…è¿‡è¿™ä¸ªå€¼ï¼Œåˆ™å°±æ¶ˆé™¤æ‰å…¶ä¸­æœ€ä¸é¢‘ç¹çš„ä¸€ä¸ª,Noneè¡¨ç¤ºæ²¡æœ‰é™åˆ¶</p><p>sampleï¼šé«˜é¢‘è¯æ±‡çš„éšæœºè´Ÿé‡‡æ ·çš„é…ç½®é˜ˆå€¼ï¼Œé»˜è®¤ä¸º1e-3ï¼ŒèŒƒå›´æ˜¯(0,1e-5)</p><p>workersï¼šè®¾ç½®å¤šçº¿ç¨‹è®­ç»ƒæ¨¡å‹ï¼Œæœºå™¨çš„æ ¸æ•°è¶Šå¤šï¼Œè®­ç»ƒè¶Šå¿«</p><p>hsï¼šå¦‚æœä¸º1åˆ™ä¼šé‡‡ç”¨hierarchicaÂ·softmaxç­–ç•¥ï¼ŒHierarchical Softmaxæ˜¯ä¸€ç§å¯¹è¾“å‡ºå±‚è¿›è¡Œä¼˜åŒ–çš„ç­–ç•¥ï¼Œè¾“å‡ºå±‚ä»åŸå§‹æ¨¡å‹çš„åˆ©ç”¨softmaxè®¡ç®—æ¦‚ç‡å€¼æ”¹ä¸ºäº†åˆ©ç”¨Huffmanæ ‘è®¡ç®—æ¦‚ç‡å€¼ã€‚å¦‚æœè®¾ç½®ä¸º0ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œåˆ™è´Ÿé‡‡æ ·ç­–ç•¥ä¼šè¢«ä½¿ç”¨</p><p>negativeï¼šå¦‚æœå¤§äº0ï¼Œé‚£å°±ä¼šé‡‡ç”¨è´Ÿé‡‡æ ·ï¼Œæ­¤æ—¶è¯¥å€¼çš„å¤§å°å°±è¡¨ç¤ºæœ‰å¤šå°‘ä¸ªâ€œnoise wordsâ€ä¼šè¢«ä½¿ç”¨ï¼Œé€šå¸¸è®¾ç½®åœ¨ï¼ˆ5-20ï¼‰ï¼Œé»˜è®¤æ˜¯5ï¼Œå¦‚æœè¯¥å€¼è®¾ç½®æˆ0ï¼Œé‚£å°±è¡¨ç¤ºä¸é‡‡ç”¨è´Ÿé‡‡æ ·</p><p>cbow_meanï¼šåœ¨é‡‡ç”¨cbowæ¨¡å‹æ—¶ï¼Œæ­¤å€¼å¦‚æœæ˜¯0ï¼Œå°±ä¼šä½¿ç”¨ä¸Šä¸‹æ–‡è¯å‘é‡çš„å’Œï¼Œå¦‚æœæ˜¯1ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œå°±ä¼šé‡‡ç”¨å‡å€¼</p><p>hashfxnï¼šhashå‡½æ•°æ¥åˆå§‹åŒ–æƒé‡ã€‚é»˜è®¤ä½¿ç”¨pythonçš„hashå‡½æ•°</p><p>iterï¼š è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º5</p><p>trim_ruleï¼š ç”¨äºè®¾ç½®è¯æ±‡è¡¨çš„æ•´ç†è§„åˆ™ï¼ŒæŒ‡å®šé‚£äº›å•è¯è¦ç•™ä¸‹ï¼Œå“ªäº›è¦è¢«åˆ é™¤ã€‚å¯ä»¥è®¾ç½®ä¸ºNoneï¼ˆmin_countä¼šè¢«ä½¿ç”¨ï¼‰æˆ–è€…ä¸€ä¸ªæ¥å—(word, count, min_count)å¹¶è¿”å›utils.RULE_DISCARDï¼Œutils.RULE_KEEPæˆ–è€…utils.RULE_DEFAULTï¼Œè¿™ä¸ªè®¾ç½®åªä¼šç”¨åœ¨æ„å»ºè¯å…¸çš„æ—¶å€™ï¼Œä¸ä¼šæˆä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†</p><p>sorted_vocabï¼š å¦‚æœä¸º1ï¼ˆdefauÂ·tï¼‰ï¼Œåˆ™åœ¨åˆ†é…word index çš„æ—¶å€™ä¼šå…ˆå¯¹å•è¯åŸºäºé¢‘ç‡é™åºæ’åºã€‚</p><p>batch_wordsï¼šæ¯ä¸€æ‰¹ä¼ é€’ç»™æ¯ä¸ªçº¿ç¨‹å•è¯çš„æ•°é‡ï¼Œé»˜è®¤ä¸º10000ï¼Œå¦‚æœè¶…è¿‡è¯¥å€¼ï¼Œåˆ™ä¼šè¢«æˆªæ–­</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1] Word2vec å·¥å…·ã€‚<a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">https://code.google.com/archive/p/word2vec/</a></p><p>[2] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</p><p>[3] Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p><p>[4] <a href="https://zh.gluon.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">è¯åµŒå…¥ï¼ˆword2vecï¼‰</a></p><p>[5] <a href="https://www.jianshu.com/p/972d0db609f2" target="_blank" rel="noopener">word2vecçš„å‡ ç§å®ç°</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>æ¢å¹½æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ä¸¤ç§æ–¹æ³•</title>
      <link href="/tan-you-shen-du-sheng-cheng-mo-xing-de-liang-chong-fang-fa-vae-he-gan/"/>
      <url>/tan-you-shen-du-sheng-cheng-mo-xing-de-liang-chong-fang-fa-vae-he-gan/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://www.zybuluo.com/sambodhi/note/1040483" target="_blank" rel="noopener">https://www.zybuluo.com/sambodhi/note/1040483</a></p><hr><blockquote><p>è¿‘å¹´æ¥ï¼Œæ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDeep Generative Modelsï¼‰å–å¾—äº†ä»¤äººç©ç›®çš„æˆåŠŸã€‚å…¶ä¸­æœ‰ä¸¤ç§å¼ºå¤§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ¡†æ¶ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational Autoencodersï¼ŒVAEï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼ŒGANï¼‰ï¼Œè¿™æ˜¯ä¸¤ç§ä¸åŒçš„èŒƒå¼ã€‚VAEçš„å¥½å¤„æ˜¯å¯ä»¥é€šè¿‡ç¼–ç è§£ç çš„æ­¥éª¤ï¼Œç›´æ¥æ¯”è¾ƒé‡å»ºå›¾ç‰‡å’ŒåŸå§‹å›¾ç‰‡çš„å·®å¼‚ï¼Œè€Œè¿™ä¸€ç‚¹GANåšä¸åˆ°ã€‚VAEçš„åŠ£åŠ¿æ˜¯æ²¡æœ‰ä½¿ç”¨å¯¹æŠ—ç½‘ç»œï¼Œå› æ­¤ä¼šæ›´è¶‹å‘äºäº§ç”Ÿæ¨¡ç³Šçš„å›¾ç‰‡ã€‚</p></blockquote><p>ç”Ÿæˆæ¨¡å‹æ˜¯ä¸€ç§åˆ©ç”¨æ— ç›‘ç£å­¦ä¹ æ¥å­¦ä¹ ä»»ä½•ç±»å‹çš„æ•°æ®åˆ†å¸ƒçš„å¼ºæœ‰åŠ›æ–¹æ³•ï¼Œå®ƒåœ¨çŸ­çŸ­å‡ å¹´å†…å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚æ‰€æœ‰ç±»å‹çš„ç”Ÿæˆæ¨¡å‹éƒ½è‡´åŠ›äºå­¦ä¹ è®­ç»ƒé›†çš„çœŸå®æ•°æ®åˆ†å¸ƒï¼Œä»è€Œäº§ç”Ÿå…·æœ‰ä¸€äº›å˜åŒ–çš„æ–°æ•°æ®ç‚¹ã€‚ä½†æˆ‘ä»¬å¹¶ä¸æ€»èƒ½å¤Ÿéšå¼æˆ–æ˜¾å¼åœ°äº†è§£æˆ‘ä»¬æ•°æ®çš„ç¡®åˆ‡åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¦è¯•å›¾å»ºç«‹ä¸€ä¸ªä¸çœŸå®æ•°æ®åˆ†å¸ƒå°½å¯èƒ½ç›¸ä¼¼çš„åˆ†å¸ƒå»ºæ¨¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ç¥ç»ç½‘ç»œçš„èƒ½åŠ›æ¥å­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå¯ä»¥å°†æ¨¡å‹åˆ†å¸ƒé€¼è¿‘çœŸå®åˆ†å¸ƒã€‚</p><p>æœ€å¸¸ç”¨ã€æœ€æœ‰æ•ˆçš„ä¸¤ç§æ–¹æ³•æ˜¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational Autoencodersï¼ŒVAEï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼ŒGANï¼‰ã€‚VAEçš„ç›®æ ‡æ˜¯æœ€å¤§é™åº¦é™ä½<strong>æ•°æ®å¯¹æ•°ä¼¼ç„¶</strong>ï¼ˆlog-likelihoodï¼‰çš„ä¸‹é™ï¼Œè€ŒGANçš„ç›®æ ‡æ˜¯å®ç°ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰å’Œåˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰ä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…å°†è§£é‡ŠVAEå’ŒGANçš„å·¥ä½œåŠå®ƒä»¬èƒŒåçš„ç›´è§‰ã€‚</p><h2 id="å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational-Autoencoderï¼‰"><a href="#å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational-Autoencoderï¼‰" class="headerlink" title="å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational Autoencoderï¼‰"></a><strong>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational Autoencoderï¼‰</strong></h2><p>å‡è®¾è¯»è€…å·²ç»ç†Ÿæ‚‰vanilla autoencoderçš„å·¥ä½œæœºåˆ¶ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªç¼–ç å™¨å°†è¾“å…¥å›¾åƒç¼–ç ä¸ºæ›´å°çš„ç»´åº¦è¡¨ç¤ºï¼Œå®ƒå¯ä»¥å­˜å‚¨å…³äºè¾“å…¥æ•°æ®åˆ†å¸ƒçš„æ½œåœ¨ä¿¡æ¯ã€‚ä½†æ˜¯åœ¨ä¸€ä¸ªvanilla autoencoderä¸­ï¼Œç¼–ç å‘é‡åªèƒ½é€šè¿‡è§£ç å™¨æ˜ å°„åˆ°ç›¸åº”çš„è¾“å…¥ã€‚å®ƒå½“ç„¶ä¸èƒ½ç”¨äºç”Ÿæˆå…·æœ‰å¯å˜æ€§çš„ç›¸ä¼¼å›¾åƒã€‚</p><blockquote><p> <strong>Vanilla</strong>æ˜¯ç¥ç»ç½‘ç»œé¢†åŸŸçš„å¸¸è§è¯æ±‡ï¼Œæ¯”å¦‚Vanilla Neural Networksã€Vanilla CNNç­‰ã€‚Vanillaæœ¬æ„æ˜¯é¦™è‰ï¼Œåœ¨è¿™é‡ŒåŸºæœ¬ç­‰åŒäºrawã€‚æ¯”å¦‚Vanilla Neural Networkså®é™…ä¸Šå°±æ˜¯BPç¥ç»ç½‘ç»œï¼Œè€ŒVanilla CNNå®é™…ä¸Šå°±æ˜¯æœ€åŸå§‹çš„CNNã€‚</p></blockquote><p>ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ è®­ç»ƒæ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒã€‚VAEæ˜¯ä¸€ç§æœ€æµè¡Œçš„å­¦ä¹ å¤æ‚æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ— ç›‘ç£çš„æ–¹å¼ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºè´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰çš„æ¦‚ç‡å›¾æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„ç›®çš„æ˜¯äº†è§£è®­ç»ƒæ•°æ®é›†æ•°æ®çš„æ½œåœ¨æ¦‚ç‡çš„åˆ†å¸ƒï¼Œä»¥ä¾¿èƒ½å¤Ÿå¾ˆå®¹æ˜“åœ°ä»æ‰€å­¦çš„åˆ†å¸ƒä¸­é‡‡æ ·æ–°çš„æ•°æ®ã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯å­¦ä¹ ä¸€ç§è¢«ç§°ä¸º<strong>éšå˜é‡</strong>ï¼ˆlatent variablesï¼‰çš„è®­ç»ƒæ•°æ®çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºï¼ˆè¿™äº›å˜é‡ä¸æ˜¯ç›´æ¥è§‚å¯Ÿåˆ°çš„ï¼Œè€Œæ˜¯é€šè¿‡æ•°å­¦æ¨¡å‹æ¨å¯¼å‡ºæ¥çš„ï¼‰ï¼Œæˆ‘ä»¬å‡è®¾è¿™äº›å˜é‡äº§ç”Ÿäº†æˆ‘ä»¬å®é™…çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›éšå˜é‡å¯ä»¥å­˜å‚¨æ¨¡å‹éœ€è¦ç”Ÿæˆçš„è¾“å‡ºç±»å‹çš„æœ‰ç”¨ä¿¡æ¯ã€‚éšå˜é‡$z$çš„æ¦‚ç‡åˆ†å¸ƒç”¨$P(z)$è¡¨ç¤ºã€‚é€‰æ‹©é«˜æ–¯åˆ†å¸ƒï¼ˆGaussian distributionï¼‰ä½œä¸ºå­¦ä¹ åˆ†å¸ƒ$P(z)$çš„å…ˆéªŒï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶æ–¹ä¾¿é‡‡æ ·æ–°çš„æ•°æ®ç‚¹ã€‚</p><p>ç°åœ¨ä¸»è¦ç›®æ ‡æ˜¯ç”¨ä¸€äº›å‚æ•°å¯¹æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æœ€å¤§åŒ–äº†è®­ç»ƒæ•°æ®$X$çš„å¯èƒ½æ€§ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å‡è®¾ä¸€ä¸ªä½ç»´çš„ç‰¹å¾å‘é‡ï¼ˆlatent vectorï¼‰äº§ç”Ÿäº†æˆ‘ä»¬çš„æ•°æ®$x(xâˆˆX)$ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç¡®å®šå‡½æ•°$f(z;Î¸)$å°†è¿™ä¸ªç‰¹å¾å‘é‡æ˜ å°„åˆ°æ•°æ®$x$ä¸Šç„¶åè¯„ä¼°ï¼ˆè§å›¾1ï¼‰ã€‚åœ¨è¿™ç§ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–$X$ä¸­æ¯ä¸ªæ•°æ®çš„æ¦‚ç‡ï¼Œ</p><p>$PÓ©(X) = âˆ«PÓ©(X, z)dz = âˆ«PÓ©(X|z)PÓ©(z)dz$                                                               (1)â€‹</p><p>åœ¨è¿™é‡Œï¼Œ$f(z;Î¸)$å·²è¢«åˆ†å¸ƒ$PÓ©(X|z)$å–ä»£äº†ã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3iv2yftuj20a408nglp.jpg" alt="img"></p><p>è¿™ä¸ªæå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆmaximum likelihood estimationï¼‰èƒŒåçš„ç›´è§‰æ˜¯ï¼Œå¦‚æœæ¨¡å‹å¯ä»¥ä»è¿™äº›éšå˜é‡äº§ç”Ÿè®­ç»ƒæ ·æœ¬ï¼Œé‚£ä¹ˆå®ƒä¹Ÿå¯ä»¥äº§ç”Ÿå…·æœ‰ä¸€äº›å˜åŒ–çš„ç›¸ä¼¼æ ·æœ¬ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä»$P(z)$ä¸­æŠ½å–å¤§é‡çš„éšå˜é‡ï¼Œå¹¶ä»è¿™äº›å˜é‡ä¸­ç”Ÿæˆ$x$ï¼Œåˆ™ç”Ÿæˆçš„$x$åº”ä¸æ•°æ®åˆ†å¸ƒ$P_{data}(x)$ç›¸åŒ¹é…ã€‚ç°åœ¨æˆ‘ä»¬æœ‰ä¸¤ä¸ªé—®é¢˜éœ€è¦å›ç­”ã€‚å¦‚ä½•æ•æ‰éšå˜é‡çš„åˆ†å¸ƒä»¥åŠå¦‚ä½•å°†æ–¹ç¨‹1æ•´åˆåˆ°$z$çš„æ‰€æœ‰ç»´ä¸Šï¼Ÿ</p><p>æ˜¾ç„¶ï¼Œæ‰‹åŠ¨æŒ‡å®šæˆ‘ä»¬æƒ³è¦åœ¨ç‰¹å¾å‘é‡ä¸­ç¼–ç çš„ç›¸å…³ä¿¡æ¯ä»¥ç”Ÿæˆè¾“å‡ºå›¾åƒæ˜¯ä¸€é¡¹ç¹ççš„ä»»åŠ¡ã€‚ç›¸åï¼Œæˆ‘ä»¬ä¾é ç¥ç»ç½‘ç»œæ¥è®¡ç®—$z$ï¼Œå‡è®¾è¿™ä¸ªç‰¹å¾å‘é‡å¯ä»¥å¾ˆå¥½åœ°è¿‘ä¼¼ä¸ºæ­£æ€åˆ†å¸ƒï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶å¾ˆå®¹æ˜“åœ°è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœæˆ‘ä»¬åœ¨$n$ç»´ç©ºé—´ä¸­æœ‰$z$çš„æ­£æ€åˆ†å¸ƒï¼Œé‚£ä¹ˆå°±å¯ä»¥ç”¨ä¸€ä¸ªè¶³å¤Ÿå¤æ‚çš„å‡½æ•°æ¥ç”Ÿæˆä»»ä½•ç±»å‹çš„åˆ†å¸ƒï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨æ­¤å‡½æ•°çš„é€†æ¥å­¦ä¹ éšå˜é‡æœ¬èº«ã€‚</p><p>åœ¨æ–¹ç¨‹1ä¸­ï¼Œç§¯åˆ†åœ¨zçš„æ‰€æœ‰ç»´ä¸Šè¿›è¡Œï¼Œå› æ­¤éš¾ä»¥å¤„ç†ã€‚ä½†æ˜¯ï¼Œå®ƒå¯ä»¥ä½¿ç”¨è’™ç‰¹å¡ç½—ç§¯åˆ†ï¼ˆMonte-Carlo integrationï¼‰æ–¹æ³•æ¥è®¡ç®—ï¼Œè¿™ä¸å®¹æ˜“å®ç°ã€‚æ‰€ä»¥æˆ‘ä»¬é‡‡ç”¨å¦ä¸€ç§æ–¹æ³•æ¥è¿‘ä¼¼åœ°æœ€å¤§åŒ–æ–¹ç¨‹1ä¸­çš„$PÓ©(X)$ ã€‚VAEçš„æƒ³æ³•æ˜¯ä½¿ç”¨æˆ‘ä»¬ä¸çŸ¥é“çš„$P(z|X)$æ¥æ¨æ–­$P(z)$ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§ç§°ä¸º<strong>å˜åˆ†æ¨æ–­</strong>ï¼ˆvariational inferenceï¼‰çš„æ–¹æ³•æ¥æ¨æ–­$P(z|X)$ï¼Œè¿™ç§æ–¹æ³•åŸºæœ¬ä¸Šæ˜¯è´å¶æ–¯ç»Ÿè®¡ï¼ˆBayesian statisticsï¼‰ä¸­çš„ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆç”¨æ˜“äºå‘ç°çš„ç®€å•åˆ†å¸ƒ$Q(z|X)$å¯¹$P(z|X)$è¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬è¯•ç€ç”¨KLæ•£åº¦åº¦é‡ï¼ˆKL-divergence metricï¼‰æ–¹æ³•æ¥å°½é‡å‡å°$P(z|X)$å’Œ$Q(z|X)$ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„å‡è®¾æ¥è¿‘çœŸå®çš„åˆ†å¸ƒã€‚æ¥ä¸‹æ¥æ˜¯å¤§é‡çš„æ•°å­¦æ–¹ç¨‹ï¼Œç¬”è€…ä¸å†èµ˜è¿°ï¼Œå¦‚ä½ æœ‰å…´è¶£å¯åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å®ƒã€‚å¦‚æœä½ æœ‰äº†VAEçš„ç›´è§‰ï¼Œè¿™äº›æ–¹ç¨‹å°±ä¸éš¾ç†è§£äº†ã€‚</p><p>VAEçš„æœ€ç»ˆç›®æ ‡æ˜¯ï¼š</p><p><img src="/images/%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3ivp74d8j20kt01v3ye.jpg" alt="img"></p><p>ä¸Šé¢çš„æ–¹ç¨‹æœ‰ä¸€ä¸ªéå¸¸å¥½çš„è§£é‡Šã€‚æœ¯è¯­$Q(z|X)$åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬çš„ç¼–ç å™¨ç½‘ç»œï¼Œ$z$æ˜¯æˆ‘ä»¬å¯¹æ•°æ®$x(xâˆˆX)$çš„ç¼–ç è¡¨ç¤ºï¼Œ$P(X|z)$æ˜¯æˆ‘ä»¬çš„è§£ç å™¨ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨ä¸Šé¢çš„æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨$ D_{KL}[Q(z|X)||P(z|X)] $ï¼Œåœ¨æŸäº›è¯¯å·®ä¸‹ï¼Œæå¤§ä¼¼ç„¶ä¼°è®¡æ•°æ®åˆ†å¸ƒã€‚ç”±äº$P(z|X)$ä¸æ˜“å¤„ç†ï¼Œä½†KLæ•£åº¦é¡¹â‰¥0ï¼Œå› æ­¤å¾ˆå®¹æ˜“çœ‹å‡ºVAEè¯•å›¾æœ€å°åŒ–$log(P(X))$çš„ä¸‹ç•Œã€‚è¿™å’Œæœ€å¤§åŒ–$E[logP(X|z)]$å’Œæœ€å°åŒ–$D_{KL}[Q(z|X)||P(z|X)]$æ˜¯ä¸€æ ·çš„ã€‚æˆ‘ä»¬çŸ¥é“æœ€å¤§åŒ–$E[logP(X|z)]$æ˜¯ä¸€ä¸ªæå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå¹¶ä½¿ç”¨è§£ç å™¨ç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚æ­£å¦‚æˆ‘å‰é¢è¯´è¿‡çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ½œåœ¨è¡¨ç¤ºæ¥è¿‘äºé«˜æ–¯å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å‡è®¾$P(z)$ä¸º$N(0,1)$ã€‚æŒ‰ç…§è¿™ä¸ªå‡è®¾ï¼Œ$Q(z|X)$ä¹Ÿåº”è¯¥æ¥è¿‘è¿™ä¸ªåˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬å‡è®¾å®ƒæ˜¯å…·æœ‰å‚æ•°$Î¼(X)$å’Œ$Æ©(X)$çš„é«˜æ–¯åˆ†å¸ƒï¼Œåˆ™ç”±KLæ•£åº¦ç»™å‡ºçš„è¿™ä¸¤ä¸ªåˆ†å¸ƒï¼ˆå³$P(z)$å’Œ$Q(z|X)$ï¼‰ä¹‹é—´çš„å·®å¼‚å¯¼è‡´çš„è¯¯å·®çš„å°é—­å½¢å¼çš„è§£ï¼Œè§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š</p><p><img src="/images/%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3iw6bbyaj20ky02bt8o.jpg" alt="img"></p><p>æˆ‘ä»¬ä¼˜åŒ–äº†è¾ƒä½çš„å˜åˆ†è¾¹ç•Œï¼Œä¼˜åŒ–å‡½æ•°æ˜¯ï¼š</p><p>$log(P(X|z))âˆ’D_{KL}[Q(z|X)â€–P(z)]$ï¼Œç¬¬äºŒä¸ªè§£å¦‚ä¸Šå›¾æ‰€ç¤ºã€‚</p><p>å› æ­¤ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°å°†åŒ…å«ä¸¤é¡¹ã€‚ç¬¬ä¸€ä¸ªæ˜¯è¾“å…¥åˆ°è¾“å‡ºçš„<strong>é‡å»ºæŸå¤±</strong>ï¼Œç¬¬äºŒä¸ªæŸå¤±æ˜¯<strong>KLæ•£åº¦é¡¹</strong>ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰ç®—æ³•æ¥è®­ç»ƒç½‘ç»œã€‚ä½†æ˜¯æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œç¬¬ä¸€é¡¹ä¸ä»…ä¾èµ–äºPçš„å‚æ•°ï¼Œä¹Ÿä¾èµ–äºQçš„å‚æ•°ï¼Œä½†æ˜¯è¿™ä¸ªä¾èµ–æ€§å¹¶æ²¡æœ‰å‡ºç°åœ¨ä¸Šé¢çš„æ–¹ç¨‹ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬å¦‚ä½•ä»åˆ†å¸ƒ$Q(z|X)$æˆ–$N[Î¼(X), Æ©(X)]$ä¸­éšæœºç©¿è¿‡æˆ‘ä»¬æŠ½æ ·çš„å±‚zï¼Œä»¥ä¾¿På¯ä»¥è§£ç ã€‚æ¸å˜ä¸èƒ½æµè¿‡éšæœºèŠ‚ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨<strong>é‡æ–°å‚æ•°åŒ–æŠ€å·§</strong>ï¼ˆè§å›¾æ‰€ç¤ºï¼‰ä½¿ç½‘ç»œå¯å¾®ã€‚æˆ‘ä»¬ä»$N(Î¼(X), Î£(X))$ä¸­æŠ½æ ·$Îµâˆ¼N(0,I)$ï¼Œç„¶åè®¡ç®—$z=Î¼(X)+Î£1/2(X)âˆ—Îµ$ã€‚</p><p>å›¾2ä¸­æ˜¾ç¤ºçš„éå¸¸å®Œç¾ã€‚åº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œå‰é¦ˆæ­¥éª¤å¯¹äºè¿™ä¸¤ä¸ªç½‘ç»œï¼ˆå·¦ä¾§å’Œå³ä¾§ï¼‰éƒ½æ˜¯ç›¸åŒçš„ï¼Œä½†æ˜¯æ¸å˜åªèƒ½é€šè¿‡æ­£ç¡®çš„ç½‘ç»œè¿›è¡Œåå‘ä¼ æ’­ã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3iwngvudj20ic0b63z4.jpg" alt="img"></p><p>åœ¨æ¨æ–­æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä»$N(0,1)$ä¸­é‡‡æ ·$z$å¹¶å°†å…¶é¦ˆé€åˆ°è§£ç å™¨ç½‘ç»œä»¥ç”Ÿæˆæ–°çš„æ•°æ®ç‚¹ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨ä¼˜åŒ–è¾ƒä½çš„å˜åˆ†è¾¹ç•Œï¼Œå› æ­¤ç”Ÿæˆçš„å›¾åƒçš„è´¨é‡ç›¸å¯¹äºåƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼ŒGANï¼‰è¿™æ ·çš„æœ€æ–°æŠ€æœ¯æ¥è¯´æ˜¯æ¯”è¾ƒå·®çš„ã€‚</p><p>VAEæœ€å¥½çš„ä¸€ç‚¹æ˜¯å®ƒåŒæ—¶å­¦ä¹ ç”Ÿæˆæ¨¡å‹å’Œæ¨ç†æ¨¡å‹ã€‚è™½ç„¶VAEå’ŒGANéƒ½æ˜¯éå¸¸ä»¤äººå…´å¥‹çš„æ–¹æ³•ï¼Œå®ƒä»¬éƒ½å¯ä»¥ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ æ¥å­¦ä¹ åŸºç¡€æ•°æ®åˆ†å¸ƒï¼Œä½†ä¸VAEç›¸æ¯”ï¼ŒGANèƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚åœ¨VAEä¸­ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†è¾ƒä½çš„å˜åˆ†è¾¹ç•Œï¼›è€Œåœ¨GANä¸­ï¼Œæ²¡æœ‰è¿™æ ·çš„å‡è®¾ã€‚äº‹å®ä¸Šï¼ŒGANå¹¶ä¸å¤„ç†ä»»ä½•æ˜¾å¼çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡ã€‚VAEåœ¨ç”Ÿæˆæ¸…æ™°å›¾åƒæ–¹é¢çš„å¤±è´¥æ„å‘³ç€æ¨¡å‹æ— æ³•å­¦ä¹ çœŸå®çš„åéªŒåˆ†å¸ƒã€‚VANå’ŒGANä¸»è¦åœ¨è®­ç»ƒæ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒã€‚ç°åœ¨è®©æˆ‘ä»¬è¿›å…¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚</p><h2 id="ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative-Adversarial-Networksï¼‰"><a href="#ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative-Adversarial-Networksï¼‰" class="headerlink" title="ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼‰"></a><strong>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Networksï¼‰</strong></h2><p>Yann LeCunè¯´ï¼Œå¯¹æŠ—è®­ç»ƒæ˜¯æœ‰å²ä»¥æ¥æœ€é…·çš„ä¸œè¥¿ã€‚æˆ‘æƒ³å¤§å¤šæ•°äººéƒ½ä¼šåŒæ„ä»–çš„è§‚ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬éƒ½çœ‹åˆ°äº†å¯¹æŠ—ç½‘ç»œçš„å¹¿ä¸ºæµè¡Œï¼Œä»¥åŠå®ƒä»¬æ‰€äº§ç”Ÿçš„çš„ç»“æœå’Œè´¨é‡ã€‚å¯¹æŠ—è®­ç»ƒå®Œå…¨æ”¹å˜äº†æˆ‘ä»¬æ•™ä¼šç¥ç»ç½‘ç»œåšç‰¹å®šä»»åŠ¡çš„æ–¹å¼ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸ä½¿ç”¨ä»»ä½•æ˜¾å¼çš„å¯†åº¦ä¼°è®¡ï¼Œå¦‚å˜åˆ†è‡ªç¼–ç å™¨ã€‚ç›¸åï¼Œå®ƒæ˜¯åŸºäºåšå¼ˆè®ºï¼ˆgame theoryï¼‰çš„æ–¹æ³•ï¼Œç›®çš„æ˜¯åœ¨ä¸¤ä¸ªç½‘ç»œï¼Œç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„çº³ä»€å‡è¡¡ï¼ˆNash equilibriumï¼‰ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ä»ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒï¼Œæ¯”å¦‚é«˜æ–¯åˆ†å¸ƒï¼Œç„¶åå­¦ä¹ åˆ©ç”¨é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰å°†å™ªå£°è½¬åŒ–ä¸ºæ•°æ®åˆ†å¸ƒã€‚</p><p>è¿™æ˜¯é€šè¿‡è¿™ä¸¤ä¸ªç½‘ç»œçš„å¯¹æŠ—è®­ç»ƒæ¥å®ç°çš„ã€‚ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰æ¨¡å‹Gå­¦ä¹ æ•æ‰æ•°æ®åˆ†å¸ƒï¼Œåˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰æ¨¡å‹Dä¼°è®¡æ ·æœ¬æ¥è‡ªæ•°æ®åˆ†å¸ƒè€Œéæ¨¡å‹åˆ†å¸ƒçš„æ¦‚ç‡ã€‚åŸºæœ¬ä¸Šï¼Œç”Ÿæˆå™¨çš„ä¸»è¦ä»»åŠ¡æ˜¯ç”Ÿæˆè‡ªç„¶çš„å›¾åƒï¼Œè€Œåˆ¤åˆ«å™¨çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å›¾åƒæ˜¯å‡çš„è¿˜æ˜¯çœŸå®çš„ã€‚è¿™å¯ä»¥è®¤ä¸ºæ˜¯ä¸€æ¬¾è¿·ä½ maxçš„åŒäººæ¸¸æˆï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸¤ä¸ªç½‘ç»œçš„æ€§èƒ½éƒ½ä¼šæé«˜ã€‚åœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œç”Ÿæˆå™¨è¯•å›¾é€šè¿‡ç”ŸæˆçœŸå®å›¾åƒæ¥æ¬ºéª—åˆ¤åˆ«å™¨ï¼Œå¹¶é€šè¿‡æé«˜å…¶è¯†åˆ«èƒ½åŠ›æ¥é¿å…è¢«åˆ¤åˆ«å™¨è¿·æƒ‘ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†GANçš„åŸºæœ¬æ¶æ„ã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3ix3jigbj20m80gfmxs.jpg" alt="img"></p><p>æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå…ˆéªŒè¾“å…¥å™ªå£°å˜é‡P(z)ï¼Œç„¶åç”Ÿæˆå™¨å°†å…¶æ˜ å°„åˆ°ä½¿ç”¨å…·æœ‰å‚æ•°Ó©gçš„å¤å¾®åˆ†å‡½æ•°çš„æ•°æ®åˆ†å¸ƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰å¦ä¸€ä¸ªç½‘ç»œç§°ä¸ºåˆ¤åˆ«å™¨ï¼Œå®ƒæ¥å—è¾“å…¥xå¹¶ä½¿ç”¨å¦ä¸€ä¸ªå¸¦å‚æ•°çš„å¾®åˆ†å‡½æ•°ã€‚è¾“å‡ºè¡¨ç¤ºxæ¥è‡ªçœŸå®æ•°æ®åˆ†å¸ƒPdata(x)çš„æ¦‚ç‡çš„å•ä¸ªæ ‡é‡å€¼ã€‚GANçš„ç›®æ ‡å‡½æ•°è¢«å®šä¹‰ä¸ºï¼š</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3jopefm2j20i801jaa2.jpg" alt="img"></p><p>åœ¨ä¸Šé¢çš„æ–¹ç¨‹ä¸­ï¼Œå¦‚æœåˆ¤åˆ«å™¨çš„è¾“å…¥æ¥è‡ªçœŸå®çš„æ•°æ®åˆ†å¸ƒï¼Œé‚£ä¹ˆD(x)åº”è¯¥è¾“å‡º1æ¥æœ€å¤§åŒ–ä¸Šè¿°å…³äºDçš„ç›®æ ‡å‡½æ•°ï¼Œå¦‚æœå›¾åƒæ˜¯ç”±ç”Ÿæˆå™¨ç”Ÿæˆçš„ï¼Œé‚£ä¹ˆD(G(z))åº”è¯¥è¾“å‡º1ï¼Œä»¥ä½¿å…³äºGçš„ç›®æ ‡å‡½æ•°æœ€å°åŒ–ã€‚ä»æœ¬è´¨ä¸Šè¯´ï¼ŒGåº”è¯¥ç”Ÿæˆè¿™æ ·çš„æ˜¾ç¤ºå›¾åƒï¼Œå®ƒå¯ä»¥æ¬ºéª—Dã€‚é‡‡ç”¨æ¢¯åº¦ä¸Šå‡æ³•ï¼ˆGradient Ascentï¼‰ä½¿æœ‰å…³åˆ¤åˆ«å™¨çš„å‚æ•°æœ€å¤§åŒ–ï¼Œé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Descentï¼‰æœ€å°åŒ–æœ‰å…³ç”Ÿæˆå™¨çš„å‚æ•°ã€‚ä½†æ˜¯åœ¨ä¼˜åŒ–ç”Ÿæˆå™¨ç›®æ ‡æ–¹é¢å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨æ¸¸æˆå¼€å§‹æ—¶ï¼Œç”Ÿæˆå™¨è¿˜æ²¡æœ‰å­¦åˆ°ä»»ä½•ä¸œè¥¿æ—¶ï¼Œæ¢¯åº¦é€šå¸¸éå¸¸å°ï¼Œå½“è¿è¡Œè‰¯å¥½æ—¶ï¼Œæ¢¯åº¦éå¸¸é«˜ï¼ˆè§å›¾4ï¼‰ã€‚ä½†æˆ‘ä»¬æƒ³è¦çš„æ˜¯ç›¸åçš„è¡Œä¸ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†E[log(D(G(z))]æœ€å¤§åŒ–ï¼Œè€Œä¸æ˜¯æœ€å°åŒ–E[log(1-D(G(z))]ã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3jp2qwnjj20ko09iwf0.jpg" alt="img"></p><p>è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descentï¼‰å¯¹åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„åŒæ­¥åº”ç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬äº¤æ›¿ä¼˜åŒ–Dçš„kçº§æ­¥éª¤å’Œå°æ‰¹é‡ä¸Šä¼˜åŒ–Gçš„ä¸€æ­¥ã€‚å½“åˆ¤åˆ«å™¨æ— æ³•åŒºåˆ†Ïgå’ŒÏdataæ—¶ï¼Œå³D(x,Ó©d)=Â½æˆ–è€…Ïg=Ïdataæ—¶ï¼Œåˆ™åœæ­¢è®­ç»ƒè¿‡ç¨‹ã€‚</p><p>GANåº”ç”¨å·ç§¯ç¥ç»ç½‘ç»œæœ€æ—©çš„æ¨¡å‹ä¹‹ä¸€æ˜¯DCGANï¼Œå®ƒä»£è¡¨äº†æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDeep Convolutional Generative Adversarial Networksï¼ŒDCGANï¼‰ã€‚è¯¥ç½‘ç»œä»å‡åŒ€åˆ†å¸ƒä¸­æå–çš„100ä¸ªéšæœºæ•°ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ‰€éœ€å½¢çŠ¶çš„å›¾åƒã€‚è¯¥ç½‘ç»œç”±è®¸å¤šå·ç§¯ã€åå·ç§¯å’Œå®Œå…¨è¿é€šçš„å±‚ç»„æˆã€‚ç½‘ç»œä½¿ç”¨è®¸å¤šåå·ç§¯å±‚å°†è¾“å…¥å™ªå£°æ˜ å°„åˆ°æ‰€éœ€çš„è¾“å‡ºå›¾åƒã€‚æ‰¹é‡æ ‡å‡†åŒ–ç”¨äºç¨³å®šç½‘ç»œçš„è®­ç»ƒã€‚é™¤äº†ä½¿ç”¨tanhå±‚å’ŒLeaky ReLUçš„è¾“å‡ºå±‚å¤–ï¼Œæ‰€æœ‰å±‚éƒ½ä½¿ç”¨ReLUæ¿€æ´»ã€‚è¯¥ç½‘ç»œä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨Adamä¼˜åŒ–å™¨æ¥åŠ é€Ÿè®­ç»ƒï¼Œå¯¹è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç¯‡è®ºæ–‡çš„ç»“æœå¾ˆæœ‰è¶£ã€‚ä½œè€…æŒ‡å‡ºï¼Œè¿™äº›ç”Ÿæˆå™¨å…·æœ‰åˆå»çš„å‘é‡è¿ç®—æ€§è´¨ï¼Œå¯ä»¥ç”¨æˆ‘ä»¬æƒ³è¦çš„æ–¹å¼æ¥å¤„ç†å›¾åƒã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3jphktgaj20lj09ogmo.jpg" alt="img"></p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3jpzkwdaj20el06y0ta.jpg" alt="img"></p><p>GANä½¿ç”¨æœ€å¹¿æ³›çš„å˜ä½“ä¹‹ä¸€æ˜¯æœ‰æ¡ä»¶çš„GANï¼ˆconditional GANï¼ŒcGANï¼‰ï¼Œå®ƒæ˜¯é€šè¿‡ç®€å•åœ°å°†æ¡ä»¶å‘é‡ä¸å™ªå£°å‘é‡ä¸€èµ·æ·»åŠ è€Œæ„æˆçš„ï¼ˆè§å›¾7ï¼‰ã€‚åœ¨cGANä¹‹å‰ï¼Œæˆ‘ä»¬ä»éšæœºçš„å™ªå£°æ ·æœ¬zä¸­éšæœºç”Ÿæˆå›¾åƒã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦ç”Ÿæˆå…·æœ‰æŸäº›æ‰€éœ€ç‰¹å¾çš„å›¾åƒï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆæ–¹æ³•å¯ä»¥ä¸ºæ¨¡å‹æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œæ— è®ºæˆ‘ä»¬æƒ³è¦ç”Ÿæˆä»€ä¹ˆæ ·çš„å›¾åƒï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œæœ‰æ¡ä»¶çš„GANæ˜¯è¿™æ ·åšçš„ã€‚é€šè¿‡å¯¹æä¾›ç»™ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„é™„åŠ ä¿¡æ¯è°ƒæ•´æ¨¡å‹ï¼Œå¯ä»¥æŒ‡å¯¼æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚æœ‰æ¡ä»¶çš„GANç”¨äºå„ç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€å›¾åƒçš„è‡ªåŠ¨æ ‡è®°ç­‰ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†è¿™ä¸¤ä¸ªç½‘ç»œçš„ç»Ÿä¸€ç»“æ„ã€‚</p><p><img src="%E6%8E%A2%E5%B9%BD%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%20-%20VAE%E5%92%8CGAN/7dcbe4ffly1fo3jq9swe9j20jr0ffwfm.jpg" alt="img"></p><p>GANçš„ä¸€ä¸ªå¾ˆé…·çš„åœ°æ–¹å°±æ˜¯ï¼Œå³ä½¿è®­ç»ƒæ•°æ®å¾ˆå°ï¼Œä¹Ÿå¯ä»¥è¿›è¡Œè®­ç»ƒã€‚ç¡®å®ï¼ŒGANçš„ç»“æœæ˜¯æœ‰å¸Œæœ›çš„ï¼Œä½†è®­ç»ƒè¿‡ç¨‹å¹¶ä¸ç®€å•ï¼Œå°¤å…¶æ˜¯å»ºç«‹ç½‘ç»œçš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼ŒGANå¾ˆéš¾è¿›è¡Œä¼˜åŒ–ï¼Œå› ä¸ºå®ƒä»¬ä¸å®¹æ˜“æ”¶æ•›ã€‚å½“ç„¶ï¼Œæœ‰ä¸€äº›æŠ€å·§å’Œçªé—¨æ¥ç ´è§£GANï¼Œä½†å®ƒä»¬å¯èƒ½å¹¶éæ€»æ˜¯æœ‰ç”¨çš„ã€‚ä½ å¯ä»¥è®¿é—®<code>https://github.com/soumith/ganhacks</code>æ‰¾åˆ°ä¸€äº›å»ºè®®ã€‚å¦å¤–ï¼Œé™¤äº†æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒæ˜¯å¦çœ‹ä¸Šå»å¾ˆçœŸå®ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯¹ç»“æœçš„å®šé‡è¯„ä»·æ²¡æœ‰ä»»ä½•æ ‡å‡†ã€‚</p><h2 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a><strong>ç»“è®º</strong></h2><p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç›‘ç£å­¦ä¹ ä¸­ç¡®å®è¾¾åˆ°äº†äººç±»æ°´å¹³çš„è¡¨ç°ï¼Œä½†åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­å´ä¸æ˜¯è¿™æ ·ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶æ·±åº¦å­¦ä¹ çš„ç§‘å­¦å®¶ä»¬æ­£åœ¨åŠªåŠ›æ”¹è¿›æ— ç›‘ç£æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸¤ä¸ªæœ€è‘—åçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶çš„ç”Ÿæˆæ¨¡å‹æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘ä»¬äº†è§£äº†å˜åˆ†è‡ªç¼–ç å™¨çš„é—®é¢˜ï¼Œä»¥åŠä¸ºä»€ä¹ˆå¯¹æŠ—ç½‘ç»œèƒ½æ›´å¥½åœ°ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ä½†æ˜¯GANä¹Ÿå­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ç¨³å®šå®ƒä»¬çš„è®­ç»ƒï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚ç„¶è€ŒGANéå¸¸å¼ºå¤§ï¼Œç›®å‰å®ƒä»¬æ­£è¢«ç”¨äºé«˜è´¨é‡å›¾åƒï¼ˆå‚è§ä»¥ä¸‹è§†é¢‘ï¼‰å’Œè§†é¢‘ç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒçš„è½¬æ¢ã€å›¾åƒå¢å¼ºã€å›¾åƒä¸­ç‰©ä½“3Dæ¨¡å‹é‡å»ºã€éŸ³ä¹ç”Ÿæˆã€å‘ç°æŠ—ç™Œè¯ç‰©ç­‰ç­‰ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè®¸å¤šæ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜ä¹Ÿåœ¨åŠªåŠ›ç»Ÿä¸€è¿™ä¸¤ç§æ¨¡å‹ï¼Œå¹¶ä½¿è¿™ä¸¤ç§æ¨¡å‹å¾—åˆ°æœ€å¥½çš„ç»“æœã€‚éšç€æ·±åº¦å­¦ä¹ çš„ä¸æ–­æé«˜ï¼Œæˆ‘ç›¸ä¿¡GANå°†ä¼šæ‰“å¼€äººå·¥æ™ºèƒ½çš„å°é—­ä¹‹é—¨ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ å¹´é‡Œï¼Œç”Ÿæˆæ¨¡å‹å°†å¯¹äºå›¾å½¢è®¾è®¡ã€è®¾è®¡æœ‰å¸å¼•åŠ›çš„ç”¨æˆ·ç•Œé¢ç­‰éå¸¸æœ‰å¸®åŠ©ã€‚ä¹Ÿå¯èƒ½ä¼šä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥ç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚</p><p>[1]: <em>Deep Generative Models\</em><br><a href="https://towardsdatascience.com/deep-generative-models-25ab2821afd3" target="_blank" rel="noopener">https://towardsdatascience.com/deep-generative-models-25ab2821afd3</a></p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ä»å›¾åµŒå…¥ç®—æ³•åˆ°å›¾ç¥ç»ç½‘ç»œ</title>
      <link href="/cong-tu-qian-ru-suan-fa-dao-tu-shen-jing-wang-luo/"/>
      <url>/cong-tu-qian-ru-suan-fa-dao-tu-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://blog.csdn.net/weixin_43269174/article/details/98492487" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43269174/article/details/98492487</a></p><hr><p>ä¸€ã€å¼•è¨€<br>è¿‘å‡ å¹´æ¥ï¼Œä¼´éšç€è®¡ç®—æœºç®—åŠ›çš„æ€¥å‰§æå‡ï¼Œç¥ç»ç½‘ç»œä»å†å²çš„å°˜åŸƒä¸­èµ°å‡ºï¼Œæ¨ªæ‰«å„å¤§é¢†åŸŸï¼Œå®Œæˆä¸€æ¬¡æ¬¡é¢ è¦†æ€§çš„åˆ›æ–°ã€‚ä¾æ‰˜é«˜åº¦å¼¹æ€§çš„å‚æ•°ç»“æ„ï¼Œçº¿æ€§ä¸éçº¿æ€§çš„çŸ©é˜µå˜æ¢ï¼Œç¥ç»ç½‘ç»œèƒ½é€‚ç”¨äºå„å¼å„æ ·çš„æ•°å­¦åœºæ™¯ï¼Œåœ¨å„ä¸ªç±»åˆ«çš„åº”ç”¨ä¸Šæˆ‘ä»¬éƒ½èƒ½çœ‹åˆ°ç¥ç»ç½‘ç»œçš„å½±å­ã€‚å…¶ä¸­è‘—åçš„åº”ç”¨æ–¹å‘ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€æœºå™¨å­¦ä¹ ã€ç”Ÿç‰©åŒ»ç–—ã€æ¨èç³»ç»Ÿã€è‡ªåŠ¨é©¾é©¶ç­‰ç­‰ã€‚å›¾ç¥ç»ç½‘ç»œï¼Œå¹¿æ³›åº”ç”¨äºç¤¾äº¤å…³ç³»ã€çŸ¥è¯†å›¾è°±ã€æ¨èç³»ç»Ÿã€è›‹ç™½è´¨åˆ†å­å»ºæ¨¡ï¼ŒåŒæ ·æºè‡ªäºå¯¹ä¼ ç»Ÿé¢†åŸŸçš„åˆ›æ–°ï¼Œå®ƒçš„å‰èº«æ˜¯å›¾åµŒå…¥ç®—æ³•ï¼›è€Œå›¾åµŒå…¥ç®—æ³•åˆä»¥å›¾æ•°æ®ä½œä¸ºè½½ä½“ã€‚è¿™ä¸€å…³ç³»ï¼Œå°†è´¯ç©¿æœ¬æ–‡å§‹æœ«ï¼Œæˆä¸ºæˆ‘ä»¬çš„å±•å¼€çº¿ç´¢ã€‚</p><p>äºŒã€å›¾<br>åœ¨è¿›å…¥å›¾åµŒå…¥ç®—æ³•å‰ï¼Œæœ¬èŠ‚å°†è¯¦ç»†ä»‹ç»è¯¥é¢†åŸŸä¸‹çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬å„ç±»å›¾çš„æ¦‚å¿µä»¥åŠè·¯å¾„ç›¸å…³ç®—æ³•ã€‚å¸Œæœ›ç›´å…¥ä¸»é¢˜çš„è¯»è€…å¯ç›´æ¥è·³åˆ°ä¸‹ä¸€èŠ‚ã€‚</p><p>ç›¸å…³æ¦‚å¿µ</p><h2 id="gt-å›¾-Graph-æ˜¯æœ€åŸºç¡€çš„å‡ ç§è®¡ç®—æœº-æ•°æ®ç»“æ„-ä¹‹ä¸€ï¼Œç”±è‹¥å¹²ä¸ª-èŠ‚ç‚¹-Node-or-Vertex-æ„æˆï¼›èŠ‚ç‚¹ä¸èŠ‚ç‚¹ç›¸è¿ï¼Œæ„æˆ-è¾¹-Edge-ï¼Œä»£è¡¨äº†ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ ¹æ®å›¾ä¸­è¾¹çš„æ–¹å‘ï¼Œæ¦‚å¿µä¸Šå¯å°†å›¾åˆ†ä¸ºä¸¤ç§ï¼šæœ‰å‘å›¾-Directed-Graph-or-Digraph-å’Œ-æ— å‘å›¾-Undirected-Graph-or-Undigraph-ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå·¦ä¾§ä¸ºæ— å‘å›¾ï¼Œå³ä¾§ä¸ºæœ‰å‘å›¾ï¼š"><a href="#gt-å›¾-Graph-æ˜¯æœ€åŸºç¡€çš„å‡ ç§è®¡ç®—æœº-æ•°æ®ç»“æ„-ä¹‹ä¸€ï¼Œç”±è‹¥å¹²ä¸ª-èŠ‚ç‚¹-Node-or-Vertex-æ„æˆï¼›èŠ‚ç‚¹ä¸èŠ‚ç‚¹ç›¸è¿ï¼Œæ„æˆ-è¾¹-Edge-ï¼Œä»£è¡¨äº†ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ ¹æ®å›¾ä¸­è¾¹çš„æ–¹å‘ï¼Œæ¦‚å¿µä¸Šå¯å°†å›¾åˆ†ä¸ºä¸¤ç§ï¼šæœ‰å‘å›¾-Directed-Graph-or-Digraph-å’Œ-æ— å‘å›¾-Undirected-Graph-or-Undigraph-ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå·¦ä¾§ä¸ºæ— å‘å›¾ï¼Œå³ä¾§ä¸ºæœ‰å‘å›¾ï¼š" class="headerlink" title="&gt; å›¾ (Graph) æ˜¯æœ€åŸºç¡€çš„å‡ ç§è®¡ç®—æœº æ•°æ®ç»“æ„ ä¹‹ä¸€ï¼Œç”±è‹¥å¹²ä¸ª èŠ‚ç‚¹ (Node, or Vertex) æ„æˆï¼›èŠ‚ç‚¹ä¸èŠ‚ç‚¹ç›¸è¿ï¼Œæ„æˆ è¾¹ (Edge)ï¼Œä»£è¡¨äº†ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ ¹æ®å›¾ä¸­è¾¹çš„æ–¹å‘ï¼Œæ¦‚å¿µä¸Šå¯å°†å›¾åˆ†ä¸ºä¸¤ç§ï¼šæœ‰å‘å›¾ (Directed Graph, or Digraph) å’Œ æ— å‘å›¾ (Undirected Graph, or Undigraph)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå·¦ä¾§ä¸ºæ— å‘å›¾ï¼Œå³ä¾§ä¸ºæœ‰å‘å›¾ï¼š"></a>&gt; å›¾ (Graph) æ˜¯æœ€åŸºç¡€çš„å‡ ç§è®¡ç®—æœº æ•°æ®ç»“æ„ ä¹‹ä¸€ï¼Œç”±è‹¥å¹²ä¸ª èŠ‚ç‚¹ (Node, or Vertex) æ„æˆï¼›èŠ‚ç‚¹ä¸èŠ‚ç‚¹ç›¸è¿ï¼Œæ„æˆ è¾¹ (Edge)ï¼Œä»£è¡¨äº†ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æ ¹æ®å›¾ä¸­è¾¹çš„æ–¹å‘ï¼Œæ¦‚å¿µä¸Šå¯å°†å›¾åˆ†ä¸ºä¸¤ç§ï¼šæœ‰å‘å›¾ (Directed Graph, or Digraph) å’Œ æ— å‘å›¾ (Undirected Graph, or Undigraph)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œå·¦ä¾§ä¸ºæ— å‘å›¾ï¼Œå³ä¾§ä¸ºæœ‰å‘å›¾ï¼š</h2><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2019080517150678.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>å½“è¾¹è¢«èµ‹äºˆæƒé‡ï¼Œåˆ™å›¾å¯ç§°ä¸º <strong>æƒé‡å›¾</strong> (Weighted Graph)ï¼š</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805171650651.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>ä¸€ä¸ªå½¢è±¡çš„ä¾‹å­æ˜¯åŸå¸‚åœ°å›¾ï¼Œæ¯ä¸€ä¸ªäº¤å‰è·¯å£æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé“è·¯æ˜¯ä¸€æ¡è¾¹ï¼Œè€Œæƒé‡æŒ‡çš„åˆ™æ˜¯é“è·¯çš„é•¿åº¦ã€‚ä½†å¦‚æœæˆ‘ä»¬å¸Œæœ›ç”¨æƒé‡å¤§å°ä»£è¡¨æ‹¥æŒ¤ç¨‹åº¦ï¼Œåœ¨åœ°å›¾ä¸Šçœ‹åˆ°æ¯æ¡é“è·¯çš„æ‹¥å µæƒ…å†µï¼Œé‚£ä¹ˆä¸€æ¡è¾¹æ˜¾ç„¶ä¸è¶³ä»¥æ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚ã€‚è¿™æ—¶å°±å¼•ç”³å‡ºäº† <strong>å¤šé‡å›¾</strong> (Multigraph)ï¼š</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805171912926.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>æ¦‚å¿µä¸Šï¼Œå¤šé‡å›¾å¿…ç„¶æ˜¯æœ‰å‘å›¾å’Œæƒé‡å›¾ï¼›ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤šé‡å›¾ä¸­ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼Œæ—¢å¯ä»¥å•å‘ï¼Œä¹Ÿå¯ä»¥åŒå‘ï¼Œi.e. èŠ‚ç‚¹ AAA å’Œ BBB ä¹‹é—´å¯ä»¥æœ‰ä¸¤æ¡æˆ–ä»¥ä¸Š Aâ†’BA\rightarrow BAâ†’B çš„è¾¹ï¼›ä¸¤æ¡æˆ–ä¸¤æ¡ä»¥ä¸Šçš„å•å‘è¾¹æˆä¸º å¹³è¡Œè¾¹ (Parallel Edges)ï¼Œè€Œå¹³è¡Œè¾¹çš„æ•°é‡ç§°ä¸º é‡æ•° (Multiplicity)ã€‚</p><p>æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–ç±»å‹å›¾çš„å®šä¹‰ï¼ŒåŒ…æ‹¬ æ··åˆå›¾ (Mixed Graph)ï¼ŒæŒ‡çš„æ˜¯æ—¢åŒ…å«æ— å‘è¾¹ä¹ŸåŒ…å«æœ‰å‘è¾¹çš„å›¾ï¼›è¿é€šå›¾ (Connected Graph)ï¼ŒæŒ‡çš„æ˜¯ä»»æ„ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æœ‰è·¯å¾„ (ä¸€ä¸ªæˆ–å¤šä¸ªè¾¹ç›¸è¿) ç›¸è¿çš„æ— å‘å›¾ï¼›å¼ºè¿é€šå›¾ (Strongly-connected Graph)ï¼ŒæŒ‡çš„æ˜¯ä»»æ„ä¸¤ä¸ªèŠ‚ç‚¹éƒ½æœ‰è·¯å¾„ç›¸è¿çš„æœ‰å‘å›¾ï¼›å¾ªç¯å›¾ (Cyclic Graph)ï¼ŒæŒ‡çš„æ˜¯å­˜åœ¨é¦–å°¾ç›¸è¿çš„è·¯å¾„ï¼Œå¯ä»¥ä¸²èµ·æ‰€æœ‰èŠ‚ç‚¹çš„å›¾ï¼›ä»¥åŠæœ€åçš„ å®Œå…¨å›¾ (Complete Graph)ï¼ŒæŒ‡çš„æ˜¯æ‰€æœ‰èŠ‚ç‚¹ä¹‹é—´éƒ½æœ‰è¾¹ç›¸è¿çš„æ— å‘å›¾ã€‚</p><p>æ¸…æ¥šè¿™äº›å›¾çš„æ¦‚å¿µï¼Œæ˜¯æˆ‘ä»¬ç†è§£ç®—æ³•ã€ç†Ÿæ‚‰ç®—æ³•åº”ç”¨åœºæ™¯çš„å‰æã€‚</p><p>è·¯å¾„ç›¸å…³ç®—æ³•<br>ä¼´éšç€å›¾ä¸€èµ·è¯ç”Ÿçš„ï¼Œæ˜¯ä¸è·¯å¾„ç›¸å…³çš„ç®—æ³•ï¼Œå…¶ä¸­éƒ¨åˆ†ç®—æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä»å›¾ä¸­æå–æ›´å¤šçš„ç‰¹å¾ä¿¡æ¯èå…¥åˆ°èŠ‚ç‚¹ä¸­ï¼Œä»è€Œä¸°å¯Œå›¾çš„æ¶æ„ï¼Œåœ¨å›¾åµŒå…¥æˆ–å›¾ç¥ç»ç½‘ç»œç®—æ³•ä¸­è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚</p><ul><li>æ‹“æ‰‘æ’åº (Topological Sorting)ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805175452934.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li></ul><p>åº”ç”¨äºæœ‰å‘éå¾ªç¯å›¾ï¼Œæ˜¯å¯¹å›¾ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ ViV_iV<br>i</p><p>  (i=1,â€¦,n)(i=1,â€¦,n)(i=1,â€¦,n) è¿›è¡Œç»Ÿä¸€æ’åºï¼Œä½¿å¾—å¯¹äºä»»æ„è¾¹ (Vaâ†’Vb)(V_a\rightarrow V_b)(V<br>a</p><p> â†’V<br>b</p><p> )ï¼Œæ»¡è¶³ a&lt;ba&lt;ba&lt;bã€‚ç»è¿‡æ‹“æ‰‘æ’åºçš„å›¾ï¼Œèƒ½åŠ é€Ÿç›®æ ‡æ£€ç´¢çš„æ•ˆç‡ï¼ŒåŒæ—¶èƒ½å¤Ÿå¿«é€Ÿè·å–ä¸¤ä¸ªèŠ‚ç‚¹é—´çš„ä¸Šä¸‹æ¸¸ä½ç½®ï¼Œåº”ç”¨åœºæ™¯åŒ…æ‹¬å­¦ä½è¯¾ç¨‹ä¹‹é—´çš„å…ˆä¿®å…³ç³»ã€é¢å¯¹å¯¹è±¡ç¨‹åºç±»ä¹‹é—´çš„ç»§æ‰¿ï¼Œä»¥åŠå·¥ç¨‹é¡¹ç›®ä¹‹é—´çš„è°ƒåº¦ç­‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€ä¸ªæœ‰å‘éå¾ªç¯å›¾å¯èƒ½å­˜åœ¨ä¸æ­¢ä¸€ç§æ‹“æ‰‘æ’åºçš„ç»“æœã€‚ç®—æ³•åŸç†åœ¨äºé€šè¿‡è¿­ä»£ï¼Œå°†æ— å…¥åº¦çš„èŠ‚ç‚¹ä»åŸå›¾ä¸­æŠ½å‡ºæ”¾å…¥æ’åºåºåˆ—ä¸­ï¼Œç›´è‡³æ‰€æœ‰èŠ‚ç‚¹å…¨éƒ¨æŠ½å‡ºã€‚</p><ul><li>æ·±åº¦ä¼˜å…ˆæœç´¢ (DFS, abbr. Depth-First Searching)ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2019080517580351.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li></ul><p>é€šè¿‡éå†æ£€æµ‹ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦è¿é€šï¼Œæˆ–æ£€æµ‹ä¸€ä¸ªå›¾æ˜¯å¦ä¸ºè¿é€šå›¾ã€‚å…¶è¿‡ç¨‹ç±»ä¼¼äºç‰µç€ç»³å­èµ°å…¥è¿·å®«ï¼Œæ¯ä¸€ä¸ªæ‹è§’æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå½“èµ°åˆ°æ­»è§’æ—¶ï¼Œè®°å½•æ¥è¿‡è¿™é‡Œï¼Œæ²¿ç€ç»³å­çš„è·¯çº¿è¿”å›å¯»æ‰¾ä¸‹ä¸€ä¸ªæ‹è§’ï¼›è¿™å°†ç”¨åˆ°ä¸¤ä¸ªæ ˆï¼Œåˆ†åˆ«è®°å½•èµ°è®¿è¿‡çš„èŠ‚ç‚¹ï¼Œä»¥åŠç»³å­æ²¿è·¯çš„æ‹è§’ã€‚å®é™…åº”ç”¨ä¸­ï¼Œå…¨ç¨‹åªç”¨ä¸€æ ¹ç»³å­æ— ç–‘æ˜¯ä½æ•ˆçš„ï¼Œå› æ­¤å¸¸å¸¸å¼•å…¥é€’å½’çš„æ€æƒ³ï¼Œæ¯åˆ°ä¸€ä¸ªæ‹ç‚¹åˆ‡å‡ºå¤šä¸ªç»³å¤´åˆ†å¤´æœç´¢ã€‚</p><ul><li><strong>å¹¿åº¦ä¼˜å…ˆæœç´¢</strong> (BFS, abbr. Breadth-First Searching)ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805180358782.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></li></ul><p>ä¾æ®ä»æºç‚¹å‡ºå‘ï¼Œè·¯å¾„ä¸Šçš„èŠ‚ç‚¹æ•°é‡ï¼Œå°†å…¨å›¾åˆ†ä¸ºä¸åŒçš„å±‚çº§ï¼Œé€å±‚å‘ä¸‹æ£€æŸ¥ã€‚ç›¸å¯¹äºæ·±åº¦ä¼˜å…ˆæœç´¢ï¼Œå¹¿åº¦ä¼˜å…ˆæœç´¢èƒ½å¤Ÿä¿è¯åœ¨è¾¹çš„æ•°é‡ä¸Šä¸¤ä¸ªèŠ‚ç‚¹é—´æ£€ç´¢åˆ°çš„è·¯å¾„æœ€çŸ­ã€‚</p><ul><li><p><strong>Dijkstra ç®—æ³•</strong>ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805185252730.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>ç®—æ³•æ€æƒ³åœ¨äºä»æºç‚¹å‡ºå‘ï¼Œæ„å»ºä¸€ä¸ªé€æ­¥æ‰©å¼ çš„â€œäº‘â€ï¼Œæ¯æ¬¡è¿­ä»£å°†äº‘å¤–ç¦»æºç‚¹æœ€è¿‘çš„èŠ‚ç‚¹æ‹‰å…¥åˆ°äº‘å†…æ¥ï¼Œä½¿äº‘é€æ¸éå¸ƒå…¨å›¾ï¼Œä»è€Œæ£€ç´¢åˆ°ä¸¤ä¸ªèŠ‚ç‚¹é—´çš„æœ€çŸ­è·¯å¾„ã€‚æ—¶é—´å¤æ‚åº¦ä¸º O(n2)O(n^2)O(n<br>2<br> )ï¼Œæ˜¯è´ªå¿ƒç®—æ³•åº”ç”¨åœ¨è·¯å¾„é—®é¢˜ä¸Šçš„ç»ä½³æ¡ˆä¾‹ã€‚</p></li><li><p><strong>Floyd-Warshall ç®—æ³•</strong>ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805184921399.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p></li></ul><p>å‡å¦‚æˆ‘ä»¬ä¸å¸Œæœ›æ¯æ¬¡æ£€ç´¢ä¸¤ä¸ªèŠ‚ç‚¹æ˜¯å¦è¿é€šï¼Œæˆ–è®¡ç®—æœ€çŸ­è·¯å¾„æ—¶ï¼Œéƒ½ä»å¤´å¼€å§‹éå†ï¼Œå¯ä»¥ä½¿ç”¨è¯¥ç®—æ³•ç”Ÿæˆ ä¼ é€’é—­åŒ… (Transitive Closure)ï¼Œä»¥åŠ é€Ÿåç»­æ£€ç´¢ï¼Œä¸€åŠ³æ°¸é€¸ã€‚è¯¥ç®—æ³•çš„åŸç†åœ¨äºæ¯æ¬¡è¿­ä»£æ—¶ï¼Œå°†æ‰€æœ‰æ»¡è¶³è¿é€šè¦æ±‚çš„ Vkâˆ’1â†’Vkâ†’Vk+1V_{k-1}\rightarrow V_k\rightarrow V_{k+1}V<br>kâˆ’1</p><p> â†’V<br>k</p><p> â†’V<br>k+1</p><p>  çš„ Vkâˆ’1V_{k-1}V<br>kâˆ’1</p><p>  å’Œ Vk+1V_{k+1}V<br>k+1</p><p>  å•ç‹¬å»ºç«‹è”ç³»ï¼Œæ„å»ºæ–°çš„è¾¹ Vkâˆ’1â†’Vk+1V_{k-1}\rightarrow V_{k+1}V<br>kâˆ’1</p><p> â†’V<br>k+1</p><p> ã€‚å¦‚æ­¤ä¸€æ¥ï¼Œå¤šæ¬¡è¿­ä»£è¿‡åï¼Œæ‰€æœ‰å¯è¿é€šçš„èŠ‚ç‚¹å¯¹ (VaV_aV<br>a</p><p> ,VbV_bV<br>b</p><p> ) éƒ½å°†æ‹¥æœ‰ç›´æ¥å…³ç³»ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸º O(kn3)O(kn^3)O(kn<br>3<br> )ï¼Œkkk ä¸ºè¿­ä»£æ¬¡æ•°ï¼Œä¸ºä¿è¯æ‰€æœ‰å¯è¾¾èŠ‚ç‚¹é…å¯¹å­˜åœ¨ç›´æ¥ç›¸è¿çš„è¾¹ï¼Œç®—æ³•çš„è¿ç®—æ¶ˆè€—æœ€é«˜å¯æ¥è¿‘ O(n4)O(n^4)O(n<br>4<br> )ã€‚æ˜¾è€Œæ˜“è§ï¼Œå½“èŠ‚ç‚¹çš„æ•°é‡ nnn é€æ¸å¢åŠ æ—¶ï¼Œç®—æ³•è¿è¡Œçš„æ—¶é—´æ¶ˆè€—å°†å‘ˆç¾éš¾æ€§åœ°å¢åŠ ã€‚</p><ul><li><strong>Prim-Jarnik ç®—æ³•</strong>ï¼š<br><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805190412487.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" style="zoom:33%;" /></li></ul><p>å½’å±äº æœ€å°ç”Ÿæˆæ ‘ (MST, abbr. Minimum-Spanning-Tree) ä¸€ç±»çš„ç®—æ³•ï¼Œæ—¨åœ¨æ±‚è§£è¿é€šæ‰€æœ‰èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚ç”± Dijkstra ç®—æ³•è°ƒæ•´è€Œæ¥ï¼Œä»¥æ‰€æœ‰é›¶å…¥åº¦èŠ‚ç‚¹ä½œä¸ºäº‘çš„åˆå§‹çŠ¶æ€ï¼Œä¸æ–­æ‰¾å¯»ç¦»äº‘å†…èŠ‚ç‚¹æœ€è¿‘çš„é‚»ç‚¹æ‹‰å…¥åˆ°äº‘å†…æ¥ï¼Œä»¥æ­¤è¿­ä»£ï¼Œç›´è‡³æ‰€æœ‰èŠ‚ç‚¹è®¿é—®å®Œæ¯•ã€‚å¦‚æœä¸å­˜åœ¨é›¶å…¥åº¦ç‚¹ï¼Œåˆ™éšæœºæŒ‘é€‰ä¸€ä¸ªåŠ å…¥åˆ°äº‘ä¸­ã€‚</p><h1 id="ä¸‰ã€å›¾åµŒå…¥ç®—æ³•"><a href="#ä¸‰ã€å›¾åµŒå…¥ç®—æ³•" class="headerlink" title="ä¸‰ã€å›¾åµŒå…¥ç®—æ³•"></a>ä¸‰ã€å›¾åµŒå…¥ç®—æ³•</h1><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190805200423616.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>abbr. Graph Embedding Algorithmsï¼Œç›®çš„åœ¨äºå­¦ä¹ å›¾çš„ç»“æ„æˆ–èŠ‚ç‚¹ä¹‹é—´çš„é‚»æ¥å…³ç³»ï¼Œå¯¹èŠ‚ç‚¹è¿›è¡Œç¼–ç  (æˆ–å¯¹å›ºæœ‰ç‰¹å¾è¿›è¡Œé™ç»´)ï¼Œå°†æ‰€æœ‰èŠ‚ç‚¹æ˜ å°„ä¸ºç­‰ç»´åº¦çš„å‘é‡ï¼Œä½¿å…¶èƒ½å¤Ÿæ–¹ä¾¿åœ°åº”ç”¨äºä¸‹æ¸¸çš„èšç±»ã€åˆ†ç±»ã€å…³è”åˆ†ææˆ–å¯è§†åŒ–ä»»åŠ¡ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå›¾åµŒå…¥å±äºé¢„å¤„ç†å·¥ä½œï¼Œç»å¤§å¤šæ•°å›¾åµŒå…¥ç®—æ³•çš†ä¸ºæ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚</p><p>å¸¸è§æ¦‚å¿µ</p><ul><li>å›¾ (Graph)ï¼š$G(V,E)$</li><li><p>èŠ‚ç‚¹ (Node, or Vertex)ï¼š$V={v1,â€¦,vn}$ï¼ŒåŒ…å«å…¨éƒ¨èŠ‚ç‚¹<br>åº¦ (Degree)ï¼šD={deg1,â€¦,degn}ï¼ŒåŒ…å«æ¯ä¸ªèŠ‚ç‚¹çš„å…¥åº¦æ•°é‡<br>è¾¹ (Edge)ï¼š$E=\{e_{ij}\}_{i,j=1}^n$ï¼ŒåŒ…å«æ‰€æœ‰çš„è¾¹ï¼›å¦‚æœè¾¹æ˜¯åŒå‘çš„ï¼Œåˆ™åˆ†åˆ«è¡¨è¾¾ä¸ºä¸¤æ¡ï¼Œe.g. $v_i\leftrightarrow v_j$</p><p>  å…³ç³»å°†å¯¹åº”$e_{ij}$ä¸$e_{ji}$ ï¼›å¦‚æœè¾¹$v_i\rightarrow v_j$ä¸å­˜åœ¨ï¼Œåˆ™ä¸ä¼šå‡ºç°åœ¨ EEE é‡Œé¢<br>é‚»ç‚¹ (Neighbors)ï¼šN(vi)\mathcal{N}(v_i)N(v<br>i</p><p> )ï¼ŒåŒ…å«èŠ‚ç‚¹ viv_iv<br>i</p><p>  çš„æ‰€æœ‰é‚»ç‚¹<br>é‚»æ¥çŸ©é˜µ (Adjacency Matrix)ï¼šA={wijâˆ£wijâ‰¥0}ni,j=1âˆˆRnÃ—nA=\{w_{ij}|w_{ij}\ge 0\}_{i,j=1}^n\in\mathbb{R}^{n\times n}A={w<br>ij</p><p> âˆ£w<br>ij</p><p> â‰¥0}<br>i,j=1<br>n</p><p> âˆˆR<br>nÃ—n<br> ï¼Œè®°å½•å›¾ä¸­è¾¹çš„æƒé‡ä¿¡æ¯ï¼›å¯¹äºæ— å‘å›¾ï¼Œwij=wjiw_{ij}=w_{ji}w<br>ij</p><p> =w<br>ji</p><p> ï¼›è¦æ±‚æ‰€æœ‰çš„è¾¹æƒé‡ä¸å¾—å°äº 0ï¼›å¯¹äºä¸ç›¸é‚»çš„èŠ‚ç‚¹ï¼Œwij=wji=0w_{ij}=w_{ji}=0w<br>ij</p><p> =w<br>ji</p><p> =0<br>ä¸€é˜¶ç›¸ä¼¼åº¦ (First-order Proximity)ï¼šè¾¹çš„æƒé‡ wijw_{ij}w<br>ij</p><p> ï¼Œä»£è¡¨ä¸¤ä¸ªèŠ‚ç‚¹çš„ç›´æ¥ä¾èµ–å…³ç³»<br>äºŒé˜¶ç›¸ä¼¼åº¦ (Second-order Proximity)ï¼šå¯¹äºèŠ‚ç‚¹ viv_iv<br>i</p><p>  å’Œ vjv_jv<br>j</p><p> ï¼Œä»é‚»æ¥çŸ©é˜µåˆ†åˆ«è·å–ç›¸åº”çš„ä¸€é˜¶ç›¸ä¼¼æ€§ wi=[wi1,â€¦,win]\mathrm{w_i}=[w_{i1},â€¦,w_{in}]w<br>i</p><p> =[w<br>i1</p><p> ,â€¦,w<br>in</p><p> ]ï¼Œsj=[wj1,â€¦,wjn]\mathrm{s_j}=[w_{j1},â€¦,w_{jn}]s<br>j</p><p> =[w<br>j1</p><p> ,â€¦,w<br>jn</p><p> ]ï¼›wi\mathrm{w_i}w<br>i</p><p>  ä¸ wj\mathrm{w_j}w<br>j</p><p>  çš„ç›¸ä¼¼åº¦å³ä¸ºäºŒé˜¶ç›¸ä¼¼åº¦ï¼Œä»£è¡¨ä¸¤ä¸ªèŠ‚ç‚¹çš„é‚»å±…ç›¸ä¼¼æ€§</p></li><li>å›¾åµŒå…¥ (Graph Embedding)ï¼šæ˜ å°„å…³ç³»$ f:viâ†’ziâˆˆR^d,âˆ€iâˆˆ[1,n]$, $d$ ä¸ºè¶…å‚æ•°ï¼Œå†³å®šæœ€ç»ˆçš„ç¼–ç é•¿åº¦</li><li>ç‰¹å¾è¡¨ç¤º (Feature Representation)ï¼š$XiâˆˆR^{d0}$    ï¼Œå¯¹èŠ‚ç‚¹ $v_i$ çš„å›ºæœ‰ç‰¹å¾ (ä¸åŒ…å«è¾¹åŠå…¶ä»–èŠ‚ç‚¹ä¿¡æ¯) è¿›è¡Œç®€å•ç¼–ç åå½¢æˆçš„å‘é‡ï¼Œå› æ­¤åˆç§°ä¸ºç‰¹å¾å‘é‡ã€‚</li></ul><h3 id="æ¼”å˜å†ç¨‹"><a href="#æ¼”å˜å†ç¨‹" class="headerlink" title="æ¼”å˜å†ç¨‹"></a>æ¼”å˜å†ç¨‹</h3><p>å›¾åµŒå…¥ç®—æ³•åˆæ­¥è¯ç”Ÿäº 21 ä¸–çºªåˆï¼Œå½¼æ—¶çš„æ¨¡å‹å°†æ›´å¤šçš„ç„¦ç‚¹æ”¾åœ¨é™ç»´ä¸Šï¼ŒåµŒå…¥çš„åŒæ—¶ä½¿å¾—ç›¸é‚»çš„èŠ‚ç‚¹åœ¨æœ€ç»ˆçš„å‘é‡ç©ºé—´ä¸Šæ›´ä¸ºæ¥è¿‘ï¼Œä»£è¡¨æ€§çš„åŒ…æ‹¬ Locally Linear Embedding (2000) å’Œ Laplacian Eigenmaps (2001) ï¼Œè¿™ä¸€ç±»ç®—æ³•å……åˆ†åˆ©ç”¨æ‰€æœ‰çš„æ ·æœ¬é—´æƒé‡ï¼Œæ—¶é—´å¤æ‚åº¦é€šå¸¸è¾ƒé«˜ï¼Œæœ€é«˜å¯è¾¾ O(n2)O(n^2)O(n<br>2<br> )ï¼Œå› æ­¤å¹¶ä¸é€‚åˆå¤§è§„æ¨¡å›¾æ•°æ®ã€‚2010 å¹´ä»¥åï¼Œæ–°è¯ç”Ÿçš„å›¾åµŒå…¥ç®—æ³•é€æ¸åœ¨æ—¶é—´å¤æ‚åº¦ä¸Šå¾—åˆ°ä¼˜åŒ–ï¼Œè½¬è€Œåº”å¯¹ç°å®ç”Ÿæ´»ä¸­å¹¿æ³›å­˜åœ¨çš„ç‰¹å¾ç¨€ç–æ€§é—®é¢˜ï¼Œè¿™å…¶ä¸­çš„ç¿˜æ¥šä¾¿æ˜¯ Graph Factorization (2013)ï¼Œè¯¥ç®—æ³•æ—¨åœ¨äºé‚»æ¥çŸ©é˜µä»¥åŠæ­£åˆ™é¡¹ä¹‹é—´å¯»æ±‚ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œä½¿å¾—ç”Ÿæˆçš„å‘é‡ä¿ç•™é‚»æ¥çŸ©é˜µçš„ç»å¤§å¤šæ•°ä¿¡æ¯ï¼›LINE (2015) å°†è¯¥æ€æƒ³å»¶ç»­ä¸‹å»ï¼Œå¹¶åŠªåŠ›åœ¨åµŒå…¥å‘é‡ä¸­ç»´æŒèŠ‚ç‚¹çš„ä¸€é˜¶å’ŒäºŒé˜¶ç›¸ä¼¼åº¦ï¼›HOPE (2016) æ›´æ˜¯å¼•å…¥äº†æ›´é«˜é˜¶çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼Œé€šè¿‡å¹¿ä¹‰å¥‡å¼‚å€¼åˆ†è§£ä¿ç•™é«˜é˜¶ç›¸ä¼¼æ€§ã€‚ä»¥ä¸Šæ‰€æœ‰ç®—æ³•çš†å¯å½’ç±»äºçŸ©é˜µåˆ†è§£ï¼Œåˆç§°ä¸ºå› å­åˆ†è§£ï¼Œå…¶ä¸­å¿ƒæ€æƒ³åœ¨äºç”Ÿæˆç›¸ä¼¼åº¦çŸ©é˜µï¼Œé€šè¿‡æ•°å­¦æ–¹æ³•å°†çŸ©é˜µä¸­åŒ…å«çš„é‚»æ¥ä¿¡æ¯èå…¥èŠ‚ç‚¹å‘é‡ä¸­ã€‚</p><p>å¦ä¸€ä¸ªçŸ¥åçš„æµæ´¾æ˜¯åŸºäºéšæœºæ¸¸èµ°ï¼Œä»¥ DeepWalk (2014) å’Œ node2vec (2016) ä½œä¸ºä»£è¡¨ã€‚å‰è€…åœ¨èŠ‚ç‚¹çš„ä¸Šä¸‹æ¸¸éšæœºèµ°åŠ¨ï¼Œç”Ÿæˆé•¿åº¦ä¸º 2k+12k+12k+1 çš„ç­‰é•¿åºåˆ—ï¼Œä½œä¸ºèŠ‚ç‚¹çš„é‚»æ¥ç‰¹å¾å¯¼å…¥ skip-gram æ¨¡å‹è®­ç»ƒï¼›åè€…åœ¨å‰è€…çš„åŸºç¡€ä¸Šå¯¹éšæœºæ¸¸èµ°åœ¨ DFS å’Œ BFS çš„æ–¹å‘ä¸Šæ–½åŠ æƒé‡ï¼Œä½¿ç”Ÿæˆçš„åºåˆ—æ›´ä¸ºçœŸå®åœ°ä½“ç°èŠ‚ç‚¹çš„ç»“æ„ä¿¡æ¯ã€‚</p><p>åœ¨è¿™ä¹‹åï¼Œå›¾åµŒå…¥ç®—æ³•é€æ¸è¿‡æ¸¡åˆ°ç¥ç»ç½‘ç»œæ—¶ä»£ï¼Œæ¶Œç°å‡ºä¸€å¤§æ‰¹ä¼˜è´¨çš„å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒ…æ‹¬ SDNE (2016) ä¸ GraphSAGE (2017) ç­‰ç­‰ï¼Œåœ¨å·¥ä¸šç•Œå¤§æ”¾å¼‚å½©ã€‚ä»æ­¤ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„å›¾åµŒå…¥ç®—æ³•ä¸å†ä»…ä»…å±€é™äºèŠ‚ç‚¹çš„é‚»æ¥ä¿¡æ¯ï¼Œè€Œå¼€å§‹å°†èŠ‚ç‚¹æœ¬èº«çš„ç‰¹å¾çº³å…¥æ¨¡å‹è€ƒé‡ï¼Œå¹¶é€æ¸ä»é™æ€çš„ç›´æ¨å¼ (transductive) å­¦ä¹ å‘åŠ¨æ€çš„å½’çº³å¼ (inductive) å­¦ä¹ æ¼”å˜ï¼Œæ— è®ºæ˜¯æ‹Ÿåˆèƒ½åŠ›è¿˜æ˜¯æ³›åŒ–èƒ½åŠ›ï¼Œéƒ½å¤§å¤§æå‡ï¼›éƒ¨åˆ†å›¾ç¥ç»ç½‘ç»œç›´æ¥é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå»ºæ¨¡ï¼Œå·²ä¸å†å±äºå›¾åµŒå…¥çš„èŒƒç•´ã€‚ä¾æ® Wu et. al (2019) çš„å®šä¹‰ï¼Œå›¾ç¥ç»ç½‘ç»œå¯åˆ†ä¸ºäº”å¤§ç±»ï¼š</p><p>å›¾å·ç§¯ç½‘ç»œ (Graph Convolution Networks)ï¼šç®€ç§°ä¸º GCNï¼Œæ˜¯ç›®å‰æœ€ä¸»æµçš„å›¾ç¥ç»ç½‘ç»œç®—æ³•ï¼Œå…¶ä½™å››ç§å›¾ç¥ç»ç½‘ç»œçš†ç”± GCN æ¼”åŒ–è€Œæ¥ã€‚ä¾æ®å»ºæ¨¡è¿‡ç¨‹ä¸­æ˜¯å¦åº”ç”¨åˆ°å‚…é‡Œå¶å˜æ¢ï¼Œå¯å°†å…¶åˆ†ä¸ºåŸºäºè°± (Spectral-based) å’ŒåŸºäºç©ºé—´ (Spatial-based) ä¸¤ä¸ªæµæ´¾ã€‚<br>å›¾æ³¨æ„åŠ›ç½‘ç»œ (Graph Attention Networks)ï¼šå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å›¾ç½‘ç»œæ•´åˆä¸ºç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼›å…·ä½“çš„åšæ³•ï¼Œæ˜¯åœ¨ GCN çš„èšåˆå‡½æ•°ä¸­åŠ å…¥å¯è®­ç»ƒçš„æƒé‡å‚æ•°ï¼Œä½¿å¾—è®­ç»ƒåçš„æ¨¡å‹å°†æ›´å¤šçš„é‡å¿ƒæ”¾åœ¨å…³é”®çš„é‚»ç‚¹ä¸Šã€‚<br>å›¾è‡ªç¼–ç å™¨ (Graph Auto-encoders)ï¼šç”±ä¸€ä¸ªç¼–ç å™¨ (encoder) å’Œä¸€ä¸ªè§£ç å™¨ (decorder) æ„æˆï¼›åœ¨åº”å¯¹æ— å›ºæœ‰ç‰¹å¾çš„å›¾æ—¶ï¼Œç¼–ç å™¨å¯¹é‚»æ¥çŸ©é˜µè¿›è¡Œä¸€å®šçš„é¢„å¤„ç†ï¼ŒåŒ…æ‹¬èå…¥æ›´ä¸°å¯Œçš„é‚»æ¥ä¿¡æ¯ (e.g. é«˜é˜¶ç›¸ä¼¼åº¦) æˆ–æ˜¯å°†é‚»æ¥çŸ©é˜µè¾“å…¥ä¸€å¥—ç¥ç»ç½‘ç»œï¼›åœ¨åº”å¯¹åŒ…å«å›ºæœ‰ç‰¹å¾çš„å›¾æ—¶ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ GCN ä½œä¸ºç¼–ç å™¨å¯¹é‚»æ¥çŸ©é˜µè¿›è¡Œç¼–ç ï¼›è§£ç å™¨å¯¹ç¼–ç ç»“æœè¿›è¡Œåç»­å¤„ç†è·å¾—ä¸€é˜¶åŠäºŒé˜¶ç›¸ä¼¼åº¦ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚<br>å›¾ç”Ÿæˆç½‘ç»œ (Graph Generative Networks)ï¼š2018 å¹´ä»¥åå‡ºç°çš„æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œç›®çš„æ˜¯åœ¨å›¾çš„èŠ‚ç‚¹å’Œè¾¹ç»éªŒåˆ†å¸ƒçš„åŸºç¡€ä¸Šï¼Œç”Ÿæˆæ–°çš„å›¾ï¼Œè¿›è¡Œå¯¹æŠ—å¼è®­ç»ƒã€‚<br>å›¾æ—¶ç©ºç½‘ç»œ (Graph Spatial-Temporal Network)ï¼šæ—¨åœ¨äºæ—¶ç©ºå›¾ä¸­å­¦ä¹ æ¨¡å¼ (pattern)ï¼Œåº”ç”¨åœ¨åˆ†ç±»æˆ–æ˜¯å¯¹æœªæ¥ç‰¹å¾çš„é¢„æµ‹ã€‚<br>GNNPapers è¯¦ç»†åˆ—ç¤ºäº†å›¾ç¥ç»ç½‘ç»œè¯ç”Ÿä»¥æ¥é‡Œç¨‹ç¢‘å¼çš„ä¼˜ç§€æ¨¡å‹ï¼Œä»¥åŠå…¶åœ¨å…·ä½“åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</p><hr><h1 id="å››ã€å›¾å·ç§¯ç½‘ç»œ"><a href="#å››ã€å›¾å·ç§¯ç½‘ç»œ" class="headerlink" title="å››ã€å›¾å·ç§¯ç½‘ç»œ"></a>å››ã€å›¾å·ç§¯ç½‘ç»œ</h1><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190806163559729.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>abbr. Graph Convolution Networksã€‚åœ¨èŠ‚ç‚¹åµŒå…¥è¿™ä¸€ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼ŒåŸºäºç©ºé—´çš„ GCNs ä»å½¼æ—¶å¤§çƒ­çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­æ±²å–æ€æƒ³ï¼Œç›´æ¥åœ¨åŸå›¾çš„æ‹“æ‰‘åºåˆ—ä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼›è€Œè€ƒè™‘åˆ°å›¾ç»“æ„çš„ä¸ç¨³å®šæ€§ï¼ŒåŸºäºè°±çš„ GCNs åˆ™å°†æ‰€æœ‰èŠ‚ç‚¹æ˜ å°„åˆ°å‚…é‡Œå¶åŸŸåè¿›è¡Œå·ç§¯ä¹˜ç§¯ï¼Œå†ç»ç”±å‚…é‡Œå¶é€†å˜æ¢å¾—åˆ°ç©ºé—´åŸŸä¸‹çš„åµŒå…¥å‘é‡ã€‚ä»¥ä¸‹æ˜¯ Wu et. al (2019) å¯¹è¿‘å¹´æ¥å›¾å·ç§¯ç½‘ç»œçš„æ€»ç»“ï¼š</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190811165703970.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><h3 id="GNN-The-Graph-Neural-Network-Model-2009"><a href="#GNN-The-Graph-Neural-Network-Model-2009" class="headerlink" title="GNN: The Graph Neural Network Model (2009)"></a>GNN: The Graph Neural Network Model (2009)</h3><blockquote><p>è®ºæ–‡åœ°å€ï¼š<a href="https://ieeexplore.ieee.org/document/4700287" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/4700287</a></p></blockquote><p>è¿™ç¯‡è®ºæ–‡é¦–æ¬¡æå‡ºå›¾ç¥ç»ç½‘ç»œ (Graph Neural Network) çš„æ¦‚å¿µï¼Œå¹¶å°†æ¨¡å‹è®¾è®¡ä¸ºæœ‰ç›®çš„çš„ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œåˆ†ä¸º è½¬æ¢ (Transition) å’Œ è¾“å‡º (Output) ä¸¤ä¸ªéƒ¨åˆ†ã€‚è½¬æ¢éƒ¨åˆ†ä¸ºæ¯ä¸€ä¸ªèŠ‚ç‚¹æå–é‚»ç‚¹ä¿¡æ¯ï¼Œç”Ÿæˆå‘é‡è¡¨ç¤ºçš„ çŠ¶æ€ (state)ï¼›è¾“å‡ºåˆ™å°†è¯¥çŠ¶æ€æ˜ å°„è‡³ç­‰ç»´çš„å‘é‡è¡¨ç¤ºï¼Œé€šè¿‡ softmax å½’ä¸€åŒ–è¿›è¡Œå¤šåˆ†ç±»é¢„æµ‹ã€‚ç›¸å…³å…¬å¼å¦‚ä¸‹ï¼š</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190811170553982.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>å…¶ä¸­ $f$ å’Œ $g$ çš†ä¸ºå¯è®­ç»ƒçš„å…¨è¿æ¥å±‚ï¼Œ$l_n$ä¸ºèŠ‚ç‚¹çš„æ ‡ç­¾ (å¯ä»¥ç†è§£ä¸ºç‰¹å¾)ï¼Œ$l_{co}$ ä¸ºä¸èŠ‚ç‚¹ç›¸æ¥çš„è¾¹çš„æ ‡ç­¾ (å³æˆ‘ä»¬å¸¸è°ˆçš„æƒé‡)ï¼Œ$x_{ne}$ ä»£è¡¨é‚»ç‚¹è½¬æ¢è¿‡åçš„çŠ¶æ€ (å‘é‡)ï¼Œ$l_{ne}$ä¸ºé‚»ç‚¹çš„æ ‡ç­¾ (ç‰¹å¾)ã€‚ç”±äºè½¬æ¢éƒ¨åˆ†çš„è¾“å…¥ $x_n$åŒ…å«äº†è¯¥éƒ¨åˆ†é‚»ç‚¹çš„è¾“å‡ºï¼Œå› è€Œéœ€è¦é€šè¿‡äº¤äº’æ€§çš„å¤šè½®è¿­ä»£å®ç°è®­ç»ƒå’Œæ¨ç†ï¼Œè®ºæ–‡ä¸­å°†å…¶ç§°å‘¼ä¸º æ‰©æ•£æœºåˆ¶ (Diffusion Mechanism)ï¼š</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190811172629419.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>ç›¸å…³çš„æ•°å­¦å®šç† Banachâ€™s Theorm è¯å®ï¼Œç»è¿‡æ‰©æ•£æœºåˆ¶åï¼Œå¯¹äºæ»¡è¶³$âˆ£âˆ£f w(x,l)âˆ’f w(y,l)âˆ£âˆ£â‰¤Î¼âˆ£âˆ£xâˆ’yâˆ£âˆ£,0â‰¤Î¼&lt;1$ çš„ $w$ï¼Œæœ‰ä»…æœ‰å”¯ä¸€çš„è§£ã€‚å› è€Œåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æå– $f_w(x,l)$ ä½œä¸ºèŠ‚ç‚¹çš„åµŒå…¥è¡¨ç¤ºã€‚</p><p>è¯¥æ¨¡å‹é™¤äº†å¯ç”¨äºèŠ‚ç‚¹çº§åˆ«çš„åˆ†ç±»ï¼ŒåŒæ ·å¯ç”¨äºå›¾çš„çº§åˆ«ï¼Œåªéœ€è¦ä¸ºæ¯å¼ è¾“å…¥çš„å›¾æ·»åŠ ä¸€ä¸ªä»£è¡¨å…¨å±€çš„ç‰¹æ®ŠèŠ‚ç‚¹å³å¯ã€‚ç”±äºæ¨¡å‹åº”ç”¨åˆ°äº†åœ¨èŠ‚ç‚¹çº§åˆ«è¿›è¡Œé‚»ç‚¹é‡‡æ ·ä½œä¸ºè¾“å…¥çš„æ€æƒ³ï¼Œä¸åæ¥çš„å›¾å·ç§¯ç¥ç»ç½‘ç»œä¸è°‹è€Œåˆï¼Œè®ºæ–‡ä½œè€…è™½æ²¡æœ‰è‡ªè¡Œæå‡ºå·ç§¯çš„æ¦‚å¿µï¼Œä½†æœ¬ç¯‡è®ºæ–‡åæ¥è¢«è®¤ä¸ºæ˜¯ç¬¬ä¸€ä¸ªå›¾å·ç§¯ç½‘ç»œçš„æå‡ºè€…ã€‚æ¨¡å‹é‡‡ç”¨é¡ºæ—¶é’ˆçš„æ–¹å¼ä¸ºæ¯ä¸ªèŠ‚ç‚¹æå–å›ºå®šé•¿åº¦çš„é‚»ç‚¹åˆ—è¡¨ï¼Œå¯¹ç›¸å¯¹ä½ç½®ä¸Šç©ºç¼ºçš„é‚»ç‚¹é‡‡å–ç»Ÿä¸€çš„æ— æ„ä¹‰å¡«å……ç­–ç•¥ï¼›è¿™æ ·çš„åšæ³•å°†ç®—æ³•çš„åº”ç”¨åœºæ™¯é™åˆ¶åœ¨äº† 2D ç©ºé—´ï¼Œä¸”éœ€è¦ä½¿ç”¨è€…è¿›è¡Œæ›´ä¸ºç¹ççš„æ•°æ®é¢„å¤„ç†ï¼Œå› è€Œæˆä¸ºé¥±å—è¯Ÿç—…çš„ä¹‹å¤„ï¼Œä¹Ÿä¸ºåç»­ä¼˜åŒ–æŒ‡å¼•äº†æ–¹å‘ã€‚</p><h3 id="GraphSAGE-Inductive-Representation-Learning-on-Large-Graphs-2017"><a href="#GraphSAGE-Inductive-Representation-Learning-on-Large-Graphs-2017" class="headerlink" title="GraphSAGE: Inductive Representation Learning on Large Graphs (2017)"></a>GraphSAGE: Inductive Representation Learning on Large Graphs (2017)</h3><blockquote><p>è®ºæ–‡é“¾æ¥ï¼š<a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02216</a></p></blockquote><p>åˆä»£ GNN ä¸­é‚»ç‚¹é‡‡æ ·çš„æ€æƒ³ä¸€ç›´ä¿ç•™äº†ä¸‹æ¥ï¼Œä½† GraphSAGE å¹¶ä¸å°†é‡‡æ ·ä¿¡æ¯å±€é™åœ¨èŠ‚ç‚¹çš„æ‹“æ‰‘ç»“æ„é‡Œï¼Œè€Œæ˜¯ä½¿ç”¨èŠ‚ç‚¹çš„å›ºæœ‰ç‰¹å¾å–è€Œä»£ä¹‹ï¼Œå¯¹é€ æˆåºå¤§å‚æ•°é‡çš„æ‰©æ•£æœºåˆ¶ä¹Ÿé€‰æ‹©äº†æ‘’å¼ƒå¤„ç†ã€‚æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„ä¸åŒï¼ŒGraphSAGE é‡‡ç”¨ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼šåº”ç”¨äºå›¾åµŒå…¥æ—¶ï¼Œä½¿ç”¨è´Ÿé‡‡æ ·æŠ€æœ¯è®¡ç®—äºŒé˜¶ç›¸ä¼¼åº¦å®ç°å‚æ•°æ”¶æ•›ï¼›åº”ç”¨äºåˆ†ç±»ä»»åŠ¡æ—¶ï¼Œä½¿ç”¨ softmax è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ã€‚å…¶ä¸­ï¼Œæ— ç›‘ç£å­¦ä¹ çš„è¡¨è¾¾å¼å¦‚ä¸‹ï¼š</p><p> $\mathcal{J}_{\mathcal{G}}(z_u)=-\log \big(\sigma(z_u^Tz_v)\big)-Q\cdot \mathbb{E}_{v_n\sim P_n(v)}\log\big(\sigma(-z_u^Tz_{v_n})\big)$</p><p>æ¨¡å‹çš„ç‰¹åˆ«ä¹‹å¤„ï¼Œåœ¨äºæ¯ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œéƒ½é‡æ–°å¯¹é‚»ç‚¹è¿›è¡Œé‡‡æ ·ï¼Œå‰é¦ˆå…¬å¼å¦‚ä¸‹ï¼š</p><p>$h_v^k=\sigma\big(W^k\cdot \text{Concat}(h_v^{k-1},h_{\mathcal{N}(v)}^k)\big) $</p><p>$h_{\mathcal{N}(v)}^k=\text{Aggregate}(\{h_u^{k-1},\forall u\in\mathcal{N}(v) \})$</p><p>å…¶ä¸­$k=1,â€¦,K$ ä¸ºè¿­ä»£æ¬¡æ•°ï¼›$h_v^kh$ ä¸ºå‰é¦ˆå±‚è¾“å‡ºçš„éšè—å‘é‡ï¼Œæ— ç›‘ç£å­¦ä¹ ä½¿ç”¨æœ€åä¸€å±‚è®¡ç®—æŸå¤±å‡½æ•°ã€‚</p><p><img src="%E4%BB%8E%E5%9B%BE%E5%B5%8C%E5%85%A5%E7%AE%97%E6%B3%95%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/20190812004554131.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ -é€’å½’ç¥ç»ç½‘ç»œ</title>
      <link href="/ling-ji-chu-ru-men-shen-du-xue-xi-7-di-gui-shen-jing-wang-luo/"/>
      <url>/ling-ji-chu-ru-men-shen-du-xue-xi-7-di-gui-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒé“¾æ¥ï¼š<a href="https://www.zybuluo.com/hanbingtao/note/626300" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/626300</a></p><hr><h2 id="å¾€æœŸå›é¡¾"><a href="#å¾€æœŸå›é¡¾" class="headerlink" title="å¾€æœŸå›é¡¾"></a>å¾€æœŸå›é¡¾</h2><p>åœ¨å‰é¢çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ï¼Œå®ƒå¯ä»¥ç”¨æ¥å¤„ç†åŒ…å«åºåˆ—ç»“æ„çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œä¿¡æ¯å¾€å¾€è¿˜å­˜åœ¨ç€è¯¸å¦‚æ ‘ç»“æ„ã€å›¾ç»“æ„ç­‰æ›´å¤æ‚çš„ç»“æ„ã€‚å¯¹äºè¿™ç§å¤æ‚çš„ç»“æ„ï¼Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å°±æ— èƒ½ä¸ºåŠ›äº†ã€‚æœ¬æ–‡ä»‹ç»ä¸€ç§æ›´ä¸ºå¼ºå¤§ã€å¤æ‚çš„ç¥ç»ç½‘ç»œï¼š<strong>é€’å½’ç¥ç»ç½‘ç»œ (Recursive Neural Network, RNN)</strong>ï¼Œä»¥åŠå®ƒçš„è®­ç»ƒç®—æ³•<strong>BPTS (Back Propagation Through Structure)</strong>ã€‚é¡¾åæ€ä¹‰ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>ï¼ˆå·§åˆçš„æ˜¯ï¼Œå®ƒçš„ç¼©å†™å’Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ä¸€æ ·ï¼Œä¹Ÿæ˜¯RNNï¼‰å¯ä»¥å¤„ç†è¯¸å¦‚æ ‘ã€å›¾è¿™æ ·çš„<strong>é€’å½’ç»“æ„</strong>ã€‚åœ¨æ–‡ç« çš„æœ€åï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ª<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>ï¼Œå¹¶ä»‹ç»å®ƒçš„å‡ ä¸ªåº”ç”¨åœºæ™¯ã€‚</p><h2 id="é€’å½’ç¥ç»ç½‘ç»œæ˜¯å•¥"><a href="#é€’å½’ç¥ç»ç½‘ç»œæ˜¯å•¥" class="headerlink" title="é€’å½’ç¥ç»ç½‘ç»œæ˜¯å•¥"></a>é€’å½’ç¥ç»ç½‘ç»œæ˜¯å•¥</h2><p>å› ä¸ºç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚å•å…ƒä¸ªæ•°æ˜¯å›ºå®šçš„ï¼Œå› æ­¤å¿…é¡»ç”¨å¾ªç¯æˆ–è€…é€’å½’çš„æ–¹å¼æ¥å¤„ç†é•¿åº¦å¯å˜çš„è¾“å…¥ã€‚<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å®ç°äº†å‰è€…ï¼Œé€šè¿‡å°†é•¿åº¦ä¸å®šçš„è¾“å…¥åˆ†å‰²ä¸ºç­‰é•¿åº¦çš„å°å—ï¼Œç„¶åå†ä¾æ¬¡çš„è¾“å…¥åˆ°ç½‘ç»œä¸­ï¼Œä»è€Œå®ç°äº†ç¥ç»ç½‘ç»œå¯¹å˜é•¿è¾“å…¥çš„å¤„ç†ã€‚ä¸€ä¸ªå…¸å‹çš„ä¾‹å­æ˜¯ï¼Œå½“æˆ‘ä»¬å¤„ç†ä¸€å¥è¯çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä¸€å¥è¯çœ‹ä½œæ˜¯è¯ç»„æˆçš„åºåˆ—ï¼Œç„¶åï¼Œæ¯æ¬¡å‘<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>è¾“å…¥ä¸€ä¸ªè¯ï¼Œå¦‚æ­¤å¾ªç¯ç›´è‡³æ•´å¥è¯è¾“å…¥å®Œæ¯•ï¼Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å°†äº§ç”Ÿå¯¹åº”çš„è¾“å‡ºã€‚å¦‚æ­¤ï¼Œæˆ‘ä»¬å°±èƒ½å¤„ç†ä»»æ„é•¿åº¦çš„å¥å­äº†ã€‚å…¥ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-a87286a58fc03563.png)</p><p>ç„¶è€Œï¼Œæœ‰æ—¶å€™æŠŠå¥å­çœ‹åšæ˜¯è¯çš„åºåˆ—æ˜¯ä¸å¤Ÿçš„ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¥è¯ã€ä¸¤ä¸ªå¤–è¯­å­¦é™¢çš„å­¦ç”Ÿã€ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-1d69d9ea35f55af0.png)</p><p>ä¸Šå›¾æ˜¾ç¤ºäº†è¿™å¥è¯çš„ä¸¤ä¸ªä¸åŒçš„è¯­æ³•è§£ææ ‘ã€‚å¯ä»¥çœ‹å‡ºæ¥è¿™å¥è¯æœ‰æ­§ä¹‰ï¼Œä¸åŒçš„è¯­æ³•è§£ææ ‘åˆ™å¯¹åº”äº†ä¸åŒçš„æ„æ€ã€‚ä¸€ä¸ªæ˜¯ã€ä¸¤ä¸ªå¤–è¯­å­¦é™¢çš„/å­¦ç”Ÿã€ï¼Œä¹Ÿå°±æ˜¯å­¦ç”Ÿå¯èƒ½æœ‰è®¸å¤šï¼Œä½†ä»–ä»¬æ¥è‡ªäºä¸¤æ‰€å¤–è¯­å­¦æ ¡ï¼›å¦ä¸€ä¸ªæ˜¯ã€ä¸¤ä¸ª/å¤–è¯­å­¦é™¢çš„å­¦ç”Ÿã€ï¼Œä¹Ÿå°±æ˜¯åªæœ‰ä¸¤ä¸ªå­¦ç”Ÿï¼Œä»–ä»¬æ˜¯å¤–è¯­å­¦é™¢çš„ã€‚ä¸ºäº†èƒ½å¤Ÿè®©æ¨¡å‹åŒºåˆ†å‡ºä¸¤ä¸ªä¸åŒçš„æ„æ€ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¿…é¡»èƒ½å¤ŸæŒ‰ç…§æ ‘ç»“æ„å»å¤„ç†ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åºåˆ—ï¼Œè¿™å°±æ˜¯<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„ä½œç”¨ã€‚å½“é¢å¯¹æŒ‰ç…§æ ‘/å›¾ç»“æ„å¤„ç†ä¿¡æ¯æ›´æœ‰æ•ˆçš„ä»»åŠ¡æ—¶ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>é€šå¸¸éƒ½ä¼šè·å¾—ä¸é”™çš„ç»“æœã€‚</p><p><strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å¯ä»¥æŠŠä¸€ä¸ªæ ‘/å›¾ç»“æ„ä¿¡æ¯ç¼–ç ä¸ºä¸€ä¸ªå‘é‡ï¼Œä¹Ÿå°±æ˜¯æŠŠä¿¡æ¯æ˜ å°„åˆ°ä¸€ä¸ªè¯­ä¹‰å‘é‡ç©ºé—´ä¸­ã€‚è¿™ä¸ªè¯­ä¹‰å‘é‡ç©ºé—´æ»¡è¶³æŸç±»æ€§è´¨ï¼Œæ¯”å¦‚è¯­ä¹‰ç›¸ä¼¼çš„å‘é‡è·ç¦»æ›´è¿‘ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä¸¤å¥è¯ï¼ˆå°½ç®¡å†…å®¹ä¸åŒï¼‰å®ƒçš„æ„æ€æ˜¯ç›¸ä¼¼çš„ï¼Œé‚£ä¹ˆæŠŠå®ƒä»¬åˆ†åˆ«ç¼–ç åçš„ä¸¤ä¸ªå‘é‡çš„è·ç¦»ä¹Ÿç›¸è¿‘ï¼›åä¹‹ï¼Œå¦‚æœä¸¤å¥è¯çš„æ„æ€æˆªç„¶ä¸åŒï¼Œé‚£ä¹ˆç¼–ç åå‘é‡çš„è·ç¦»åˆ™å¾ˆè¿œã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-712ec37a73b854c1.png)</p><p>ä»ä¸Šå›¾æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å°†æ‰€æœ‰çš„è¯ã€å¥éƒ½æ˜ å°„åˆ°ä¸€ä¸ª2ç»´å‘é‡ç©ºé—´ä¸­ã€‚å¥å­ã€the country of my birthã€å’Œå¥å­ã€the place where I was bornã€çš„æ„æ€æ˜¯éå¸¸æ¥è¿‘çš„ï¼Œæ‰€ä»¥è¡¨ç¤ºå®ƒä»¬çš„ä¸¤ä¸ªå‘é‡åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»å¾ˆè¿‘ã€‚å¦å¤–ä¸¤ä¸ªè¯ã€Germanyã€å’Œã€Franceã€å› ä¸ºè¡¨ç¤ºçš„éƒ½æ˜¯åœ°ç‚¹ï¼Œå®ƒä»¬çš„å‘é‡ä¸ä¸Šé¢ä¸¤å¥è¯çš„å‘é‡çš„è·ç¦»ï¼Œå°±æ¯”å¦å¤–ä¸¤ä¸ªè¡¨ç¤ºæ—¶é—´çš„è¯ã€Mondayã€å’Œã€Tuesdayã€çš„å‘é‡çš„è·ç¦»è¿‘å¾—å¤šã€‚è¿™æ ·ï¼Œé€šè¿‡å‘é‡çš„è·ç¦»ï¼Œå°±å¾—åˆ°äº†ä¸€ç§è¯­ä¹‰çš„è¡¨ç¤ºã€‚</p><p>ä¸Šå›¾è¿˜æ˜¾ç¤ºäº†è‡ªç„¶è¯­è¨€<strong>å¯ç»„åˆ</strong>çš„æ€§è´¨ï¼šè¯å¯ä»¥ç»„æˆå¥ã€å¥å¯ä»¥ç»„æˆæ®µè½ã€æ®µè½å¯ä»¥ç»„æˆç¯‡ç« ï¼Œè€Œæ›´é«˜å±‚çš„è¯­ä¹‰å–å†³äºåº•å±‚çš„è¯­ä¹‰ä»¥åŠå®ƒä»¬çš„ç»„åˆæ–¹å¼ã€‚<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>æ˜¯ä¸€ç§è¡¨ç¤ºå­¦ä¹ ï¼Œå®ƒå¯ä»¥å°†è¯ã€å¥ã€æ®µã€ç¯‡æŒ‰ç…§ä»–ä»¬çš„è¯­ä¹‰æ˜ å°„åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ä¸­ï¼Œä¹Ÿå°±æ˜¯æŠŠå¯ç»„åˆï¼ˆæ ‘/å›¾ç»“æ„ï¼‰çš„ä¿¡æ¯è¡¨ç¤ºä¸ºä¸€ä¸ªä¸ªæœ‰æ„ä¹‰çš„å‘é‡ã€‚æ¯”å¦‚ä¸Šé¢è¿™ä¸ªä¾‹å­ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>æŠŠå¥å­â€the country of my birthâ€è¡¨ç¤ºä¸ºäºŒç»´å‘é‡[1,5]ã€‚æœ‰äº†è¿™ä¸ªã€ç¼–ç å™¨ã€ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ä»¥è¿™äº›æœ‰æ„ä¹‰çš„å‘é‡ä¸ºåŸºç¡€å»å®Œæˆæ›´é«˜çº§çš„ä»»åŠ¡ï¼ˆæ¯”å¦‚æƒ…æ„Ÿåˆ†æç­‰ï¼‰ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>åœ¨åšæƒ…æ„Ÿåˆ†ææ—¶ï¼Œå¯ä»¥æ¯”è¾ƒå¥½çš„å¤„ç†å¦å®šå¥ï¼Œè¿™æ˜¯èƒœè¿‡å…¶ä»–ä¸€äº›æ¨¡å‹çš„ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-1f91307ac606ba00.png)</p><p>åœ¨ä¸Šå›¾ä¸­ï¼Œè“è‰²è¡¨ç¤ºæ­£é¢è¯„ä»·ï¼Œçº¢è‰²è¡¨ç¤ºè´Ÿé¢è¯„ä»·ã€‚æ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¿™ä¸ªå‘é‡è¡¨è¾¾äº†ä»¥å®ƒä¸ºæ ¹çš„å­æ ‘çš„æƒ…æ„Ÿè¯„ä»·ã€‚æ¯”å¦‚â€intelligent humorâ€æ˜¯æ­£é¢è¯„ä»·ï¼Œè€Œâ€care about cleverness wit or any other kind of intelligent humorâ€æ˜¯ä¸­æ€§è¯„ä»·ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®çš„å¤„ç†doesnâ€™tçš„å«ä¹‰ï¼Œå°†æ­£é¢è¯„ä»·è½¬å˜ä¸ºè´Ÿé¢è¯„ä»·ã€‚</p><p>å°½ç®¡<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å…·æœ‰æ›´ä¸ºå¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†æ˜¯åœ¨å®é™…åº”ç”¨ä¸­å¹¶ä¸å¤ªæµè¡Œã€‚å…¶ä¸­ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„è¾“å…¥æ˜¯æ ‘/å›¾ç»“æ„ï¼Œè€Œè¿™ç§ç»“æ„éœ€è¦èŠ±è´¹å¾ˆå¤šäººå·¥å»æ ‡æ³¨ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ç”¨<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å¤„ç†å¥å­ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç›´æ¥æŠŠå¥å­ä½œä¸ºè¾“å…¥ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬ç”¨<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å¤„ç†å¥å­ï¼Œæˆ‘ä»¬å°±å¿…é¡»æŠŠæ¯ä¸ªå¥å­æ ‡æ³¨ä¸ºè¯­æ³•è§£ææ ‘çš„å½¢å¼ï¼Œè¿™æ— ç–‘è¦èŠ±è´¹éå¸¸å¤§çš„ç²¾åŠ›ã€‚å¾ˆå¤šæ—¶å€™ï¼Œç›¸å¯¹äº<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>èƒ½å¤Ÿå¸¦æ¥çš„æ€§èƒ½æå‡ï¼Œè¿™ä¸ªæŠ•å…¥æ˜¯ä¸å¤ªåˆ’ç®—çš„ã€‚</p><p>æˆ‘ä»¬å·²ç»åŸºæœ¬äº†è§£äº†<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>æ˜¯åšä»€ä¹ˆç”¨çš„ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢è®¨å®ƒçš„ç®—æ³•ç»†èŠ‚ã€‚</p><h2 id="é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘è®¡ç®—"><a href="#é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘è®¡ç®—" class="headerlink" title="é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘è®¡ç®—"></a>é€’å½’ç¥ç»ç½‘ç»œçš„å‰å‘è®¡ç®—</h2><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»ä¸€ä¸‹<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>æ˜¯å¦‚ä½•å¤„ç†æ ‘/å›¾ç»“æ„çš„ä¿¡æ¯çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥å¤„ç†æ ‘å‹ä¿¡æ¯ä¸ºä¾‹è¿›è¡Œä»‹ç»ã€‚</p><p><strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„è¾“å…¥æ˜¯ä¸¤ä¸ªå­èŠ‚ç‚¹ï¼ˆä¹Ÿå¯ä»¥æ˜¯å¤šä¸ªï¼‰ï¼Œè¾“å‡ºå°±æ˜¯å°†è¿™ä¸¤ä¸ªå­èŠ‚ç‚¹ç¼–ç åäº§ç”Ÿçš„çˆ¶èŠ‚ç‚¹ï¼Œçˆ¶èŠ‚ç‚¹çš„ç»´åº¦å’Œæ¯ä¸ªå­èŠ‚ç‚¹æ˜¯ç›¸åŒçš„ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-f2ea8885320110a5.png)</p><p>$\mathbf{c}_1$å’Œ$\mathbf{c}_2$åˆ†åˆ«æ˜¯è¡¨ç¤ºä¸¤ä¸ªå­èŠ‚ç‚¹çš„å‘é‡ï¼Œ$\mathbf{p}$æ˜¯è¡¨ç¤ºçˆ¶èŠ‚ç‚¹çš„å‘é‡ã€‚å­èŠ‚ç‚¹å’Œçˆ¶èŠ‚ç‚¹ç»„æˆä¸€ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œä¹Ÿå°±æ˜¯å­èŠ‚ç‚¹çš„æ¯ä¸ªç¥ç»å…ƒéƒ½å’Œçˆ¶èŠ‚ç‚¹çš„æ¯ä¸ªç¥ç»å…ƒä¸¤ä¸¤ç›¸è¿ã€‚æˆ‘ä»¬ç”¨çŸ©é˜µ$W$è¡¨ç¤ºè¿™äº›è¿æ¥ä¸Šçš„æƒé‡ï¼Œå®ƒçš„ç»´åº¦å°†æ˜¯$d\times 2d$ï¼Œå…¶ä¸­ï¼Œ$d$è¡¨ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„ç»´åº¦ã€‚çˆ¶èŠ‚ç‚¹çš„è®¡ç®—å…¬å¼å¯ä»¥å†™æˆï¼š</p><p>$\begin{align} \mathbf{p} = tanh(W\begin{bmatrix}\mathbf{c}_1\\\mathbf{c}_2\end{bmatrix}+\mathbf{b})\qquad(å¼1)\end {align} $</p><p>åœ¨ä¸Šå¼ä¸­ï¼Œtanhæ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ç”¨å…¶å®ƒçš„æ¿€æ´»å‡½æ•°ï¼‰ï¼Œ$\mathbf{b}$æ˜¯åç½®é¡¹ï¼Œå®ƒä¹Ÿæ˜¯ä¸€ä¸ªç»´åº¦ä¸º$d$çš„å‘é‡ã€‚å¦‚æœè¯»è¿‡å‰é¢çš„æ–‡ç« ï¼Œç›¸ä¿¡å¤§å®¶å·²ç»éå¸¸ç†Ÿæ‚‰è¿™äº›è®¡ç®—äº†ï¼Œåœ¨æ­¤ä¸åšè¿‡å¤šçš„è§£é‡Šäº†ã€‚</p><p>ç„¶åï¼Œæˆ‘ä»¬æŠŠäº§ç”Ÿçš„çˆ¶èŠ‚ç‚¹çš„å‘é‡å’Œå…¶ä»–å­èŠ‚ç‚¹çš„å‘é‡å†æ¬¡ä½œä¸ºç½‘ç»œçš„è¾“å…¥ï¼Œå†æ¬¡äº§ç”Ÿå®ƒä»¬çš„çˆ¶èŠ‚ç‚¹ã€‚å¦‚æ­¤é€’å½’ä¸‹å»ï¼Œç›´è‡³æ•´æ£µæ ‘å¤„ç†å®Œæ¯•ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ ¹èŠ‚ç‚¹çš„å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºå®ƒæ˜¯å¯¹æ•´æ£µæ ‘çš„è¡¨ç¤ºï¼Œè¿™æ ·æˆ‘ä»¬å°±å®ç°äº†æŠŠæ ‘æ˜ å°„ä¸ºä¸€ä¸ªå‘é‡ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å¤„ç†ä¸€æ£µæ ‘ï¼Œæœ€ç»ˆå¾—åˆ°çš„å‘é‡$\mathbf{p}_3$ï¼Œå°±æ˜¯å¯¹æ•´æ£µæ ‘çš„è¡¨ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-2e300754026038f5.png)</p><p>ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>é€’å½’ç¥å°†ç½‘ç»œ</strong>å°†ã€ä¸¤ä¸ªå¤–è¯­å­¦æ ¡çš„å­¦ç”Ÿã€æ˜ å°„ä¸ºä¸€ä¸ªå‘é‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-577b3ccc7c34eb34.png)</p><p>æœ€åå¾—åˆ°çš„å‘é‡$\mathbf{p}_3$å°±æ˜¯å¯¹æ•´ä¸ªå¥å­ã€ä¸¤ä¸ªå¤–è¯­å­¦æ ¡çš„å­¦ç”Ÿã€çš„è¡¨ç¤ºã€‚ç”±äºæ•´ä¸ªç»“æ„æ˜¯é€’å½’çš„ï¼Œä¸ä»…ä»…æ˜¯æ ¹èŠ‚ç‚¹ï¼Œäº‹å®ä¸Šæ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯ä»¥å…¶ä¸ºæ ¹çš„å­æ ‘çš„è¡¨ç¤ºã€‚æ¯”å¦‚ï¼Œåœ¨å·¦è¾¹çš„è¿™æ£µæ ‘ä¸­ï¼Œå‘é‡$\mathbf{p}_2$æ˜¯çŸ­è¯­ã€å¤–è¯­å­¦é™¢çš„å­¦ç”Ÿã€çš„è¡¨ç¤ºï¼Œè€Œå‘é‡$\mathbf{p}_1$æ˜¯çŸ­è¯­ã€å¤–è¯­å­¦é™¢çš„ã€çš„è¡¨ç¤ºã€‚</p><p><strong>å¼1</strong>å°±æ˜¯<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„å‰å‘è®¡ç®—ç®—æ³•ã€‚å®ƒå’Œå…¨è¿æ¥ç¥ç»ç½‘ç»œçš„è®¡ç®—æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œåªæ˜¯åœ¨è¾“å…¥çš„è¿‡ç¨‹ä¸­éœ€è¦æ ¹æ®è¾“å…¥çš„æ ‘ç»“æ„ä¾æ¬¡è¾“å…¥æ¯ä¸ªå­èŠ‚ç‚¹ã€‚</p><p>éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„æƒé‡$W$å’Œåç½®é¡¹$\mathbf{b}$åœ¨æ‰€æœ‰çš„èŠ‚ç‚¹éƒ½æ˜¯<strong>å…±äº«</strong>çš„ã€‚</p><h2 id="é€’å½’ç¥ç»ç½‘ç»œçš„è®­ç»ƒ"><a href="#é€’å½’ç¥ç»ç½‘ç»œçš„è®­ç»ƒ" class="headerlink" title="é€’å½’ç¥ç»ç½‘ç»œçš„è®­ç»ƒ"></a>é€’å½’ç¥ç»ç½‘ç»œçš„è®­ç»ƒ</h2><p><strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„è®­ç»ƒç®—æ³•å’Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ç±»ä¼¼ï¼Œä¸¤è€…ä¸åŒä¹‹å¤„åœ¨äºï¼Œå‰è€…éœ€è¦å°†æ®‹å·®$\delta$ä»æ ¹èŠ‚ç‚¹åå‘ä¼ æ’­åˆ°å„ä¸ªå­èŠ‚ç‚¹ï¼Œè€Œåè€…æ˜¯å°†æ®‹å·®$\delta$ä»å½“å‰æ—¶åˆ»$t_k$åå‘ä¼ æ’­åˆ°åˆå§‹æ—¶åˆ»$t_{1}$ã€‚</p><p>ä¸‹é¢ï¼Œæˆ‘ä»¬ä»‹ç»é€‚ç”¨äº<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„è®­ç»ƒç®—æ³•ï¼Œä¹Ÿå°±æ˜¯<strong>BPTS</strong>ç®—æ³•ã€‚</p><h3 id="è¯¯å·®é¡¹çš„ä¼ é€’"><a href="#è¯¯å·®é¡¹çš„ä¼ é€’" class="headerlink" title="è¯¯å·®é¡¹çš„ä¼ é€’"></a>è¯¯å·®é¡¹çš„ä¼ é€’</h3><p>é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæ¨å¯¼å°†è¯¯å·®ä»çˆ¶èŠ‚ç‚¹ä¼ é€’åˆ°å­èŠ‚ç‚¹çš„å…¬å¼ï¼Œå¦‚ä¸‹å›¾ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-9ab001431eb2f2a4.png)</p><p>å®šä¹‰$\delta_p$ä¸ºè¯¯å·®å‡½æ•°Eç›¸å¯¹äºçˆ¶èŠ‚ç‚¹çš„åŠ æƒè¾“å…¥$\mathbf{net}_p$çš„å¯¼æ•°ï¼Œå³ï¼š</p><p>$\begin {align} \delta_p\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{net}_p}} \end{align} $</p><p>è®¾$\mathbf{net}_p$æ˜¯çˆ¶èŠ‚ç‚¹çš„<strong>åŠ æƒè¾“å…¥</strong>ï¼Œåˆ™</p><p>$\begin{align} \mathbf{net}_p=W\begin{bmatrix}\mathbf{c}_1\\\mathbf{c}_2\end{bmatrix}+\mathbf{b} \end{align} $</p><p>åœ¨ä¸Šè¿°å¼å­é‡Œï¼Œ$\mathbf{net}_p$ã€$\mathbf{c}_1$ã€$\mathbf{c}_2$éƒ½æ˜¯å‘é‡ï¼Œè€Œ$\mathbf W$æ˜¯çŸ©é˜µã€‚ä¸ºäº†çœ‹æ¸…æ¥šå®ƒä»¬çš„å…³ç³»ï¼Œæˆ‘ä»¬å°†å…¶å±•å¼€ï¼š</p><p>$\begin{align} \begin{bmatrix} net_{p_1}\\ net_{p_2}\\ â€¦\\ net_{p_n} \end{bmatrix}&amp;= \begin{bmatrix} w_{p_1c_{11}}&amp;w_{p_1c_{12}}&amp;â€¦&amp;w_{p_1c_{1n}}&amp;w_{p_1c_{21}}&amp;w_{p_1c_{22}}&amp;â€¦&amp;w_{p_1c_{2n}}\\ w_{p_2c_{11}}&amp;w_{p_2c_{12}}&amp;â€¦&amp;w_{p_2c_{1n}}&amp;w_{p_2c_{21}}&amp;w_{p_2c_{22}}&amp;â€¦&amp;w_{p_2c_{2n}}\\ â€¦\\ w_{p_nc_{11}}&amp;w_{p_nc_{12}}&amp;â€¦&amp;w_{p_nc_{1n}}&amp;w_{p_nc_{21}}&amp;w_{p_nc_{22}}&amp;â€¦&amp;w_{p_nc_{2n}}\\ \end{bmatrix} \begin{bmatrix} c_{11}\\ c_{12}\\ â€¦\\ c_{1n}\\ c_{21}\\ c_{22}\\ â€¦\\ c_{2n} \end{bmatrix}+\begin{bmatrix}\\ b_1\\ b_2\\ â€¦\\ b_n\\ \end{bmatrix} \end{align} $</p><p>åœ¨ä¸Šé¢çš„å…¬å¼ä¸­ï¼Œ$p_i$è¡¨ç¤ºçˆ¶èŠ‚ç‚¹pçš„ç¬¬iä¸ªåˆ†é‡ï¼›$c_{1i}$è¡¨ç¤º$c_{1}$å­èŠ‚ç‚¹çš„ç¬¬iä¸ªåˆ†é‡ï¼›$c_{2i}$è¡¨ç¤º$c_2$å­èŠ‚ç‚¹çš„ç¬¬iä¸ªåˆ†é‡ï¼›$w_{p_ic_{jk}}$è¡¨ç¤ºå­èŠ‚ç‚¹$c_j$çš„ç¬¬kä¸ªåˆ†é‡åˆ°çˆ¶èŠ‚ç‚¹pçš„ç¬¬iä¸ªåˆ†é‡çš„çš„æƒé‡ã€‚æ ¹æ®ä¸Šé¢å±•å¼€åçš„çŸ©é˜µä¹˜æ³•å½¢å¼ï¼Œæˆ‘ä»¬ä¸éš¾çœ‹å‡ºï¼Œå¯¹äºå­èŠ‚ç‚¹$c_{jk}$æ¥è¯´ï¼Œå®ƒä¼šå½±å“çˆ¶èŠ‚ç‚¹æ‰€æœ‰çš„åˆ†é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ±‚è¯¯å·®å‡½æ•°Eå¯¹$c_{jk}$çš„å¯¼æ•°æ—¶ï¼Œå¿…é¡»ç”¨åˆ°å…¨å¯¼æ•°å…¬å¼ï¼Œä¹Ÿå°±æ˜¯ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{c_{jk}}}&amp;=\sum_i{\frac{\partial{E}}{\partial{net_{p_i}}}}\frac{\partial{net_{p_i}}}{\partial{c_{jk}}}\\ &amp;=\sum_i{\delta_{p_i}}w_{p_ic_{jk}} \end{align} $</p><p>æœ‰äº†ä¸Šå¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠå®ƒè¡¨ç¤ºä¸ºçŸ©é˜µå½¢å¼ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå‘é‡åŒ–è¡¨è¾¾ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{\mathbf{c}_j}}&amp;=U_j\delta_p \end{align} $</p><p>å…¶ä¸­ï¼ŒçŸ©é˜µ$U_j$æ˜¯ä»çŸ©é˜µWä¸­æå–éƒ¨åˆ†å…ƒç´ ç»„æˆçš„çŸ©é˜µã€‚å…¶å•å…ƒä¸ºï¼š</p><p>$\begin {align} u_{j_{ik}}=w_{p_kc_{ji}} \end{align} $</p><p>ä¸Šå¼çœ‹ä¸Šå»å¯èƒ½ä¼šè®©äººæ™•èœï¼Œä»ä¸‹å›¾ï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚çš„çœ‹åˆ°$U_j$åˆ°åº•æ˜¯å•¥ã€‚é¦–å…ˆæˆ‘ä»¬æŠŠWçŸ©é˜µæ‹†åˆ†ä¸ºä¸¤ä¸ªçŸ©é˜µ$W_1$å’Œ$W_2$ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-fb58d74fbc7e9ce5.png)</p><p>æ˜¾ç„¶ï¼Œå­çŸ©é˜µ$W_1$å’Œ$W_2$åˆ†åˆ«å¯¹åº”å­èŠ‚ç‚¹$\mathbf{c}_1$å’Œ$\mathbf{c}_2$çš„åˆ°çˆ¶èŠ‚ç‚¹$\mathbf{p}$æƒé‡ã€‚åˆ™çŸ©é˜µ$U_j$ä¸ºï¼š</p><p>$\begin {align} U_j=W_j^T \end{align} $</p><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå°†è¯¯å·®é¡¹åå‘ä¼ é€’åˆ°ç›¸åº”å­èŠ‚ç‚¹$\mathbf{c}_j$çš„çŸ©é˜µ$U_j$å°±æ˜¯å…¶å¯¹åº”æƒé‡çŸ©é˜µ$W_j$çš„è½¬ç½®ã€‚</p><p>ç°åœ¨ï¼Œæˆ‘ä»¬è®¾$\mathbf{net}_{c_j}$æ˜¯å­èŠ‚ç‚¹$\mathbf{c}_j$çš„åŠ æƒè¾“å…¥ï¼Œ$f$æ˜¯å­èŠ‚ç‚¹cçš„æ¿€æ´»å‡½æ•°ï¼Œåˆ™ï¼š</p><p>$\begin{align} \mathbf{c}_j=f(\mathbf{net}_{c_j})\\ \end{align} $</p><p>è¿™æ ·ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p><p>$\begin{align} \delta_{c_j}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{c_j}}}\\ &amp;=\frac{\partial{E}}{\partial{\mathbf{c}_j}}\frac{\partial{\mathbf{c}_j}}{\partial{\mathbf{net}_{c_j}}}\\ &amp;=W_j^T\delta_p\circ fâ€™(\mathbf{net}_{c_j}) \end{align} $</p><p>å¦‚æœæˆ‘ä»¬å°†ä¸åŒå­èŠ‚ç‚¹$\mathbf{c}_j$å¯¹åº”çš„è¯¯å·®é¡¹$\delta_{c_j}$è¿æ¥æˆä¸€ä¸ªå‘é‡$\delta_c=\begin{bmatrix}\delta_{c_1}\\\delta_{c_2}\end{bmatrix}$ã€‚é‚£ä¹ˆï¼Œä¸Šå¼å¯ä»¥å†™æˆï¼š</p><p>$\begin{align} \delta_c=W^T\delta_p\circ fâ€™(\mathbf{net}_c)\qquad(å¼2)\end{align} $</p><p><strong>å¼2</strong>å°±æ˜¯å°†è¯¯å·®é¡¹ä»çˆ¶èŠ‚ç‚¹ä¼ é€’åˆ°å…¶å­èŠ‚ç‚¹çš„å…¬å¼ã€‚æ³¨æ„ï¼Œä¸Šå¼ä¸­çš„$\mathbf{net}_c$ä¹Ÿæ˜¯å°†ä¸¤ä¸ªå­èŠ‚ç‚¹çš„åŠ æƒè¾“å…¥$\mathbf{net}_{c_1}$å’Œ$\mathbf{net}_{c_2}$è¿åœ¨ä¸€èµ·çš„å‘é‡ã€‚</p><p>æœ‰äº†ä¼ é€’ä¸€å±‚çš„å…¬å¼ï¼Œæˆ‘ä»¬å°±ä¸éš¾å†™å‡ºé€å±‚ä¼ é€’çš„å…¬å¼ã€‚</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-02c71c9fade90557.png)</p><p>ä¸Šå›¾æ˜¯åœ¨æ ‘å‹ç»“æ„ä¸­åå‘ä¼ é€’è¯¯å·®é¡¹çš„å…¨æ™¯å›¾ï¼Œåå¤åº”ç”¨<strong>å¼2</strong>ï¼Œåœ¨å·²çŸ¥$\delta_p^{(3)}$çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éš¾ç®—å‡º$\delta_p^{(1)}$ä¸ºï¼š</p><p>$\begin{align} \delta^{(2)}&amp;=W^T\delta_p^{(3)}\circ fâ€™(\mathbf{net}^{(2)})\\ \delta_p^{(2)}&amp;=[\delta^{(2)}]_p\\ \delta^{(1)}&amp;=W^T\delta_p^{(2)}\circ fâ€™(\mathbf{net}^{(1)})\\ \delta_p^{(1)}&amp;=[\delta^{(1)}]_p\\ \end{align} $</p><p>åœ¨ä¸Šé¢çš„å…¬å¼ä¸­ï¼Œ$\delta^{(2)}=\begin{bmatrix}\delta_c^{(2)}\\\delta_p^{(2)}\end{bmatrix}$ï¼Œ$[\delta^{(2)}]_p$è¡¨ç¤ºå–å‘é‡$\delta^{(2)}$å±äºèŠ‚ç‚¹pçš„éƒ¨åˆ†ã€‚</p><h3 id="æƒé‡æ¢¯åº¦çš„è®¡ç®—"><a href="#æƒé‡æ¢¯åº¦çš„è®¡ç®—" class="headerlink" title="æƒé‡æ¢¯åº¦çš„è®¡ç®—"></a>æƒé‡æ¢¯åº¦çš„è®¡ç®—</h3><p>æ ¹æ®åŠ æƒè¾“å…¥çš„è®¡ç®—å…¬å¼ï¼š</p><p>$\begin {align} \mathbf{net}_p^{(l)}=W\mathbf{c}^{(l)}+\mathbf{b} \end{align} $</p><p>å…¶ä¸­ï¼Œ$\mathbf{net}_p^{(l)}$è¡¨ç¤ºç¬¬lå±‚çš„çˆ¶èŠ‚ç‚¹çš„åŠ æƒè¾“å…¥ï¼Œ$\mathbf{c}^{(l)}$è¡¨ç¤ºç¬¬lå±‚çš„å­èŠ‚ç‚¹ã€‚$W$æ˜¯æƒé‡çŸ©é˜µï¼Œ$\mathbf{b}$æ˜¯åç½®é¡¹ã€‚å°†å…¶å±•å¼€å¯å¾—ï¼š</p><p>$\begin {align} \mathbf{net}_{p_j}^l=\sum_i{w_{ji}c_i^l}+b_j \end{align} $</p><p>é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥æ±‚å¾—è¯¯å·®å‡½æ•°åœ¨ç¬¬lå±‚å¯¹æƒé‡çš„æ¢¯åº¦ä¸ºï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{w_{ji}^{(l)}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{p_j}^{(l)}}}\frac{\partial{\mathbf{net}_{p_j}^{(l)}}}{\partial{w_{ji}^{(l)}}}\\ &amp;=\delta_{p_j}^{(l)}c_i^{(l)}\\ \end{align} $</p><p>ä¸Šå¼æ˜¯é’ˆå¯¹ä¸€ä¸ªæƒé‡é¡¹$w_{ji}$çš„å…¬å¼ï¼Œç°åœ¨éœ€è¦æŠŠå®ƒæ‰©å±•ä¸ºå¯¹æ‰€æœ‰çš„æƒé‡é¡¹çš„å…¬å¼ã€‚æˆ‘ä»¬å¯ä»¥æŠŠä¸Šå¼å†™æˆçŸ©é˜µçš„å½¢å¼ï¼ˆåœ¨ä¸‹é¢çš„å…¬å¼ä¸­ï¼Œm=2nï¼‰ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{W^{(l)}}}&amp;= \begin{bmatrix} \frac{\partial{E}}{\partial{w_{11}^{(l)}}}&amp; \frac{\partial{E}}{\partial{w_{12}^{(l)}}}&amp; â€¦&amp; \frac{\partial{E}}{\partial{w_{1m}^{(l)}}}\\ \frac{\partial{E}}{\partial{w_{21}^{(l)}}}&amp; \frac{\partial{E}}{\partial{w_{22}^{(l)}}}&amp; â€¦&amp; \frac{\partial{E}}{\partial{w_{2m}^{(l)}}}\\ \\\\\...\\\\ \frac{\partial{E}}{\partial{w_{n1}^{(l)}}}&amp; \frac{\partial{E}}{\partial{w_{n2}^{(l)}}}&amp; â€¦&amp; \frac{\partial{E}}{\partial{w_{nm}^{(l)}}}\\ \end{bmatrix}\\ &amp;= \begin{bmatrix} \delta_{p_1}^{(l)}c_1^l&amp;\delta_{p_1}^{(l)}c_2^l&amp;â€¦&amp;\delta_{p_1}^lc_m^{(l)}\\ \delta_{p_2}^{(l)}c_1^l&amp;\delta_{p_2}^{(l)}c_2^l&amp;â€¦&amp;\delta_{p_2}^lc_m^{(l)}\\ \\\\\...\\\\ \delta_{p_n}^{(l)}c_1^l&amp;\delta_{p_n}^{(l)}lc_2^l&amp;â€¦&amp;\delta_{p_n}^lc_m^{(l)}\\ \end{bmatrix}\\ &amp;=\delta^(\mathbf{c}^{(l)})^T\qquad(å¼3) \end{align} $</p><p><strong>å¼3</strong>å°±æ˜¯ç¬¬lå±‚æƒé‡é¡¹çš„æ¢¯åº¦è®¡ç®—å…¬å¼ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œç”±äºæƒé‡$W$æ˜¯åœ¨æ‰€æœ‰å±‚å…±äº«çš„ï¼Œæ‰€ä»¥å’Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ä¸€æ ·ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„æœ€ç»ˆçš„<strong>æƒé‡æ¢¯åº¦æ˜¯å„ä¸ªå±‚æƒé‡æ¢¯åº¦ä¹‹å’Œ</strong>ã€‚å³ï¼š</p><p>$\begin {align} \frac{\partial{E}}{\partial{W}}=\sum_l\frac{\partial{E}}{\partial{W^{(l)}}}\qquad(å¼4) \end{align} $</p><p>å› ä¸º<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>çš„è¯æ˜è¿‡ç¨‹å·²ç»åœ¨<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (4) - å·ç§¯ç¥ç»ç½‘ç»œ</a>ä¸€æ–‡ä¸­ç»™å‡ºï¼Œå› æ­¤ï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>ã€ä¸ºä»€ä¹ˆæœ€ç»ˆæ¢¯åº¦æ˜¯å„å±‚æ¢¯åº¦ä¹‹å’Œã€çš„è¯æ˜å°±ç•™ç»™è¯»è€…è‡ªè¡Œå®Œæˆå•¦ã€‚</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ±‚åç½®é¡¹$\mathbf{b}$çš„æ¢¯åº¦è®¡ç®—å…¬å¼ã€‚å…ˆè®¡ç®—è¯¯å·®å‡½æ•°å¯¹ç¬¬lå±‚åç½®é¡¹$\mathbf{b}^{(l)}$çš„æ¢¯åº¦ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{b_j^{(l)}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{p_j}^{(l)}}}\frac{\partial{\mathbf{net}_{p_j}^{(l)}}}{\partial{b_j^{(l)}}}\\ &amp;=\delta_{p_j}^{(l)}\\ \end{align} $</p><p>æŠŠä¸Šå¼æ‰©å±•ä¸ºçŸ©é˜µçš„å½¢å¼ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{\mathbf{b}^{(l)}}}&amp;= \begin{bmatrix} \frac{\partial{E}}{\partial{b_1^{(l)}}}\\ \frac{\partial{E}}{\partial{b_2^{(l)}}}\\ \\\\\...\\\\ \frac{\partial{E}}{\partial{b_n^{(l)}}}\\ \end{bmatrix}\\ &amp;= \begin{bmatrix} \delta_{p_1}^{(l)}\\ \delta_{p_2}^{(l)}\\ \\\\\...\\\\ \delta_{p_n}^{(l)}\\ \end{bmatrix}\\ &amp;=\delta_p^\qquad(å¼5) \end{align} $</p><p><strong>å¼5</strong>æ˜¯ç¬¬$l$å±‚åç½®é¡¹çš„æ¢¯åº¦ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„åç½®é¡¹æ¢¯åº¦æ˜¯å„ä¸ªå±‚åç½®é¡¹æ¢¯åº¦ä¹‹å’Œï¼Œå³ï¼š</p><p>$\begin {align}\frac{\partial{E}}{\partial{\mathbf{b}}}=\sum_l\frac{\partial{E}}{\partial{\mathbf{b}^{(l)}}}\qquad(å¼6) \end {align} $</p><h3 id="æƒé‡æ›´æ–°"><a href="#æƒé‡æ›´æ–°" class="headerlink" title="æƒé‡æ›´æ–°"></a>æƒé‡æ›´æ–°</h3><p>å¦‚æœä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•ï¼Œé‚£ä¹ˆæƒé‡æ›´æ–°å…¬å¼ä¸ºï¼š</p><p>$\begin {align} W\gets W + \eta\frac{\partial{E}}{\partial{W}} \end{align}  $</p><p>å…¶ä¸­ï¼Œ$\eta$æ˜¯å­¦ä¹ é€Ÿç‡å¸¸æ•°ã€‚æŠŠ<strong>å¼4</strong>å¸¦å…¥åˆ°ä¸Šå¼ï¼Œå³å¯å®Œæˆæƒé‡çš„æ›´æ–°ã€‚åŒç†ï¼Œåç½®é¡¹çš„æ›´æ–°å…¬å¼ä¸ºï¼š</p><p>$\begin {align} \mathbf{b}\gets \mathbf{b} + \eta\frac{\partial{E}}{\partial{\mathbf{b}}} \end {align} $</p><p>æŠŠ<strong>å¼6</strong>å¸¦å…¥åˆ°ä¸Šå¼ï¼Œå³å¯å®Œæˆåç½®é¡¹çš„æ›´æ–°ã€‚</p><p>è¿™å°±æ˜¯<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„è®­ç»ƒç®—æ³•BPTSã€‚ç”±äºæˆ‘ä»¬æœ‰äº†å‰é¢å‡ ç¯‡æ–‡ç« çš„åŸºç¡€ï¼Œç›¸ä¿¡è¯»è€…ä»¬ç†è§£BPTSç®—æ³•ä¹Ÿä¼šæ¯”è¾ƒå®¹æ˜“ã€‚</p><h2 id="é€’å½’ç¥ç»ç½‘ç»œçš„å®ç°"><a href="#é€’å½’ç¥ç»ç½‘ç»œçš„å®ç°" class="headerlink" title="é€’å½’ç¥ç»ç½‘ç»œçš„å®ç°"></a>é€’å½’ç¥ç»ç½‘ç»œçš„å®ç°</h2><blockquote><p>å®Œæ•´ä»£ç è¯·å‚è€ƒGitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/recursive.py" target="_blank" rel="noopener">https://github.com/hanbt/learn_dl/blob/master/recursive.py</a> (python2.7)</p></blockquote><p>ç°åœ¨ï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªå¤„ç†æ ‘å‹ç»“æ„çš„<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>ã€‚</p><p>åœ¨æ–‡ä»¶çš„å¼€å¤´ï¼ŒåŠ å…¥å¦‚ä¸‹ä»£ç ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">#!/usr/bin/env python# -*- coding: UTF-8 -*-import numpy as npfrom cnn import IdentityActivator</code></pre><p>ä¸Šè¿°å››è¡Œä»£ç éå¸¸ç®€å•ï¼Œæ²¡æœ‰ä»€ä¹ˆéœ€è¦è§£é‡Šçš„ã€‚IdentityActivatoræ¿€æ´»å‡½æ•°æ˜¯åœ¨æˆ‘ä»¬ä»‹ç»<strong>å·ç§¯ç¥ç»ç½‘ç»œ</strong>æ—¶å†™çš„ï¼Œç°åœ¨å¼•ç”¨ä¸€ä¸‹å®ƒã€‚</p><p>æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ ‘èŠ‚ç‚¹ç»“æ„ï¼Œè¿™æ ·ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨å®ƒä¿å­˜å·ç§¯ç¥ç»ç½‘ç»œç”Ÿæˆçš„æ•´æ£µæ ‘ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">class TreeNode(object):    def __init__(self, data, children=[], children_data=[]):        self.parent = None        self.children = children        self.children_data = children_data        self.data = data        for child in children:            child.parent = self</code></pre><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„å®ç°ä»£ç éƒ½æ”¾åœ¨RecursiveLayerç±»ä¸­ï¼Œä¸‹é¢æ˜¯è¿™ä¸ªç±»çš„æ„é€ å‡½æ•°ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python"># é€’å½’ç¥ç»ç½‘ç»œå®ç°class RecursiveLayer(object):    def __init__(self, node_width, child_count,                  activator, learning_rate):        '''        é€’å½’ç¥ç»ç½‘ç»œæ„é€ å‡½æ•°        node_width: è¡¨ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„å‘é‡çš„ç»´åº¦        child_count: æ¯ä¸ªçˆ¶èŠ‚ç‚¹æœ‰å‡ ä¸ªå­èŠ‚ç‚¹        activator: æ¿€æ´»å‡½æ•°å¯¹è±¡        learning_rate: æ¢¯åº¦ä¸‹é™ç®—æ³•å­¦ä¹ ç‡        '''        self.node_width = node_width        self.child_count = child_count        self.activator = activator        self.learning_rate = learning_rate        # æƒé‡æ•°ç»„W        self.W = np.random.uniform(-1e-4, 1e-4,            (node_width, node_width * child_count))        # åç½®é¡¹b        self.b = np.zeros((node_width, 1))        # é€’å½’ç¥ç»ç½‘ç»œç”Ÿæˆçš„æ ‘çš„æ ¹èŠ‚ç‚¹        self.root = None</code></pre><p>ä¸‹é¢æ˜¯å‰å‘è®¡ç®—çš„å®ç°ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">    def forward(self, *children):        '''        å‰å‘è®¡ç®—        '''        children_data = self.concatenate(children)        parent_data = self.activator.forward(            np.dot(self.W, children_data) + self.b        )        self.root = TreeNode(parent_data, children                            , children_data)</code></pre><p>forwardå‡½æ•°æ¥æ”¶ä¸€ç³»åˆ—çš„æ ‘èŠ‚ç‚¹å¯¹è±¡ä½œä¸ºè¾“å…¥ï¼Œç„¶åï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å°†è¿™äº›æ ‘èŠ‚ç‚¹ä½œä¸ºå­èŠ‚ç‚¹ï¼Œå¹¶è®¡ç®—å®ƒä»¬çš„çˆ¶èŠ‚ç‚¹ã€‚æœ€åï¼Œå°†è®¡ç®—çš„çˆ¶èŠ‚ç‚¹ä¿å­˜åœ¨self.rootå˜é‡ä¸­ã€‚</p><p>ä¸Šé¢ç”¨åˆ°çš„concatenateå‡½æ•°ï¼Œæ˜¯å°†å„ä¸ªå­èŠ‚ç‚¹ä¸­çš„æ•°æ®æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡ï¼Œå…¶ä»£ç å¦‚ä¸‹ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">    def concatenate(self, tree_nodes):        '''        å°†å„ä¸ªæ ‘èŠ‚ç‚¹ä¸­çš„æ•°æ®æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡        '''        concat = np.zeros((0,1))        for node in tree_nodes:            concat = np.concatenate((concat, node.data))        return concat</code></pre><p>ä¸‹é¢æ˜¯åå‘ä¼ æ’­ç®—æ³•BPTSçš„å®ç°ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">    def backward(self, parent_delta):        '''        BPTSåå‘ä¼ æ’­ç®—æ³•        '''        self.calc_delta(parent_delta, self.root)        self.W_grad, self.b_grad = self.calc_gradient(self.root)    def calc_delta(self, parent_delta, parent):        '''        è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„delta        '''        parent.delta = parent_delta        if parent.children:            # æ ¹æ®å¼2è®¡ç®—æ¯ä¸ªå­èŠ‚ç‚¹çš„delta            children_delta = np.dot(self.W.T, parent_delta) * (                self.activator.backward(parent.children_data)            )            # slices = [(å­èŠ‚ç‚¹ç¼–å·ï¼Œå­èŠ‚ç‚¹deltaèµ·å§‹ä½ç½®ï¼Œå­èŠ‚ç‚¹deltaç»“æŸä½ç½®)]            slices = [(i, i * self.node_width,                         (i + 1) * self.node_width)                        for i in range(self.child_count)]            # é’ˆå¯¹æ¯ä¸ªå­èŠ‚ç‚¹ï¼Œé€’å½’è°ƒç”¨calc_deltaå‡½æ•°            for s in slices:                self.calc_delta(children_delta[s[1]:s[2]],                                 parent.children[s[0]])    def calc_gradient(self, parent):        '''        è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹æƒé‡çš„æ¢¯åº¦ï¼Œå¹¶å°†å®ƒä»¬æ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„æ¢¯åº¦        '''        W_grad = np.zeros((self.node_width,                             self.node_width * self.child_count))        b_grad = np.zeros((self.node_width, 1))        if not parent.children:            return W_grad, b_grad        parent.W_grad = np.dot(parent.delta, parent.children_data.T)        parent.b_grad = parent.delta        W_grad += parent.W_grad        b_grad += parent.b_grad        for child in parent.children:            W, b = self.calc_gradient(child)            W_grad += W            b_grad += b        return W_grad, b_grad</code></pre><p>åœ¨ä¸Šè¿°ç®—æ³•ä¸­ï¼Œcalc_deltaå‡½æ•°å’Œcalc_gradientå‡½æ•°åˆ†åˆ«è®¡ç®—å„ä¸ªèŠ‚ç‚¹çš„è¯¯å·®é¡¹ä»¥åŠæœ€ç»ˆçš„æ¢¯åº¦ã€‚å®ƒä»¬éƒ½é‡‡ç”¨é€’å½’ç®—æ³•ï¼Œå…ˆåºéå†æ•´ä¸ªæ ‘ï¼Œå¹¶é€ä¸€å®Œæˆæ¯ä¸ªèŠ‚ç‚¹çš„è®¡ç®—ã€‚</p><p>ä¸‹é¢æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®ç°ï¼ˆæ²¡æœ‰weight decayï¼‰ï¼Œè¿™ä¸ªéå¸¸ç®€å•ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">    def update(self):        '''        ä½¿ç”¨SGDç®—æ³•æ›´æ–°æƒé‡        '''        self.W -= self.learning_rate * self.W_grad        self.b -= self.learning_rate * self.b_grad</code></pre><p>ä»¥ä¸Šå°±æ˜¯<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>çš„å®ç°ï¼Œæ€»å…±100è¡Œå·¦å³ï¼Œå’Œä¸Šä¸€ç¯‡æ–‡ç« çš„LSTMç›¸æ¯”ç®€å•å¤šäº†ã€‚</p><p>æœ€åï¼Œæˆ‘ä»¬ç”¨æ¢¯åº¦æ£€æŸ¥æ¥éªŒè¯ç¨‹åºçš„æ­£ç¡®æ€§ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">def gradient_check():    '''    æ¢¯åº¦æ£€æŸ¥    '''    # è®¾è®¡ä¸€ä¸ªè¯¯å·®å‡½æ•°ï¼Œå–æ‰€æœ‰èŠ‚ç‚¹è¾“å‡ºé¡¹ä¹‹å’Œ    error_function = lambda o: o.sum()    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)    # è®¡ç®—forwardå€¼    x, d = data_set()    rnn.forward(x[0], x[1])    rnn.forward(rnn.root, x[2])    # æ±‚å–sensitivity map    sensitivity_array = np.ones((rnn.node_width, 1),                                dtype=np.float64)    # è®¡ç®—æ¢¯åº¦    rnn.backward(sensitivity_array)    # æ£€æŸ¥æ¢¯åº¦    epsilon = 10e-4    for i in range(rnn.W.shape[0]):        for j in range(rnn.W.shape[1]):            rnn.W[i,j] += epsilon            rnn.reset_state()            rnn.forward(x[0], x[1])            rnn.forward(rnn.root, x[2])            err1 = error_function(rnn.root.data)            rnn.W[i,j] -= 2*epsilon            rnn.reset_state()            rnn.forward(x[0], x[1])            rnn.forward(rnn.root, x[2])            err2 = error_function(rnn.root.data)            expect_grad = (err1 - err2) / (2 * epsilon)            rnn.W[i,j] += epsilon            print 'weights(%d,%d): expected - actural %.4e - %.4e' % (                i, j, expect_grad, rnn.W_grad[i,j])    return rnn</code></pre><p>ä¸‹é¢æ˜¯æ¢¯åº¦æ£€æŸ¥çš„ç»“æœï¼Œå®Œå…¨æ­£ç¡®ï¼ŒOH YEAHï¼</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-bea78770357e735a.png)</p><h2 id="é€’å½’ç¥ç»ç½‘ç»œçš„åº”ç”¨"><a href="#é€’å½’ç¥ç»ç½‘ç»œçš„åº”ç”¨" class="headerlink" title="é€’å½’ç¥ç»ç½‘ç»œçš„åº”ç”¨"></a>é€’å½’ç¥ç»ç½‘ç»œçš„åº”ç”¨</h2><h3 id="è‡ªç„¶è¯­è¨€å’Œè‡ªç„¶åœºæ™¯è§£æ"><a href="#è‡ªç„¶è¯­è¨€å’Œè‡ªç„¶åœºæ™¯è§£æ" class="headerlink" title="è‡ªç„¶è¯­è¨€å’Œè‡ªç„¶åœºæ™¯è§£æ"></a>è‡ªç„¶è¯­è¨€å’Œè‡ªç„¶åœºæ™¯è§£æ</h3><p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå®ç°ä¸€ä¸ªè§£æå™¨ï¼Œå°†è‡ªç„¶è¯­è¨€è§£æä¸ºè¯­æ³•æ ‘ï¼Œé‚£ä¹ˆæ¯«æ— ç–‘é—®ï¼Œè¿™å°†å¤§å¤§æå‡æˆ‘ä»¬å¯¹è‡ªç„¶è¯­è¨€çš„å¤„ç†èƒ½åŠ›ã€‚è§£æå™¨å¦‚ä¸‹æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-6b7f075645c12c92.png)</p><p>å¯ä»¥çœ‹å‡ºï¼Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>èƒ½å¤Ÿå®Œæˆå¥å­çš„è¯­æ³•åˆ†æï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªè¯­æ³•è§£ææ ‘ã€‚</p><p>é™¤äº†è‡ªç„¶è¯­è¨€ä¹‹å¤–ï¼Œè‡ªç„¶åœºæ™¯ä¹Ÿå…·æœ‰<strong>å¯ç»„åˆ</strong>çš„æ€§è´¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ç±»ä¼¼çš„æ¨¡å‹å®Œæˆè‡ªç„¶åœºæ™¯çš„è§£æï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-7ed5b7d06a8cc583.png)</p><p>ä¸¤ç§ä¸åŒçš„åœºæ™¯ï¼Œå¯ä»¥ç”¨ç›¸åŒçš„<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>æ¨¡å‹æ¥å®ç°ã€‚æˆ‘ä»¬ä»¥ç¬¬ä¸€ä¸ªåœºæ™¯ï¼Œè‡ªç„¶è¯­è¨€è§£æä¸ºä¾‹ã€‚</p><p>æˆ‘ä»¬å¸Œæœ›å°†ä¸€å¥è¯é€å­—è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œç„¶åï¼Œç¥ç»ç½‘ç»œè¿”å›ä¸€ä¸ªè§£æå¥½çš„æ ‘ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ç»™ç¥ç»ç½‘ç»œå†åŠ ä¸Šä¸€å±‚ï¼Œè´Ÿè´£æ‰“åˆ†ã€‚åˆ†æ•°è¶Šé«˜ï¼Œè¯´æ˜ä¸¤ä¸ªå­èŠ‚ç‚¹ç»“åˆæ›´åŠ ç´§å¯†ï¼Œåˆ†æ•°è¶Šä½ï¼Œè¯´æ˜ä¸¤ä¸ªå­èŠ‚ç‚¹ç»“åˆæ›´æ¾æ•£ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-2f0393edbc04c30e.png)</p><p>ä¸€æ—¦è¿™ä¸ªæ‰“åˆ†å‡½æ•°è®­ç»ƒå¥½äº†ï¼ˆä¹Ÿå°±æ˜¯çŸ©é˜µUçš„å„é¡¹å€¼å˜ä¸ºåˆé€‚çš„å€¼ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è´ªå¿ƒç®—æ³•æ¥å®ç°å¥å­çš„è§£æã€‚ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å…ˆå°†è¯æŒ‰ç…§é¡ºåºä¸¤ä¸¤è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå¾—åˆ°ç¬¬ä¸€ç»„æ‰“åˆ†ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-be0eb2d1a3527bc6.png)</p><p>æˆ‘ä»¬å‘ç°ï¼Œç°åœ¨åˆ†æ•°æœ€é«˜çš„æ˜¯ç¬¬ä¸€ç»„ï¼ŒThe catï¼Œè¯´æ˜å®ƒä»¬çš„ç»“åˆæ˜¯æœ€ç´§å¯†çš„ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå°†å®ƒä»¬ç»„åˆä¸ºä¸€ä¸ªèŠ‚ç‚¹ã€‚ç„¶åï¼Œå†æ¬¡ä¸¤ä¸¤è®¡ç®—ç›¸é‚»å­èŠ‚ç‚¹çš„æ‰“åˆ†ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-bb8f87bc4f56f5ee.png)</p><p>ç°åœ¨ï¼Œåˆ†æ•°æœ€é«˜çš„æ˜¯æœ€åä¸€ç»„ï¼Œthe matã€‚äºæ˜¯ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ç»„åˆä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œå†ä¸¤ä¸¤è®¡ç®—ç›¸é‚»èŠ‚ç‚¹çš„æ‰“åˆ†ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-9c3332923237d11c.png)</p><p>è¿™æ—¶ï¼Œæˆ‘ä»¬å‘ç°æœ€é«˜çš„åˆ†æ•°æ˜¯on the matï¼ŒæŠŠå®ƒä»¬ç»„åˆä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œç»§ç»­ä¸¤ä¸¤è®¡ç®—ç›¸é‚»èŠ‚ç‚¹çš„æ‰“åˆ†â€¦â€¦æœ€ç»ˆï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿå¾—åˆ°æ•´ä¸ªè§£ææ ‘ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-14f0140e4eafa5ca.png)</p><p>ç°åœ¨ï¼Œæˆ‘ä»¬å›°æƒ‘è¿™æ ·ç‰›é€¼çš„æ‰“åˆ†å‡½æ•°scoreæ˜¯æ€æ ·è®­ç»ƒå‡ºæ¥çš„å‘¢ï¼Ÿæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨Max-Marginç›®æ ‡å‡½æ•°ã€‚å®ƒçš„å®šä¹‰å¦‚ä¸‹ï¼š</p><p>$\begin {align} J(\theta)=max(0, \sum_i\underset{y\in A(x_i)}{max}(s(x_i,y)+\Delta(y,y_i))-s(x_i,y_i)) \end{align} $</p><p>åœ¨ä¸Šå¼ä¸­ï¼Œ$x_i$ã€$y_i$åˆ†åˆ«è¡¨ç¤ºç¬¬iä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å…¥å’Œæ ‡ç­¾ï¼Œæ³¨æ„è¿™é‡Œçš„æ ‡ç­¾$y_i$æ˜¯ä¸€æ£µè§£ææ ‘ã€‚$s(x_i,y_i)$å°±æ˜¯æ‰“åˆ†å‡½æ•°så¯¹ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬çš„æ‰“åˆ†ã€‚å› ä¸ºè®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾è‚¯å®šæ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬å¸Œæœ›så¯¹å®ƒçš„æ‰“åˆ†è¶Šé«˜è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯$s(x_i,y_i)$è¶Šå¤§è¶Šå¥½ã€‚$A(x_1)$æ˜¯æ‰€æœ‰å¯èƒ½çš„è§£ææ ‘çš„é›†åˆï¼Œè€Œ$s(x_i,y)$åˆ™æ˜¯å¯¹æŸä¸ªå¯èƒ½çš„è§£ææ ‘$y$çš„æ‰“åˆ†ã€‚$\Delta(y,y_i)$æ˜¯å¯¹é”™è¯¯çš„æƒ©ç½šã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæŸä¸ªè§£ææ ‘$y$å’Œæ ‡ç­¾$y_{i}$æ˜¯ä¸€æ ·çš„ï¼Œé‚£ä¹ˆ$\Delta(y,y_i)$ä¸º0ï¼Œå¦‚æœç½‘ç»œçš„è¾“å‡ºé”™çš„è¶Šç¦»è°±ï¼Œé‚£ä¹ˆæƒ©ç½šé¡¹$\Delta(y,y_i)$çš„å€¼å°±è¶Šé«˜ã€‚$max(s(x_i,y)+\Delta(y,y_i))$è¡¨ç¤ºæ‰€æœ‰æ ‘é‡Œé¢æœ€é«˜å¾—åˆ†ã€‚åœ¨è¿™é‡Œï¼Œæƒ©ç½šé¡¹ç›¸å½“äºMarginï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è™½ç„¶å¸Œæœ›æ‰“åˆ†å‡½æ•°så¯¹æ­£ç¡®çš„æ ‘æ‰“åˆ†æ¯”å¯¹é”™è¯¯çš„æ ‘æ‰“åˆ†é«˜ï¼Œä½†ä¹Ÿä¸è¦é«˜è¿‡Marginçš„å€¼ã€‚æˆ‘ä»¬ä¼˜åŒ–$\theta$ï¼Œä½¿ç›®æ ‡å‡½æ•°å–æœ€å°å€¼ï¼Œå³ï¼š</p><p>$\theta=\underset{\theta}{argmin}J(\theta)$</p><p>ä¸‹é¢æ˜¯æƒ©ç½šå‡½æ•°$\Delta$çš„å®šä¹‰ï¼š</p><script type="math/tex; mode=display">\begin {align} \Delta(y,y_i)=k\sum_{d\in N(y)}\mathbf{1}{\{subTree(d)\notin y_i\}} \end{align}</script><p>ä¸Šå¼ä¸­ï¼ŒN(y)æ˜¯æ ‘yèŠ‚ç‚¹çš„é›†åˆï¼›subTree(d)æ˜¯ä»¥dä¸ºèŠ‚ç‚¹çš„å­æ ‘ã€‚ä¸Šå¼çš„å«ä¹‰æ˜¯ï¼Œå¦‚æœä»¥dä¸ºèŠ‚ç‚¹çš„å­æ ‘æ²¡æœ‰å‡ºç°åœ¨æ ‡ç­¾ä¸­ï¼Œé‚£ä¹ˆå‡½æ•°å€¼+1ã€‚æœ€ç»ˆï¼Œæƒ©ç½šå‡½æ•°çš„å€¼ï¼Œæ˜¯æ ‘yä¸­æ²¡æœ‰å‡ºç°åœ¨æ ‘ä¸­çš„å­æ ‘çš„ä¸ªæ•°ï¼Œå†ä¹˜ä¸Šä¸€ä¸ªç³»æ•°kã€‚å…¶å®ä¹Ÿå°±æ˜¯å…³äºä¸¤æ£µæ ‘å·®å¼‚çš„ä¸€ä¸ªåº¦é‡ã€‚</p><p>$s(x,y)$æ˜¯å¯¹ä¸€ä¸ªæ ·æœ¬æœ€ç»ˆçš„æ‰“åˆ†ï¼Œå®ƒæ˜¯å¯¹æ ‘yæ¯ä¸ªèŠ‚ç‚¹æ‰“åˆ†çš„æ€»å’Œã€‚</p><p>$\begin {align} s(x,y)=\sum_{n\in nodes(y)}s_n \end{align} $</p><p>å…·ä½“ç»†èŠ‚ï¼Œè¯»è€…å¯ä»¥æŸ¥é˜…ã€å‚è€ƒèµ„æ–™3ã€çš„è®ºæ–‡ã€‚</p><h2 id="å°ç»“"><a href="#å°ç»“" class="headerlink" title="å°ç»“"></a>å°ç»“</h2><p>æˆ‘ä»¬åœ¨ç³»åˆ—æ–‡ç« ä¸­å·²ç»ä»‹ç»çš„<strong>å…¨è¿æ¥ç¥ç»ç½‘ç»œ</strong>ã€<strong>å·ç§¯ç¥ç»ç½‘ç»œ</strong>ã€<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å’Œ<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>ï¼Œåœ¨è®­ç»ƒæ—¶éƒ½ä½¿ç”¨äº†<strong>ç›‘ç£å­¦ä¹ (Supervised Learning)</strong>ä½œä¸ºè®­ç»ƒæ–¹æ³•ã€‚åœ¨<strong>ç›‘ç£å­¦ä¹ </strong>ä¸­ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬æ—¢åŒ…æ‹¬è¾“å…¥ç‰¹å¾$\mathbf{x}$ï¼Œä¹ŸåŒ…æ‹¬æ ‡è®°$\mathbf{y}$ï¼Œå³æ ·æœ¬$d^{(i)}=\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}$ã€‚ç„¶è€Œï¼Œå¾ˆå¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—å½¢å¦‚$\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}$çš„æ ·æœ¬ï¼Œè¿™æ—¶ï¼Œæˆ‘ä»¬å°±ä¸èƒ½é‡‡ç”¨<strong>ç›‘ç£å­¦ä¹ </strong>çš„æ–¹æ³•ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»å¦å¤–ä¸€ç§å­¦ä¹ æ–¹æ³•ï¼š<strong>å¢å¼ºå­¦ä¹ (Reinforcement Learning)</strong>ã€‚åœ¨äº†è§£<strong>å¢å¼ºå­¦ä¹ </strong>çš„ä¸»è¦ç®—æ³•ä¹‹åï¼Œæˆ‘ä»¬è¿˜å°†ä»‹ç»è‘—åçš„å›´æ£‹è½¯ä»¶<strong>AlphaGo</strong>ï¼Œå®ƒæ˜¯ä¸€ä¸ªæŠŠ<strong>ç›‘ç£å­¦ä¹ </strong>å’Œ<strong>å¢å¼ºå­¦ä¹ </strong>è¿›è¡Œå®Œç¾ç»“åˆçš„æ¡ˆä¾‹ã€‚</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(7" alt="img">%20-%20%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-9f3e58723eee0af3.jpg)</p><h2 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h2><ol><li><a href="http://cs224d.stanford.edu/" target="_blank" rel="noopener">CS224d: Deep Learning for Natural Language Processing</a></li><li><a href="https://pdfs.semanticscholar.org/794e/6ed81d21f1bf32a0fd3be05c44c1fa362688.pdf" target="_blank" rel="noopener">Learning Task-Dependent Distributed Representations by Back Propagation Through Structure</a></li><li><a href="http://ai.stanford.edu/~ang/papers/icml11-ParsingWithRecursiveNeuralNetworks.pdf" target="_blank" rel="noopener">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ -å¾ªç¯ç¥ç»ç½‘ç»œ</title>
      <link href="/ling-ji-chu-ru-men-shen-du-xue-xi-5-xun-huan-shen-jing-wang-luo/"/>
      <url>/ling-ji-chu-ru-men-shen-du-xue-xi-5-xun-huan-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒèµ„æ–™ï¼š<a href="https://www.zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/541458</a></p><hr><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-06627c71f0d8c0dc.jpg)</p><blockquote><p>æ— è®ºå³å°†åˆ°æ¥çš„æ˜¯å¤§æ•°æ®æ—¶ä»£è¿˜æ˜¯äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œäº¦æˆ–æ˜¯ä¼ ç»Ÿè¡Œä¸šä½¿ç”¨äººå·¥æ™ºèƒ½åœ¨äº‘ä¸Šå¤„ç†å¤§æ•°æ®çš„æ—¶ä»£ï¼Œä½œä¸ºä¸€ä¸ªæœ‰ç†æƒ³æœ‰è¿½æ±‚çš„ç¨‹åºå‘˜ï¼Œä¸æ‡‚æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰è¿™ä¸ªè¶…çƒ­çš„æŠ€æœ¯ï¼Œä¼šä¸ä¼šæ„Ÿè§‰é©¬ä¸Šå°±outäº†ï¼Ÿç°åœ¨æ•‘å‘½ç¨»è‰æ¥äº†ï¼Œã€Šé›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ ã€‹ç³»åˆ—æ–‡ç« æ—¨åœ¨è®²å¸®åŠ©çˆ±ç¼–ç¨‹çš„ä½ ä»é›¶åŸºç¡€è¾¾åˆ°å…¥é—¨çº§æ°´å¹³ã€‚é›¶åŸºç¡€æ„å‘³ç€ä½ ä¸éœ€è¦å¤ªå¤šçš„æ•°å­¦çŸ¥è¯†ï¼Œåªè¦ä¼šå†™ç¨‹åºå°±è¡Œäº†ï¼Œæ²¡é”™ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç¨‹åºå‘˜å†™çš„æ–‡ç« ã€‚è™½ç„¶æ–‡ä¸­ä¼šæœ‰å¾ˆå¤šå…¬å¼ä½ ä¹Ÿè®¸çœ‹ä¸æ‡‚ï¼Œä½†åŒæ—¶ä¹Ÿä¼šæœ‰æ›´å¤šçš„ä»£ç ï¼Œç¨‹åºå‘˜çš„ä½ ä¸€å®šèƒ½çœ‹æ‡‚çš„ï¼ˆæˆ‘å‘¨å›´æ˜¯ä¸€ç¾¤ç‹‚çƒ­çš„Clean Codeç¨‹åºå‘˜ï¼Œæ‰€ä»¥æˆ‘å†™çš„ä»£ç ä¹Ÿä¸ä¼šå¾ˆå·®ï¼‰ã€‚</p></blockquote><h2 id="å¾€æœŸå›é¡¾"><a href="#å¾€æœŸå›é¡¾" class="headerlink" title="å¾€æœŸå›é¡¾"></a>å¾€æœŸå›é¡¾</h2><p>åœ¨å‰é¢çš„æ–‡ç« ç³»åˆ—æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å…¨è¿æ¥ç¥ç»ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥åŠå®ƒä»¬çš„è®­ç»ƒå’Œä½¿ç”¨ã€‚ä»–ä»¬éƒ½åªèƒ½å•ç‹¬çš„å–å¤„ç†ä¸€ä¸ªä¸ªçš„è¾“å…¥ï¼Œå‰ä¸€ä¸ªè¾“å…¥å’Œåä¸€ä¸ªè¾“å…¥æ˜¯å®Œå…¨æ²¡æœ‰å…³ç³»çš„ã€‚ä½†æ˜¯ï¼ŒæŸäº›ä»»åŠ¡éœ€è¦èƒ½å¤Ÿæ›´å¥½çš„å¤„ç†<strong>åºåˆ—</strong>çš„ä¿¡æ¯ï¼Œå³å‰é¢çš„è¾“å…¥å’Œåé¢çš„è¾“å…¥æ˜¯æœ‰å…³ç³»çš„ã€‚æ¯”å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨ç†è§£ä¸€å¥è¯æ„æ€æ—¶ï¼Œå­¤ç«‹çš„ç†è§£è¿™å¥è¯çš„æ¯ä¸ªè¯æ˜¯ä¸å¤Ÿçš„ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†è¿™äº›è¯è¿æ¥èµ·æ¥çš„æ•´ä¸ª<strong>åºåˆ—</strong>ï¼›å½“æˆ‘ä»¬å¤„ç†è§†é¢‘çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸èƒ½åªå•ç‹¬çš„å»åˆ†ææ¯ä¸€å¸§ï¼Œè€Œè¦åˆ†æè¿™äº›å¸§è¿æ¥èµ·æ¥çš„æ•´ä¸ª<strong>åºåˆ—</strong>ã€‚è¿™æ—¶ï¼Œå°±éœ€è¦ç”¨åˆ°æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­å¦ä¸€ç±»éå¸¸é‡è¦ç¥ç»ç½‘ç»œï¼š<strong>å¾ªç¯ç¥ç»ç½‘ç»œ(Recurrent Neural Network)</strong>ã€‚RNNç§ç±»å¾ˆå¤šï¼Œä¹Ÿæ¯”è¾ƒç»•è„‘å­ã€‚ä¸è¿‡è¯»è€…ä¸ç”¨æ‹…å¿ƒï¼Œæœ¬æ–‡å°†ä¸€å¦‚æ—¢å¾€çš„å¯¹å¤æ‚çš„ä¸œè¥¿å‰¥èŒ§æŠ½ä¸ï¼Œå¸®åŠ©æ‚¨ç†è§£RNNsä»¥åŠå®ƒçš„è®­ç»ƒç®—æ³•ï¼Œå¹¶åŠ¨æ‰‹å®ç°ä¸€ä¸ª<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ã€‚</p><h2 id="è¯­è¨€æ¨¡å‹"><a href="#è¯­è¨€æ¨¡å‹" class="headerlink" title="è¯­è¨€æ¨¡å‹"></a>è¯­è¨€æ¨¡å‹</h2><p>RNNæ˜¯åœ¨<strong>è‡ªç„¶è¯­è¨€å¤„ç†</strong>é¢†åŸŸä¸­æœ€å…ˆè¢«ç”¨èµ·æ¥çš„ï¼Œæ¯”å¦‚ï¼ŒRNNå¯ä»¥ä¸º<strong>è¯­è¨€æ¨¡å‹</strong>æ¥å»ºæ¨¡ã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹å‘¢ï¼Ÿ</p><p>æˆ‘ä»¬å¯ä»¥å’Œç”µè„‘ç©ä¸€ä¸ªæ¸¸æˆï¼Œæˆ‘ä»¬å†™å‡ºä¸€ä¸ªå¥å­å‰é¢çš„ä¸€äº›è¯ï¼Œç„¶åï¼Œè®©ç”µè„‘å¸®æˆ‘ä»¬å†™ä¸‹æ¥ä¸‹æ¥çš„ä¸€ä¸ªè¯ã€‚æ¯”å¦‚ä¸‹é¢è¿™å¥ï¼š</p><blockquote><p>æˆ‘æ˜¨å¤©ä¸Šå­¦è¿Ÿåˆ°äº†ï¼Œè€å¸ˆæ‰¹è¯„äº†____ã€‚</p></blockquote><p>æˆ‘ä»¬ç»™ç”µè„‘å±•ç¤ºäº†è¿™å¥è¯å‰é¢è¿™äº›è¯ï¼Œç„¶åï¼Œè®©ç”µè„‘å†™ä¸‹æ¥ä¸‹æ¥çš„ä¸€ä¸ªè¯ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¥ä¸‹æ¥çš„è¿™ä¸ªè¯æœ€æœ‰å¯èƒ½æ˜¯ã€æˆ‘ã€ï¼Œè€Œä¸å¤ªå¯èƒ½æ˜¯ã€å°æ˜ã€ï¼Œç”šè‡³æ˜¯ã€åƒé¥­ã€ã€‚</p><p><strong>è¯­è¨€æ¨¡å‹</strong>å°±æ˜¯è¿™æ ·çš„ä¸œè¥¿ï¼šç»™å®šä¸€ä¸ªä¸€å¥è¯å‰é¢çš„éƒ¨åˆ†ï¼Œé¢„æµ‹æ¥ä¸‹æ¥æœ€æœ‰å¯èƒ½çš„ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆã€‚</p><p><strong>è¯­è¨€æ¨¡å‹</strong>æ˜¯å¯¹ä¸€ç§è¯­è¨€çš„ç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œå®ƒæœ‰å¾ˆå¤šå¾ˆå¤šç”¨å¤„ã€‚æ¯”å¦‚åœ¨è¯­éŸ³è½¬æ–‡æœ¬(STT)çš„åº”ç”¨ä¸­ï¼Œå£°å­¦æ¨¡å‹è¾“å‡ºçš„ç»“æœï¼Œå¾€å¾€æ˜¯è‹¥å¹²ä¸ªå¯èƒ½çš„å€™é€‰è¯ï¼Œè¿™æ—¶å€™å°±éœ€è¦<strong>è¯­è¨€æ¨¡å‹</strong>æ¥ä»è¿™äº›å€™é€‰è¯ä¸­é€‰æ‹©ä¸€ä¸ªæœ€å¯èƒ½çš„ã€‚å½“ç„¶ï¼Œå®ƒåŒæ ·ä¹Ÿå¯ä»¥ç”¨åœ¨å›¾åƒåˆ°æ–‡æœ¬çš„è¯†åˆ«ä¸­(OCR)ã€‚</p><p>ä½¿ç”¨RNNä¹‹å‰ï¼Œè¯­è¨€æ¨¡å‹ä¸»è¦æ˜¯é‡‡ç”¨N-Gramã€‚Nå¯ä»¥æ˜¯ä¸€ä¸ªè‡ªç„¶æ•°ï¼Œæ¯”å¦‚2æˆ–è€…3ã€‚å®ƒçš„å«ä¹‰æ˜¯ï¼Œå‡è®¾ä¸€ä¸ªè¯å‡ºç°çš„æ¦‚ç‡åªä¸å‰é¢Nä¸ªè¯ç›¸å…³ã€‚æˆ‘ä»¬ä»¥2-Gramä¸ºä¾‹ã€‚é¦–å…ˆï¼Œå¯¹å‰é¢çš„ä¸€å¥è¯è¿›è¡Œåˆ‡è¯ï¼š</p><blockquote><p>æˆ‘ æ˜¨å¤© ä¸Šå­¦ è¿Ÿåˆ° äº† ï¼Œè€å¸ˆ æ‰¹è¯„ äº† ____ã€‚</p></blockquote><p>å¦‚æœç”¨2-Gramè¿›è¡Œå»ºæ¨¡ï¼Œé‚£ä¹ˆç”µè„‘åœ¨é¢„æµ‹çš„æ—¶å€™ï¼Œåªä¼šçœ‹åˆ°å‰é¢çš„ã€äº†ã€ï¼Œç„¶åï¼Œç”µè„‘ä¼šåœ¨è¯­æ–™åº“ä¸­ï¼Œæœç´¢ã€äº†ã€åé¢æœ€å¯èƒ½çš„ä¸€ä¸ªè¯ã€‚ä¸ç®¡æœ€åç”µè„‘é€‰çš„æ˜¯ä¸æ˜¯ã€æˆ‘ã€ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“è¿™ä¸ªæ¨¡å‹æ˜¯ä¸é è°±çš„ï¼Œå› ä¸ºã€äº†ã€å‰é¢è¯´äº†é‚£ä¹ˆä¸€å¤§å †å®é™…ä¸Šæ˜¯æ²¡æœ‰ç”¨åˆ°çš„ã€‚å¦‚æœæ˜¯3-Gramæ¨¡å‹å‘¢ï¼Œä¼šæœç´¢ã€æ‰¹è¯„äº†ã€åé¢æœ€å¯èƒ½çš„è¯ï¼Œæ„Ÿè§‰ä¸Šæ¯”2-Gramé è°±äº†ä¸å°‘ï¼Œä½†è¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚å› ä¸ºè¿™å¥è¯æœ€å…³é”®çš„ä¿¡æ¯ã€æˆ‘ã€ï¼Œè¿œåœ¨9ä¸ªè¯ä¹‹å‰ï¼</p><p>ç°åœ¨è¯»è€…å¯èƒ½ä¼šæƒ³ï¼Œå¯ä»¥æå‡ç»§ç»­æå‡Nçš„å€¼å‘€ï¼Œæ¯”å¦‚4-Gramã€5-Gramâ€¦â€¦.ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªæƒ³æ³•æ˜¯æ²¡æœ‰å®ç”¨æ€§çš„ã€‚å› ä¸ºæˆ‘ä»¬æƒ³å¤„ç†ä»»æ„é•¿åº¦çš„å¥å­ï¼ŒNè®¾ä¸ºå¤šå°‘éƒ½ä¸åˆé€‚ï¼›å¦å¤–ï¼Œæ¨¡å‹çš„å¤§å°å’ŒNçš„å…³ç³»æ˜¯æŒ‡æ•°çº§çš„ï¼Œ4-Gramæ¨¡å‹å°±ä¼šå ç”¨æµ·é‡çš„å­˜å‚¨ç©ºé—´ã€‚</p><p>æ‰€ä»¥ï¼Œè¯¥è½®åˆ°RNNå‡ºåœºäº†ï¼ŒRNNç†è®ºä¸Šå¯ä»¥å¾€å‰çœ‹(å¾€åçœ‹)ä»»æ„å¤šä¸ªè¯ã€‚</p><h2 id="å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯å•¥"><a href="#å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯å•¥" class="headerlink" title="å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯å•¥"></a>å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯å•¥</h2><p>å¾ªç¯ç¥ç»ç½‘ç»œç§ç±»ç¹å¤šï¼Œæˆ‘ä»¬å…ˆä»æœ€ç®€å•çš„åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œå¼€å§‹å§ã€‚</p><h3 id="åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œ"><a href="#åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œ" class="headerlink" title="åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œ"></a>åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œ</h3><p>ä¸‹å›¾æ˜¯ä¸€ä¸ªç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œå¦‚ï¼Œå®ƒç”±è¾“å…¥å±‚ã€ä¸€ä¸ªéšè—å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ç»„æˆï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-479f2a7488b91671.jpg)</p><p>çº³å°¼ï¼Ÿï¼ç›¸ä¿¡ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªç©æ„çš„è¯»è€…å†…å¿ƒå’Œæˆ‘ä¸€æ ·æ˜¯å´©æºƒçš„ã€‚å› ä¸º<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å®åœ¨æ˜¯å¤ªéš¾ç”»å‡ºæ¥äº†ï¼Œç½‘ä¸Šæ‰€æœ‰å¤§ç¥ä»¬éƒ½ä¸å¾—ä¸ç”¨äº†è¿™ç§æŠ½è±¡è‰ºæœ¯æ‰‹æ³•ã€‚ä¸è¿‡ï¼Œé™ä¸‹å¿ƒæ¥ä»”ç»†çœ‹çœ‹çš„è¯ï¼Œå…¶å®ä¹Ÿæ˜¯å¾ˆå¥½ç†è§£çš„ã€‚å¦‚æœæŠŠä¸Šé¢æœ‰Wçš„é‚£ä¸ªå¸¦ç®­å¤´çš„åœˆå»æ‰ï¼Œå®ƒå°±å˜æˆäº†æœ€æ™®é€šçš„<strong>å…¨è¿æ¥ç¥ç»ç½‘ç»œ</strong>ã€‚xæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒè¡¨ç¤º<strong>è¾“å…¥å±‚</strong>çš„å€¼ï¼ˆè¿™é‡Œé¢æ²¡æœ‰ç”»å‡ºæ¥è¡¨ç¤ºç¥ç»å…ƒèŠ‚ç‚¹çš„åœ†åœˆï¼‰ï¼›sæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒè¡¨ç¤º<strong>éšè—å±‚</strong>çš„å€¼ï¼ˆè¿™é‡Œéšè—å±‚é¢ç”»äº†ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä½ ä¹Ÿå¯ä»¥æƒ³è±¡è¿™ä¸€å±‚å…¶å®æ˜¯å¤šä¸ªèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹æ•°ä¸å‘é‡sçš„ç»´åº¦ç›¸åŒï¼‰ï¼›Uæ˜¯è¾“å…¥å±‚åˆ°éšè—å±‚çš„<strong>æƒé‡çŸ©é˜µ</strong>ï¼ˆè¯»è€…å¯ä»¥å›åˆ°ç¬¬ä¸‰ç¯‡æ–‡ç« <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (3) - ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ç®—æ³•</a>ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯æ€æ ·ç”¨çŸ©é˜µæ¥è¡¨ç¤º<strong>å…¨è¿æ¥ç¥ç»ç½‘ç»œ</strong>çš„è®¡ç®—çš„ï¼‰ï¼›oä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒè¡¨ç¤º<strong>è¾“å‡ºå±‚</strong>çš„å€¼ï¼›Væ˜¯éšè—å±‚åˆ°è¾“å‡ºå±‚çš„<strong>æƒé‡çŸ©é˜µ</strong>ã€‚é‚£ä¹ˆï¼Œç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹Wæ˜¯ä»€ä¹ˆã€‚<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>çš„<strong>éšè—å±‚</strong>çš„å€¼sä¸ä»…ä»…å–å†³äºå½“å‰è¿™æ¬¡çš„è¾“å…¥xï¼Œè¿˜å–å†³äºä¸Šä¸€æ¬¡<strong>éšè—å±‚</strong>çš„å€¼sã€‚<strong>æƒé‡çŸ©é˜µ</strong> Wå°±æ˜¯<strong>éšè—å±‚</strong>ä¸Šä¸€æ¬¡çš„å€¼ä½œä¸ºè¿™ä¸€æ¬¡çš„è¾“å…¥çš„æƒé‡ã€‚</p><p>å¦‚æœæˆ‘ä»¬æŠŠä¸Šé¢çš„å›¾å±•å¼€ï¼Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ä¹Ÿå¯ä»¥ç”»æˆä¸‹é¢è¿™ä¸ªæ ·å­ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-cf18bb1f06e750a4.jpg)</p><p>ç°åœ¨çœ‹ä¸Šå»å°±æ¯”è¾ƒæ¸…æ¥šäº†ï¼Œè¿™ä¸ªç½‘ç»œåœ¨tæ—¶åˆ»æ¥æ”¶åˆ°è¾“å…¥ä¹‹åï¼Œéšè—å±‚çš„å€¼æ˜¯ï¼Œè¾“å‡ºå€¼æ˜¯ã€‚å…³é”®ä¸€ç‚¹æ˜¯ï¼Œçš„å€¼ä¸ä»…ä»…å–å†³äºï¼Œè¿˜å–å†³äºã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„å…¬å¼æ¥è¡¨ç¤º<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>çš„è®¡ç®—æ–¹æ³•ï¼š</p><p>$\begin{align} \mathrm{o}_t&amp;=g(V\mathrm{s}_t)\qquad\qquad\quad(å¼1)\\ \mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\qquad(å¼2)\\ \end{align} $</p><p><strong>å¼1</strong>æ˜¯<strong>è¾“å‡ºå±‚</strong>çš„è®¡ç®—å…¬å¼ï¼Œè¾“å‡ºå±‚æ˜¯ä¸€ä¸ª<strong>å…¨è¿æ¥å±‚</strong>ï¼Œä¹Ÿå°±æ˜¯å®ƒçš„æ¯ä¸ªèŠ‚ç‚¹éƒ½å’Œéšè—å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ç›¸è¿ã€‚Væ˜¯è¾“å‡ºå±‚çš„<strong>æƒé‡çŸ©é˜µ</strong>ï¼Œgæ˜¯<strong>æ¿€æ´»å‡½æ•°</strong>ã€‚å¼2æ˜¯éšè—å±‚çš„è®¡ç®—å…¬å¼ï¼Œå®ƒæ˜¯<strong>å¾ªç¯å±‚</strong>ã€‚Uæ˜¯è¾“å…¥xçš„æƒé‡çŸ©é˜µï¼ŒWæ˜¯ä¸Šä¸€æ¬¡çš„å€¼$\mathrm{s}_{t-1}$ä½œä¸ºè¿™ä¸€æ¬¡çš„è¾“å…¥çš„<strong>æƒé‡çŸ©é˜µ</strong>ï¼Œfæ˜¯<strong>æ¿€æ´»å‡½æ•°</strong>ã€‚</p><p>ä»ä¸Šé¢çš„å…¬å¼æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œ<strong>å¾ªç¯å±‚</strong>å’Œ<strong>å…¨è¿æ¥å±‚</strong>çš„åŒºåˆ«å°±æ˜¯<strong>å¾ªç¯å±‚</strong>å¤šäº†ä¸€ä¸ª<strong>æƒé‡çŸ©é˜µ</strong> Wã€‚</p><p>å¦‚æœåå¤æŠŠ<strong>å¼2</strong>å¸¦å…¥åˆ°<strong>å¼1</strong>ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š</p><p>$\begin {align}\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\\ &amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\ &amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))\\ &amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))\\ &amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+â€¦)))) \end{align}$</p><p>ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>çš„è¾“å‡ºå€¼ï¼Œæ˜¯å—å‰é¢å†æ¬¡è¾“å…¥å€¼$\mathrm{x}_{t}$, $\mathrm{x}_{t-1}$â€¦å½±å“çš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å¯ä»¥å¾€å‰çœ‹ä»»æ„å¤šä¸ª<strong>è¾“å…¥å€¼</strong>çš„åŸå› ã€‚</p><h3 id="åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ"><a href="#åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ" class="headerlink" title="åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ"></a>åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ</h3><p>å¯¹äº<strong>è¯­è¨€æ¨¡å‹</strong>æ¥è¯´ï¼Œå¾ˆå¤šæ—¶å€™å…‰çœ‹å‰é¢çš„è¯æ˜¯ä¸å¤Ÿçš„ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¥è¯ï¼š</p><blockquote><p>æˆ‘çš„æ‰‹æœºåäº†ï¼Œæˆ‘æ‰“ç®—____ä¸€éƒ¨æ–°æ‰‹æœºã€‚</p></blockquote><p>å¯ä»¥æƒ³è±¡ï¼Œå¦‚æœæˆ‘ä»¬åªçœ‹æ¨ªçº¿å‰é¢çš„è¯ï¼Œæ‰‹æœºåäº†ï¼Œé‚£ä¹ˆæˆ‘æ˜¯æ‰“ç®—ä¿®ä¸€ä¿®ï¼Ÿæ¢ä¸€éƒ¨æ–°çš„ï¼Ÿè¿˜æ˜¯å¤§å“­ä¸€åœºï¼Ÿè¿™äº›éƒ½æ˜¯æ— æ³•ç¡®å®šçš„ã€‚ä½†å¦‚æœæˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†æ¨ªçº¿åé¢çš„è¯æ˜¯ã€ä¸€éƒ¨æ–°æ‰‹æœºã€ï¼Œé‚£ä¹ˆï¼Œæ¨ªçº¿ä¸Šçš„è¯å¡«ã€ä¹°ã€çš„æ¦‚ç‡å°±å¤§å¾—å¤šäº†ã€‚</p><p>åœ¨ä¸Šä¸€å°èŠ‚ä¸­çš„<strong>åŸºæœ¬å¾ªç¯ç¥ç»ç½‘ç»œ</strong>æ˜¯æ— æ³•å¯¹æ­¤è¿›è¡Œå»ºæ¨¡çš„ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦<strong>åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-039a45251aa5d220.png)</p><p>å½“é‡åˆ°è¿™ç§ä»æœªæ¥ç©¿è¶Šå›æ¥çš„åœºæ™¯æ—¶ï¼Œéš¾å…å¤„äºæ‡µé€¼çš„çŠ¶æ€ã€‚ä¸è¿‡æˆ‘ä»¬è¿˜æ˜¯å¯ä»¥ç”¨å±¡è¯•ä¸çˆ½çš„è€åŠæ³•ï¼šå…ˆåˆ†æä¸€ä¸ªç‰¹æ®Šåœºæ™¯ï¼Œç„¶åå†æ€»ç»“ä¸€èˆ¬è§„å¾‹ã€‚æˆ‘ä»¬å…ˆè€ƒè™‘ä¸Šå›¾ä¸­$\mathrm{y}_2$çš„è®¡ç®—ã€‚</p><p>ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œ<strong>åŒå‘å·ç§¯ç¥ç»ç½‘ç»œ</strong>çš„éšè—å±‚è¦ä¿å­˜ä¸¤ä¸ªå€¼ï¼Œä¸€ä¸ªAå‚ä¸æ­£å‘è®¡ç®—ï¼Œå¦ä¸€ä¸ªå€¼Aâ€™å‚ä¸åå‘è®¡ç®—ã€‚æœ€ç»ˆçš„è¾“å‡ºå€¼$\mathrm{y}_2$å–å†³äº$A_2$å’Œ$A_2â€™$ã€‚å…¶è®¡ç®—æ–¹æ³•ä¸ºï¼š</p><p>$\mathrm{y}_2=g(VA_2+Vâ€™A_2â€™)$</p><p>$A_2$å’Œ$A_2â€™$åˆ™åˆ†åˆ«è®¡ç®—ï¼š</p><p>$\begin{align} A_2&amp;=f(WA_1+U\mathrm{x}_2)\\ A_2â€™&amp;=f(Wâ€™A_3â€™+Uâ€™\mathrm{x}_2)\\ \end {align}$</p><p>ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹å‡ºä¸€èˆ¬çš„è§„å¾‹ï¼šæ­£å‘è®¡ç®—æ—¶ï¼Œéšè—å±‚çš„å€¼$s_t$ä¸$s_{t-1}$æœ‰å…³ï¼›åå‘è®¡ç®—æ—¶ï¼Œéšè—å±‚çš„å€¼$s_tâ€™$ä¸$s_{t+1}â€™$æœ‰å…³ï¼›æœ€ç»ˆçš„è¾“å‡ºå–å†³äºæ­£å‘å’Œåå‘è®¡ç®—çš„<strong>åŠ å’Œ</strong>ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä»¿ç…§<strong>å¼1</strong>å’Œ<strong>å¼2</strong>ï¼Œå†™å‡ºåŒå‘å¾ªç¯ç¥ç»ç½‘ç»œçš„è®¡ç®—æ–¹æ³•ï¼š</p><p>$\begin{align} \mathrm{o}_t&amp;=g(V\mathrm{s}_t+Vâ€™\mathrm{s}_tâ€™)\\ \mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\ \mathrm{s}_tâ€™&amp;=f(Uâ€™\mathrm{x}_t+Wâ€™\mathrm{s}_{t+1}â€™)\\ \end {align}$</p><p>ä»ä¸Šé¢ä¸‰ä¸ªå…¬å¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ­£å‘è®¡ç®—å’Œåå‘è®¡ç®—<strong>ä¸å…±äº«æƒé‡</strong>ï¼Œä¹Ÿå°±æ˜¯è¯´Uå’ŒUâ€™ã€Wå’ŒWâ€™ã€Vå’ŒVâ€™éƒ½æ˜¯ä¸åŒçš„<strong>æƒé‡çŸ©é˜µ</strong>ã€‚</p><h3 id="æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ"><a href="#æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ" class="headerlink" title="æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ"></a>æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ</h3><p>å‰é¢æˆ‘ä»¬ä»‹ç»çš„<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>åªæœ‰ä¸€ä¸ªéšè—å±‚ï¼Œæˆ‘ä»¬å½“ç„¶ä¹Ÿå¯ä»¥å †å ä¸¤ä¸ªä»¥ä¸Šçš„éšè—å±‚ï¼Œè¿™æ ·å°±å¾—åˆ°äº†<strong>æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-df137de8007c3d26.png)</p><p>æˆ‘ä»¬æŠŠç¬¬iä¸ªéšè—å±‚çš„å€¼è¡¨ç¤ºä¸º$\mathrm{s}_t^{(i)}$ã€$\mathrm{s}_tâ€™^{(i)}$ï¼Œåˆ™<strong>æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ</strong>çš„è®¡ç®—æ–¹å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p><p>$\begin{align} \mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+Vâ€™^{(i)}\mathrm{s}_tâ€™^{(i)})\\ \mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})\\ \mathrm{s}_tâ€™^{(i)}&amp;=f(Uâ€™^{(i)}\mathrm{s}_tâ€™^{(i-1)}+Wâ€™^{(i)}\mathrm{s}_{t+1}â€™)\\ â€¦\\ \mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})\\ \mathrm{s}_tâ€™^{(1)}&amp;=f(Uâ€™^{(1)}\mathrm{x}_t+Wâ€™^{(1)}\mathrm{s}_{t+1}â€™)\\ \end {align}$</p><h2 id="å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒ"><a href="#å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒ" class="headerlink" title="å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒ"></a>å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒ</h2><h3 id="å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•ï¼šBPTT"><a href="#å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•ï¼šBPTT" class="headerlink" title="å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•ï¼šBPTT"></a>å¾ªç¯ç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•ï¼šBPTT</h3><p>BPTTç®—æ³•æ˜¯é’ˆå¯¹<strong>å¾ªç¯å±‚</strong>çš„è®­ç»ƒç®—æ³•ï¼Œå®ƒçš„åŸºæœ¬åŸç†å’ŒBPç®—æ³•æ˜¯ä¸€æ ·çš„ï¼Œä¹ŸåŒ…å«åŒæ ·çš„ä¸‰ä¸ªæ­¥éª¤ï¼š</p><ol><li>å‰å‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå€¼ï¼›</li><li>åå‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„<strong>è¯¯å·®é¡¹</strong>$\delta_j$å€¼ï¼Œå®ƒæ˜¯è¯¯å·®å‡½æ•°Eå¯¹ç¥ç»å…ƒjçš„<strong>åŠ æƒè¾“å…¥</strong>$net_j$çš„åå¯¼æ•°ï¼›</li><li>è®¡ç®—æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ã€‚</li></ol><p>æœ€åå†ç”¨<strong>éšæœºæ¢¯åº¦ä¸‹é™</strong>ç®—æ³•æ›´æ–°æƒé‡ã€‚</p><p>å¾ªç¯å±‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-3b20294694c3904b.png)</p><h4 id="å‰å‘è®¡ç®—"><a href="#å‰å‘è®¡ç®—" class="headerlink" title="å‰å‘è®¡ç®—"></a>å‰å‘è®¡ç®—</h4><p>ä½¿ç”¨å‰é¢çš„<strong>å¼2</strong>å¯¹å¾ªç¯å±‚è¿›è¡Œå‰å‘è®¡ç®—ï¼š</p><p>$ \mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1}) $</p><p>æ³¨æ„ï¼Œä¸Šé¢çš„$\mathrm{s}_t$ã€$\mathrm{x}_t$ã€$\mathrm{s}_{t-1}$éƒ½æ˜¯å‘é‡ï¼Œç”¨<strong>é»‘ä½“å­—æ¯</strong>è¡¨ç¤ºï¼›è€ŒUã€Væ˜¯<strong>çŸ©é˜µ</strong>ï¼Œç”¨å¤§å†™å­—æ¯è¡¨ç¤ºã€‚<strong>å‘é‡çš„ä¸‹æ ‡</strong>è¡¨ç¤º<strong>æ—¶åˆ»</strong>ï¼Œä¾‹å¦‚ï¼Œ${s}_t$è¡¨ç¤ºåœ¨tæ—¶åˆ»å‘é‡sçš„å€¼ã€‚</p><p>æˆ‘ä»¬å‡è®¾è¾“å…¥å‘é‡xçš„ç»´åº¦æ˜¯mï¼Œè¾“å‡ºå‘é‡sçš„ç»´åº¦æ˜¯nï¼Œåˆ™çŸ©é˜µUçš„ç»´åº¦æ˜¯$n\times m$ï¼ŒçŸ©é˜µWçš„ç»´åº¦æ˜¯$n\times n$ã€‚ä¸‹é¢æ˜¯ä¸Šå¼å±•å¼€æˆçŸ©é˜µçš„æ ·å­ï¼Œçœ‹èµ·æ¥æ›´ç›´è§‚ä¸€äº›ï¼š</p><p>$\begin{align} \begin{bmatrix} s_1^t\\ s_2^t\\ .\.\\ s_n^t\\ \end{bmatrix}=f( \begin{bmatrix} u_{11} u_{12} â€¦ u_{1m}\\ u_{21} u_{22} â€¦ u_{2m}\\ .\.\\ u_{n1} u_{n2} â€¦ u_{nm}\\ \end{bmatrix} \begin{bmatrix} x_1\\ x_2\\ .\.\\ x_m\\ \end{bmatrix}+ \begin{bmatrix} w_{11} w_{12} â€¦ w_{1n}\\ w_{21} w_{22} â€¦ w_{2n}\\ .\.\\ w_{n1} w_{n2} â€¦ w_{nn}\\ \end{bmatrix} \begin{bmatrix} s_1^{t-1}\\ s_2^{t-1}\\ .\.\\ s_n^{t-1}\\ \end{bmatrix}) \end {align}$</p><p>åœ¨è¿™é‡Œæˆ‘ä»¬ç”¨<strong>æ‰‹å†™ä½“å­—æ¯</strong>è¡¨ç¤ºå‘é‡çš„ä¸€ä¸ª<strong>å…ƒç´ </strong>ï¼Œå®ƒçš„ä¸‹æ ‡è¡¨ç¤ºå®ƒæ˜¯è¿™ä¸ªå‘é‡çš„ç¬¬å‡ ä¸ªå…ƒç´ ï¼Œå®ƒçš„ä¸Šæ ‡è¡¨ç¤ºç¬¬å‡ ä¸ª<strong>æ—¶åˆ»</strong>ã€‚ä¾‹å¦‚ï¼Œ$s_j^t$è¡¨ç¤ºå‘é‡sçš„ç¬¬jä¸ªå…ƒç´ åœ¨tæ—¶åˆ»çš„å€¼ã€‚$u_{ji}$è¡¨ç¤º<strong>è¾“å…¥å±‚</strong>ç¬¬iä¸ªç¥ç»å…ƒåˆ°<strong>å¾ªç¯å±‚</strong>ç¬¬jä¸ªç¥ç»å…ƒçš„æƒé‡ã€‚$w_{ji}$è¡¨ç¤º<strong>å¾ªç¯å±‚</strong>ç¬¬t-1æ—¶åˆ»çš„ç¬¬iä¸ªç¥ç»å…ƒåˆ°<strong>å¾ªç¯å±‚</strong>ç¬¬tä¸ªæ—¶åˆ»çš„ç¬¬jä¸ªç¥ç»å…ƒçš„æƒé‡ã€‚</p><h4 id="è¯¯å·®é¡¹çš„è®¡ç®—"><a href="#è¯¯å·®é¡¹çš„è®¡ç®—" class="headerlink" title="è¯¯å·®é¡¹çš„è®¡ç®—"></a>è¯¯å·®é¡¹çš„è®¡ç®—</h4><p>BTPPç®—æ³•å°†ç¬¬lå±‚tæ—¶åˆ»çš„<strong>è¯¯å·®é¡¹</strong>$\delta_t^l$å€¼æ²¿ä¸¤ä¸ªæ–¹å‘ä¼ æ’­ï¼Œä¸€ä¸ªæ–¹å‘æ˜¯å…¶ä¼ é€’åˆ°ä¸Šä¸€å±‚ç½‘ç»œï¼Œå¾—åˆ°$\delta_t^{l-1}$ï¼Œè¿™éƒ¨åˆ†åªå’Œæƒé‡çŸ©é˜µUæœ‰å…³ï¼›å¦ä¸€ä¸ªæ˜¯æ–¹å‘æ˜¯å°†å…¶æ²¿æ—¶é—´çº¿ä¼ é€’åˆ°åˆå§‹$t_1$æ—¶åˆ»ï¼Œå¾—åˆ°$\delta_1^l$ï¼Œè¿™éƒ¨åˆ†åªå’Œæƒé‡çŸ©é˜µWæœ‰å…³ã€‚</p><p>æˆ‘ä»¬ç”¨å‘é‡$\mathrm{net}_t$è¡¨ç¤ºç¥ç»å…ƒåœ¨tæ—¶åˆ»çš„<strong>åŠ æƒè¾“å…¥</strong>ï¼Œå› ä¸ºï¼š</p><p>$\begin{align} \mathrm{net}_t&amp;=U\mathrm{x}_t+W\mathrm{s}_{t-1}\\ \mathrm{s}_{t-1}&amp;=f(\mathrm{net}_{t-1})\\ \end {align} $</p><p>å› æ­¤ï¼š</p><script type="math/tex; mode=display">\begin{align} \frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\ \end {align}</script><p>æˆ‘ä»¬ç”¨aè¡¨ç¤ºåˆ—å‘é‡ï¼Œç”¨è¡¨ç¤º$\mathrm{a}^T$è¡Œå‘é‡ã€‚ä¸Šå¼çš„ç¬¬ä¸€é¡¹æ˜¯å‘é‡å‡½æ•°å¯¹å‘é‡æ±‚å¯¼ï¼Œå…¶ç»“æœä¸ºJacobiançŸ©é˜µï¼š</p><p>$\begin{align} \frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}&amp;= \begin{bmatrix} \frac{\partial{net_1^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_1^t}}{\partial{s_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{net_1^t}}{\partial{s_n^{t-1}}}\\ \frac{\partial{net_2^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_2^t}}{\partial{s_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{net_2^t}}{\partial{s_n^{t-1}}}\\ &amp;.\\&amp;.\\ \frac{\partial{net_n^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{net_n^t}}{\partial{s_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{net_n^t}}{\partial{s_n^{t-1}}}\\ \end{bmatrix}\\ &amp;=\begin{bmatrix} w_{11} &amp; w_{12} &amp; â€¦ &amp; w_{1n}\\ w_{21} &amp; w_{22} &amp; â€¦ &amp; w_{2n}\\ &amp;.\\&amp;.\\ w_{n1} &amp; w_{n2} &amp; â€¦ &amp; w_{nn}\\ \end{bmatrix}\\ &amp;=W \end {align}$</p><p>åŒç†ï¼Œä¸Šå¼ç¬¬äºŒé¡¹ä¹Ÿæ˜¯ä¸€ä¸ªJacobiançŸ©é˜µï¼š</p><p>$\begin{align} \frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}&amp;= \begin{bmatrix} \frac{\partial{s_1^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_1^{t-1}}}{\partial{net_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{s_1^{t-1}}}{\partial{net_n^{t-1}}}\\ \frac{\partial{s_2^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_2^{t-1}}}{\partial{net_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{s_2^{t-1}}}{\partial{net_n^{t-1}}}\\ &amp;.\\&amp;.\\ \frac{\partial{s_n^{t-1}}}{\partial{net_1^{t-1}}}&amp; \frac{\partial{s_n^{t-1}}}{\partial{net_2^{t-1}}}&amp; â€¦&amp;  \frac{\partial{s_n^{t-1}}}{\partial{net_n^{t-1}}}\\ \end{bmatrix}\\ &amp;=\begin{bmatrix} fâ€™(net_1^{t-1}) &amp; 0 &amp; â€¦ &amp; 0\\ 0 &amp; fâ€™(net_2^{t-1}) &amp; â€¦ &amp; 0\\ &amp;.\\&amp;.\\ 0 &amp; 0 &amp; â€¦ &amp; fâ€™(net_n^{t-1})\\ \end{bmatrix}\\ &amp;=diag[fâ€™(\mathrm{net}_{t-1})] \end {align}$</p><p>å…¶ä¸­ï¼Œdiag[a]è¡¨ç¤ºæ ¹æ®å‘é‡aåˆ›å»ºä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå³</p><p>$diag(\mathrm{a})=\begin{bmatrix} a_1 &amp; 0 &amp; â€¦ &amp; 0\\ 0 &amp; a_2 &amp; â€¦ &amp; 0\\ &amp;.\\&amp;.\\ 0 &amp; 0 &amp; â€¦ &amp; a_n\\ \end{bmatrix}\\ $ </p><p>æœ€åï¼Œå°†ä¸¤é¡¹åˆåœ¨ä¸€èµ·ï¼Œå¯å¾—ï¼š</p><p>$\begin{align} \frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&amp;=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\ &amp;=Wdiag[fâ€™(\mathrm{net}_{t-1})]\\ &amp;=\begin{bmatrix} w_{11}fâ€™(net_1^{t-1}) &amp; w_{12}fâ€™(net_2^{t-1}) &amp; â€¦ &amp; w_{1n}f(net_n^{t-1})\\ w_{21}fâ€™(net_1^{t-1}) &amp; w_{22} fâ€™(net_2^{t-1}) &amp; â€¦ &amp; w_{2n}f(net_n^{t-1})\\ &amp;.\\&amp;.\\ w_{n1}fâ€™(net_1^{t-1}) &amp; w_{n2} fâ€™(net_2^{t-1}) &amp; â€¦ &amp; w_{nn} fâ€™(net_n^{t-1})\\ \end{bmatrix}\\ \end {align}$</p><p>ä¸Šå¼æè¿°äº†å°†æ²¿æ—¶é—´å¾€å‰ä¼ é€’ä¸€ä¸ªæ—¶åˆ»çš„è§„å¾‹ï¼Œæœ‰äº†è¿™ä¸ªè§„å¾‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ±‚å¾—ä»»æ„æ—¶åˆ»kçš„<strong>è¯¯å·®é¡¹</strong>$\delta_k$ï¼š</p><p>$\begin{align} \delta_k^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_k}}\\ =&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_k}}\\ =&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{\mathrm{net}_{t-2}}}â€¦\frac{\partial{\mathrm{net}_{k+1}}}{\partial{\mathrm{net}_{k}}}\\ =&amp;Wdiag[fâ€™(\mathrm{net}_{t-1})] Wdiag[fâ€™(\mathrm{net}_{t-2})] â€¦ Wdiag[fâ€™(\mathrm{net}_{k})] \delta_t^l\\ =&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[fâ€™(\mathrm{net}_{i})]\qquad(å¼3) \end {align}$</p><p><strong>å¼3</strong>å°±æ˜¯å°†è¯¯å·®é¡¹æ²¿æ—¶é—´åå‘ä¼ æ’­çš„ç®—æ³•ã€‚</p><p><strong>å¾ªç¯å±‚</strong>å°†<strong>è¯¯å·®é¡¹</strong>åå‘ä¼ é€’åˆ°ä¸Šä¸€å±‚ç½‘ç»œï¼Œä¸æ™®é€šçš„<strong>å…¨è¿æ¥å±‚</strong>æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œè¿™åœ¨å‰é¢çš„æ–‡ç« <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (3) - ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ç®—æ³•</a>ä¸­å·²ç»è¯¦ç»†è®²è¿‡äº†ï¼Œåœ¨æ­¤ä»…ç®€è¦æè¿°ä¸€ä¸‹ã€‚</p><p><strong>å¾ªç¯å±‚</strong>çš„<strong>åŠ æƒè¾“å…¥</strong>$\mathrm{net}^l$ä¸ä¸Šä¸€å±‚çš„<strong>åŠ æƒè¾“å…¥</strong>$\mathrm{net}^{l-1}$å…³ç³»å¦‚ä¸‹ï¼š</p><p>$\begin{align} \mathrm{net}_t^l=&amp;U\mathrm{a}_t^{l-1}+W\mathrm{s}_{t-1}\\ \mathrm{a}_t^{l-1}=&amp;f^{l-1}(\mathrm{net}_t^{l-1}) \end {align}$</p><p>ä¸Šå¼ä¸­$\mathrm{net}_t^l$æ˜¯ç¬¬$l$å±‚ç¥ç»å…ƒçš„<strong>åŠ æƒè¾“å…¥</strong>(å‡è®¾ç¬¬lå±‚æ˜¯<strong>å¾ªç¯å±‚</strong>)ï¼›$\mathrm{net}_t^{l-1}$æ˜¯ç¬¬$l-1$å±‚ç¥ç»å…ƒçš„<strong>åŠ æƒè¾“å…¥</strong>ï¼›$\mathrm{a}_t^{l-1}$æ˜¯ç¬¬$l-1$å±‚ç¥ç»å…ƒçš„è¾“å‡ºï¼›$f^{l-1}$æ˜¯ç¬¬$l-1$å±‚çš„<strong>æ¿€æ´»å‡½æ•°</strong>ã€‚</p><p>$\begin{align} \frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}=&amp;\frac{\partial{\mathrm{net}^l}}{\partial{\mathrm{a}_t^{l-1}}}\frac{\partial{\mathrm{a}_t^{l-1}}}{\partial{\mathrm{net}_t^{l-1}}}\\ =&amp;Udiag[fâ€™^{l-1}(\mathrm{net}_t^{l-1})] \end {align}$</p><p>æ‰€ä»¥ï¼Œ</p><p>$\begin{align} (\delta_t^{l-1})^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^{l-1}}}\\ =&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^l}}\frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}\\ =&amp;(\delta_t^l)^TUdiag[fâ€™^{l-1}(\mathrm{net}_t^{l-1})]\qquad(å¼4) \end {align}$</p><p><strong>å¼4</strong>å°±æ˜¯å°†è¯¯å·®é¡¹ä¼ é€’åˆ°ä¸Šä¸€å±‚ç®—æ³•ã€‚</p><h4 id="æƒé‡æ¢¯åº¦çš„è®¡ç®—"><a href="#æƒé‡æ¢¯åº¦çš„è®¡ç®—" class="headerlink" title="æƒé‡æ¢¯åº¦çš„è®¡ç®—"></a>æƒé‡æ¢¯åº¦çš„è®¡ç®—</h4><p>ç°åœ¨ï¼Œæˆ‘ä»¬ç»ˆäºæ¥åˆ°äº†BPTTç®—æ³•çš„æœ€åä¸€æ­¥ï¼šè®¡ç®—æ¯ä¸ªæƒé‡çš„æ¢¯åº¦$\frac{\partial{E}}{\partial{W}}$ã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—è¯¯å·®å‡½æ•°Eå¯¹æƒé‡çŸ©é˜µWçš„æ¢¯åº¦ã€‚</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-f7d034c8f05812f7.png)</p><p>ä¸Šå›¾å±•ç¤ºäº†æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨å‰ä¸¤æ­¥ä¸­å·²ç»è®¡ç®—å¾—åˆ°çš„é‡ï¼ŒåŒ…æ‹¬æ¯ä¸ªæ—¶åˆ»t <strong>å¾ªç¯å±‚</strong>çš„è¾“å‡ºå€¼$s_t$ï¼Œä»¥åŠè¯¯å·®é¡¹$\sigma_t$ã€‚</p><p>å›å¿†ä¸€ä¸‹æˆ‘ä»¬åœ¨æ–‡ç« <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (3) - ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ç®—æ³•</a>ä»‹ç»çš„å…¨è¿æ¥ç½‘ç»œçš„æƒé‡æ¢¯åº¦è®¡ç®—ç®—æ³•ï¼šåªè¦çŸ¥é“äº†ä»»æ„ä¸€ä¸ªæ—¶åˆ»çš„<strong>è¯¯å·®é¡¹</strong>$\sigma_t$ï¼Œä»¥åŠä¸Šä¸€ä¸ªæ—¶åˆ»å¾ªç¯å±‚çš„è¾“å‡ºå€¼$s_{t-1}$ï¼Œå°±å¯ä»¥æŒ‰ç…§ä¸‹é¢çš„å…¬å¼æ±‚å‡ºæƒé‡çŸ©é˜µåœ¨$t$æ—¶åˆ»çš„æ¢¯åº¦$\nabla_{Wt}E$ï¼š</p><p>$\begin{align}\nabla_{W_t}E=\begin{bmatrix} \delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; â€¦ &amp;  \delta_1^ts_n^{t-1}\\ \delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; â€¦ &amp;  \delta_2^ts_n^{t-1}\\ .\.\\ \delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; â€¦ &amp;  \delta_n^ts_n^{t-1}\\ \end{bmatrix}\qquad(å¼5)\end {align}$</p><p>åœ¨<strong>å¼5</strong>ä¸­ï¼Œ$\delta_i^t$è¡¨ç¤ºtæ—¶åˆ»<strong>è¯¯å·®é¡¹</strong>å‘é‡çš„ç¬¬$i$ä¸ªåˆ†é‡ï¼›$s_i^{t-1}$è¡¨ç¤º$t-1$æ—¶åˆ»<strong>å¾ªç¯å±‚</strong>ç¬¬$i$ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå€¼ã€‚</p><p>æˆ‘ä»¬ä¸‹é¢å¯ä»¥ç®€å•æ¨å¯¼ä¸€ä¸‹<strong>å¼5</strong>ã€‚</p><p>æˆ‘ä»¬çŸ¥é“ï¼š</p><p>$\begin{align} \mathrm{net}_t=&amp;U\mathrm{x}_t+W\mathrm{s}_{t-1}\\ \begin{bmatrix} net_1^t\\ net_2^t\\ .\.\\ net_n^t\\ \end{bmatrix}=&amp;U\mathrm{x}_t+ \begin{bmatrix} w_{11} &amp; w_{12} &amp; â€¦ &amp; w_{1n}\\ w_{21} &amp; w_{22} &amp; â€¦ &amp; w_{2n}\\ .\.\\ w_{n1} &amp; w_{n2} &amp; â€¦ &amp; w_{nn}\\ \end{bmatrix} \begin{bmatrix} s_1^{t-1}\\ s_2^{t-1}\\ .\.\\ s_n^{t-1}\\ \end{bmatrix}\\ =&amp;U\mathrm{x}_t+ \begin{bmatrix} w_{11}s_1^{t-1}+w_{12}s_2^{t-1}â€¦w_{1n}s_n^{t-1}\\ w_{21}s_1^{t-1}+w_{22}s_2^{t-1}â€¦w_{2n}s_n^{t-1}\\ .\.\\ w_{n1}s_1^{t-1}+w_{n2}s_2^{t-1}â€¦w_{nn}s_n^{t-1}\\ \end{bmatrix}\\ \end {align}$</p><p>å› ä¸ºå¯¹Wæ±‚å¯¼ä¸$U\mathrm{x}_t$æ— å…³ï¼Œæˆ‘ä»¬ä¸å†è€ƒè™‘ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è€ƒè™‘å¯¹æƒé‡é¡¹$w_{ji}$æ±‚å¯¼ã€‚é€šè¿‡è§‚å¯Ÿä¸Šå¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°$w_{ji}$åªä¸$net_j^t$æœ‰å…³ï¼Œæ‰€ä»¥ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{w_{ji}}}=&amp;\frac{\partial{E}}{\partial{net_j^t}}\frac{\partial{net_j^t}}{\partial{w_{ji}}}\\ =&amp;\delta_j^ts_i^{t-1} \end {align}$</p><p>æŒ‰ç…§ä¸Šé¢çš„è§„å¾‹å°±å¯ä»¥ç”Ÿæˆ<strong>å¼5</strong>é‡Œé¢çš„çŸ©é˜µã€‚</p><p>æˆ‘ä»¬å·²ç»æ±‚å¾—äº†æƒé‡çŸ©é˜µWåœ¨tæ—¶åˆ»çš„æ¢¯åº¦$\nabla_{Wt}E$ï¼Œæœ€ç»ˆçš„æ¢¯åº¦$\nabla_WE$æ˜¯å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦<strong>ä¹‹å’Œ</strong>ï¼š</p><p>$\begin{align} \nabla_WE=&amp;\sum_{i=1}^t\nabla_{W_i}E\\ =&amp;\begin{bmatrix} \delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; â€¦ &amp;  \delta_1^ts_n^{t-1}\\ \delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; â€¦ &amp;  \delta_2^ts_n^{t-1}\\ .\.\\ \delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; â€¦ &amp;  \delta_n^ts_n^{t-1}\\ \end{bmatrix} +â€¦+ \begin{bmatrix} \delta_1^1s_1^0 &amp; \delta_1^1s_2^0 &amp; â€¦ &amp;  \delta_1^1s_n^0\\ \delta_2^1s_1^0 &amp; \delta_2^1s_2^0 &amp; â€¦ &amp;  \delta_2^1s_n^0\\ .\.\\ \delta_n^1s_1^0 &amp; \delta_n^1s_2^0 &amp; â€¦ &amp;  \delta_n^1s_n^0\\ \end{bmatrix}\qquad(å¼6) \end{align} $</p><p><strong>å¼6</strong>å°±æ˜¯è®¡ç®—<strong>å¾ªç¯å±‚</strong>æƒé‡çŸ©é˜µWçš„æ¢¯åº¦çš„å…¬å¼ã€‚</p><pre><code>----------æ•°å­¦å…¬å¼è¶…é«˜èƒ½é¢„è­¦----------</code></pre><p>å‰é¢å·²ç»ä»‹ç»äº†$\nabla_WE$çš„è®¡ç®—æ–¹æ³•ï¼Œçœ‹ä¸Šå»è¿˜æ˜¯æ¯”è¾ƒç›´è§‚çš„ã€‚ç„¶è€Œï¼Œè¯»è€…ä¹Ÿè®¸ä¼šå›°æƒ‘ï¼Œä¸ºä»€ä¹ˆæœ€ç»ˆçš„æ¢¯åº¦æ˜¯å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦<strong>ä¹‹å’Œ</strong>å‘¢ï¼Ÿæˆ‘ä»¬å‰é¢åªæ˜¯ç›´æ¥ç”¨äº†è¿™ä¸ªç»“è®ºï¼Œå®é™…ä¸Šè¿™é‡Œé¢æ˜¯æœ‰é“ç†çš„ï¼Œåªæ˜¯è¿™ä¸ªæ•°å­¦æ¨å¯¼æ¯”è¾ƒç»•è„‘å­ã€‚æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ä»”ç»†é˜…è¯»æ¥ä¸‹æ¥è¿™ä¸€æ®µï¼Œå®ƒç”¨åˆ°äº†çŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼ã€å¼ é‡ä¸å‘é‡ç›¸ä¹˜è¿ç®—çš„ä¸€äº›æ³•åˆ™ã€‚</p><p>æˆ‘ä»¬è¿˜æ˜¯ä»è¿™ä¸ªå¼å­å¼€å§‹ï¼š</p><p>$ \begin {align}\mathrm{net}_t=U\mathrm{x}_t+Wf(\mathrm{net}_{t-1}) \end {align} $</p><p>å› ä¸º$U\mathrm{x}_t$ä¸Wå®Œå…¨æ— å…³ï¼Œæˆ‘ä»¬æŠŠå®ƒçœ‹åšå¸¸é‡ã€‚ç°åœ¨ï¼Œè€ƒè™‘ç¬¬ä¸€ä¸ªå¼å­åŠ å·å³è¾¹çš„éƒ¨åˆ†ï¼Œå› ä¸ºWå’Œ$f(\mathrm{net}_{t-1})$éƒ½æ˜¯Wçš„å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬è¦ç”¨åˆ°å¤§å­¦é‡Œé¢éƒ½å­¦è¿‡çš„å¯¼æ•°ä¹˜æ³•è¿ç®—ï¼š</p><p>$ \begin{align}(uv)â€™=uâ€™v+uvâ€™ \end {align}$</p><p>å› æ­¤ï¼Œä¸Šé¢ç¬¬ä¸€ä¸ªå¼å­å†™æˆï¼š</p><p>$ \begin{align} \frac{\partial{\mathrm{net}_t}}{\partial{W}}=\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+W\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}\\ \end{align} $</p><p>æˆ‘ä»¬æœ€ç»ˆéœ€è¦è®¡ç®—çš„æ˜¯$\nabla_WE$ï¼š</p><p>$\begin{align} \nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\ =&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\ =&amp;\delta_t^T\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+ \delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}\qquad(å¼7)\\ \end{align} $</p><p>æˆ‘ä»¬å…ˆè®¡ç®—<strong>å¼7</strong>åŠ å·å·¦è¾¹çš„éƒ¨åˆ†ã€‚$\frac{\partial{W}}{\partial{W}}$æ˜¯<strong>çŸ©é˜µå¯¹çŸ©é˜µæ±‚å¯¼</strong>ï¼Œå…¶ç»“æœæ˜¯ä¸€ä¸ªå››ç»´<strong>å¼ é‡(tensor)</strong>ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p><p>$\begin{align} \frac{\partial{W}}{\partial{W}}=&amp; \begin{bmatrix} \frac{\partial{w_{11}}}{\partial{W}} &amp; \frac{\partial{w_{12}}}{\partial{W}} &amp; â€¦ &amp; \frac{\partial{w_{1n}}}{\partial{W}}\\ \frac{\partial{w_{21}}}{\partial{W}} &amp; \frac{\partial{w_{22}}}{\partial{W}} &amp; â€¦ &amp; \frac{\partial{w_{2n}}}{\partial{W}}\\ .\.\\ \frac{\partial{w_{n1}}}{\partial{W}} &amp; \frac{\partial{w_{n2}}}{\partial{W}} &amp; â€¦ &amp; \frac{\partial{w_{nn}}}{\partial{W}}\\ \end{bmatrix}\\ =&amp; \begin{bmatrix} \begin{bmatrix} \frac{\partial{w_{11}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{12}}} &amp; â€¦ &amp; \frac{\partial{w_{11}}}{\partial{_{1n}}}\\ \frac{\partial{w_{11}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{22}}} &amp; â€¦ &amp; \frac{\partial{w_{11}}}{\partial{_{2n}}}\\ .\.\\ \frac{\partial{w_{11}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{n2}}} &amp; â€¦ &amp; \frac{\partial{w_{11}}}{\partial{_{nn}}}\\ \end{bmatrix} &amp; \begin{bmatrix} \frac{\partial{w_{12}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{12}}} &amp; â€¦ &amp; \frac{\partial{w_{12}}}{\partial{_{1n}}}\\ \frac{\partial{w_{12}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{22}}} &amp; â€¦ &amp; \frac{\partial{w_{12}}}{\partial{_{2n}}}\\ .\.\\ \frac{\partial{w_{12}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{n2}}} &amp; â€¦ &amp; \frac{\partial{w_{12}}}{\partial{_{nn}}}\\ \end{bmatrix}&amp;â€¦\\ .\.\\ \end{bmatrix}\\ =&amp; \begin{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; â€¦ &amp; 0\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ .\.\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ \end{bmatrix} &amp; \begin{bmatrix} 0 &amp; 1 &amp; â€¦ &amp; 0\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ .\.\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ \end{bmatrix}&amp;â€¦\\ .\.\\ \end{bmatrix}\\ \end{align} $</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çŸ¥é“$s_{t-1}=f({\mathrm{net}_{t-1}})$ï¼Œå®ƒæ˜¯ä¸€ä¸ª<strong>åˆ—å‘é‡</strong>ã€‚æˆ‘ä»¬è®©ä¸Šé¢çš„å››ç»´å¼ é‡ä¸è¿™ä¸ªå‘é‡ç›¸ä¹˜ï¼Œå¾—åˆ°äº†ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå†å·¦ä¹˜è¡Œå‘é‡$\delta_t^T$ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªçŸ©é˜µï¼š</p><p>$\begin{align} \delta_t^T\frac{\partial{W}}{\partial{W}}f({\mathrm{net}_{t-1}})=&amp; \delta_t^T\frac{\partial{W}}{\partial{W}}{\mathrm{s}_{t-1}}\\ =&amp;\delta_t^T \begin{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; â€¦ &amp; 0\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ .\.\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ \end{bmatrix} &amp; \begin{bmatrix} 0 &amp; 1 &amp; â€¦ &amp; 0\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ .\.\\ 0 &amp; 0 &amp; â€¦ &amp; 0\\ \end{bmatrix}&amp;â€¦\\ .\.\\ \end{bmatrix} \begin{bmatrix} s_1^{t-1}\\ s_2^{t-1}\\ .\.\\ s_n^{t-1}\\ \end{bmatrix}\\ =&amp;\delta_t^T \begin{bmatrix} \begin{bmatrix} s_1^{t-1}\\ 0\\ .\.\\ 0\\ \end{bmatrix} &amp; \begin{bmatrix} s_2^{t-1}\\ 0\\ .\.\\ 0\\ \end{bmatrix}&amp;â€¦\\ .\.\\ \end{bmatrix}\\ =&amp; \begin{bmatrix} \delta_1^t &amp; \delta_2^t &amp; â€¦ &amp;\delta_n^t \end{bmatrix} \begin{bmatrix} \begin{bmatrix} s_1^{t-1}\\ 0\\ .\.\\ 0\\ \end{bmatrix} &amp; \begin{bmatrix} s_2^{t-1}\\ 0\\ .\.\\ 0\\ \end{bmatrix}&amp;â€¦\\ .\.\\ \end{bmatrix}\\ =&amp; \begin{bmatrix} \delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; â€¦ &amp;  \delta_1^ts_n^{t-1}\\ \delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; â€¦ &amp;  \delta_2^ts_n^{t-1}\\ .\.\\ \delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; â€¦ &amp;  \delta_n^ts_n^{t-1}\\ \end{bmatrix}\\ =&amp;\nabla_{Wt}E \end{align} $</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—<strong>å¼7</strong>åŠ å·å³è¾¹çš„éƒ¨åˆ†ï¼š</p><p>$\begin{align} \delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}=&amp; \delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\ =&amp;\delta_t^TWfâ€™(\mathrm{net}_{t-1})\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\ =&amp;\delta_t^T\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\ =&amp;\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\ \end{align} $</p><p>äºæ˜¯ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹é€’æ¨å…¬å¼ï¼š</p><p>$\begin{align} \nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\ =&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\ =&amp;\nabla_{Wt}E+\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\ =&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+\delta_{t-2}^T\frac{\partial{\mathrm{net}_{t-2}}}{\partial{W}}\\ =&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+â€¦+\nabla_{W1}E\\ =&amp;\sum_{k=1}^t\nabla_{Wk}E \end{align} $</p><p>è¿™æ ·ï¼Œæˆ‘ä»¬å°±è¯æ˜äº†ï¼šæœ€ç»ˆçš„æ¢¯åº¦æ˜¯å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦$\nabla_WE$ä¹‹å’Œã€‚</p><pre><code>----------æ•°å­¦å…¬å¼è¶…é«˜èƒ½é¢„è­¦è§£é™¤----------</code></pre><p>åŒæƒé‡çŸ©é˜µWç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æƒé‡çŸ©é˜µUçš„è®¡ç®—æ–¹æ³•ã€‚</p><p>$\begin{align}\nabla_{U_t}E=\begin{bmatrix} \delta_1^tx_1^t &amp; \delta_1^tx_2^t &amp; â€¦ &amp;  \delta_1^tx_m^t\\ \delta_2^tx_1^t &amp; \delta_2^tx_2^t &amp; â€¦ &amp;  \delta_2^tx_m^t\\ .\.\\ \delta_n^tx_1^t &amp; \delta_n^tx_2^t &amp; â€¦ &amp;  \delta_n^tx_m^t\\ \end{bmatrix}\qquad(å¼8) \end{align} $</p><p><strong>å¼8</strong>æ˜¯è¯¯å·®å‡½æ•°åœ¨tæ—¶åˆ»å¯¹æƒé‡çŸ©é˜µUçš„æ¢¯åº¦ã€‚å’Œæƒé‡çŸ©é˜µWä¸€æ ·ï¼Œæœ€ç»ˆçš„æ¢¯åº¦ä¹Ÿæ˜¯å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦ä¹‹å’Œï¼š</p><p>$\begin{align}\nabla_UE=\sum_{i=1}^t\nabla_{U_i}E \end{align} $</p><p>å…·ä½“çš„è¯æ˜è¿™é‡Œå°±ä¸å†èµ˜è¿°äº†ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥ç»ƒä¹ æ¨å¯¼ä¸€ä¸‹ã€‚</p><h3 id="RNNçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜"><a href="#RNNçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜" class="headerlink" title="RNNçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜"></a>RNNçš„æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜</h3><p>ä¸å¹¸çš„æ˜¯ï¼Œå®è·µä¸­å‰é¢ä»‹ç»çš„å‡ ç§RNNså¹¶ä¸èƒ½å¾ˆå¥½çš„å¤„ç†è¾ƒé•¿çš„åºåˆ—ã€‚ä¸€ä¸ªä¸»è¦çš„åŸå› æ˜¯ï¼ŒRNNåœ¨è®­ç»ƒä¸­å¾ˆå®¹æ˜“å‘ç”Ÿ<strong>æ¢¯åº¦çˆ†ç‚¸</strong>å’Œ<strong>æ¢¯åº¦æ¶ˆå¤±</strong>ï¼Œè¿™å¯¼è‡´è®­ç»ƒæ—¶æ¢¯åº¦ä¸èƒ½åœ¨è¾ƒé•¿åºåˆ—ä¸­ä¸€ç›´ä¼ é€’ä¸‹å»ï¼Œä»è€Œä½¿RNNæ— æ³•æ•æ‰åˆ°é•¿è·ç¦»çš„å½±å“ã€‚</p><p>ä¸ºä»€ä¹ˆRNNä¼šäº§ç”Ÿæ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜å‘¢ï¼Ÿæˆ‘ä»¬æ¥ä¸‹æ¥å°†è¯¦ç»†åˆ†æä¸€ä¸‹åŸå› ã€‚æˆ‘ä»¬æ ¹æ®<strong>å¼3</strong>å¯å¾—ï¼š</p><p>$\begin{align} \delta_k^T=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[fâ€™(\mathrm{net}_{i})]\\ |\delta_k^T|\leqslant&amp;|\delta_t^T|\prod_{i=k}^{t-1}|W||diag[fâ€™(\mathrm{net}_{i})]|\\ \leqslant&amp;|\delta_t^T|(\beta_W\beta_f)^{t-k} \end{align} $</p><p>ä¸Šå¼çš„$\beta$å®šä¹‰ä¸ºçŸ©é˜µçš„æ¨¡çš„ä¸Šç•Œã€‚å› ä¸ºä¸Šå¼æ˜¯ä¸€ä¸ªæŒ‡æ•°å‡½æ•°ï¼Œå¦‚æœt-kå¾ˆå¤§çš„è¯ï¼ˆä¹Ÿå°±æ˜¯å‘å‰çœ‹å¾ˆè¿œçš„æ—¶å€™ï¼‰ï¼Œä¼šå¯¼è‡´å¯¹åº”çš„<strong>è¯¯å·®é¡¹</strong>çš„å€¼å¢é•¿æˆ–ç¼©å°çš„éå¸¸å¿«ï¼Œè¿™æ ·å°±ä¼šå¯¼è‡´ç›¸åº”çš„<strong>æ¢¯åº¦çˆ†ç‚¸</strong>å’Œ<strong>æ¢¯åº¦æ¶ˆå¤±</strong>é—®é¢˜ï¼ˆå–å†³äºå¤§$\beta$äº1è¿˜æ˜¯å°äº1ï¼‰ã€‚</p><p>é€šå¸¸æ¥è¯´ï¼Œ<strong>æ¢¯åº¦çˆ†ç‚¸</strong>æ›´å®¹æ˜“å¤„ç†ä¸€äº›ã€‚å› ä¸ºæ¢¯åº¦çˆ†ç‚¸çš„æ—¶å€™ï¼Œæˆ‘ä»¬çš„ç¨‹åºä¼šæ”¶åˆ°NaNé”™è¯¯ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è®¾ç½®ä¸€ä¸ªæ¢¯åº¦é˜ˆå€¼ï¼Œå½“æ¢¯åº¦è¶…è¿‡è¿™ä¸ªé˜ˆå€¼çš„æ—¶å€™å¯ä»¥ç›´æ¥æˆªå–ã€‚</p><p><strong>æ¢¯åº¦æ¶ˆå¤±</strong>æ›´éš¾æ£€æµ‹ï¼Œè€Œä¸”ä¹Ÿæ›´éš¾å¤„ç†ä¸€äº›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸‰ç§æ–¹æ³•åº”å¯¹æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š</p><ol><li>åˆç†çš„åˆå§‹åŒ–æƒé‡å€¼ã€‚åˆå§‹åŒ–æƒé‡ï¼Œä½¿æ¯ä¸ªç¥ç»å…ƒå°½å¯èƒ½ä¸è¦å–æå¤§æˆ–æå°å€¼ï¼Œä»¥èº²å¼€æ¢¯åº¦æ¶ˆå¤±çš„åŒºåŸŸã€‚</li><li>ä½¿ç”¨reluä»£æ›¿sigmoidå’Œtanhä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚åŸç†è¯·å‚è€ƒä¸Šä¸€ç¯‡æ–‡ç« <a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (4) - å·ç§¯ç¥ç»ç½‘ç»œ</a>çš„<strong>æ¿€æ´»å‡½æ•°</strong>ä¸€èŠ‚ã€‚</li><li>ä½¿ç”¨å…¶ä»–ç»“æ„çš„RNNsï¼Œæ¯”å¦‚é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLTSMï¼‰å’ŒGated Recurrent Unitï¼ˆGRUï¼‰ï¼Œè¿™æ˜¯æœ€æµè¡Œçš„åšæ³•ã€‚æˆ‘ä»¬å°†åœ¨ä»¥åçš„æ–‡ç« ä¸­ä»‹ç»è¿™ä¸¤ç§ç½‘ç»œã€‚</li></ol><h2 id="RNNçš„åº”ç”¨ä¸¾ä¾‹â€”â€”åŸºäºRNNçš„è¯­è¨€æ¨¡å‹"><a href="#RNNçš„åº”ç”¨ä¸¾ä¾‹â€”â€”åŸºäºRNNçš„è¯­è¨€æ¨¡å‹" class="headerlink" title="RNNçš„åº”ç”¨ä¸¾ä¾‹â€”â€”åŸºäºRNNçš„è¯­è¨€æ¨¡å‹"></a>RNNçš„åº”ç”¨ä¸¾ä¾‹â€”â€”åŸºäºRNNçš„è¯­è¨€æ¨¡å‹</h2><p>ç°åœ¨ï¼Œæˆ‘ä»¬ä»‹ç»ä¸€ä¸‹åŸºäºRNNè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆæŠŠè¯ä¾æ¬¡è¾“å…¥åˆ°å¾ªç¯ç¥ç»ç½‘ç»œä¸­ï¼Œæ¯è¾“å…¥ä¸€ä¸ªè¯ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œå°±è¾“å‡ºæˆªæ­¢åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„è¯ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬ä¾æ¬¡è¾“å…¥ï¼š</p><blockquote><p>æˆ‘ æ˜¨å¤© ä¸Šå­¦ è¿Ÿåˆ° äº†</p></blockquote><p>ç¥ç»ç½‘ç»œçš„è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-a69765380a75f860.png)</p><p>å…¶ä¸­ï¼Œså’Œeæ˜¯ä¸¤ä¸ªç‰¹æ®Šçš„è¯ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸€ä¸ªåºåˆ—çš„å¼€å§‹å’Œç»“æŸã€‚</p><h3 id="å‘é‡åŒ–"><a href="#å‘é‡åŒ–" class="headerlink" title="å‘é‡åŒ–"></a>å‘é‡åŒ–</h3><p>æˆ‘ä»¬çŸ¥é“ï¼Œç¥ç»ç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯<strong>å‘é‡</strong>ï¼Œä¸ºäº†è®©è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¢«ç¥ç»ç½‘ç»œå¤„ç†ï¼Œæˆ‘ä»¬å¿…é¡»æŠŠè¯è¡¨è¾¾ä¸ºå‘é‡çš„å½¢å¼ï¼Œè¿™æ ·ç¥ç»ç½‘ç»œæ‰èƒ½å¤„ç†å®ƒã€‚</p><p>ç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯<strong>è¯</strong>ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ­¥éª¤å¯¹è¾“å…¥è¿›è¡Œ<strong>å‘é‡åŒ–</strong>ï¼š</p><ol><li>å»ºç«‹ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¯çš„è¯å…¸ï¼Œæ¯ä¸ªè¯åœ¨è¯å…¸é‡Œé¢æœ‰ä¸€ä¸ªå”¯ä¸€çš„ç¼–å·ã€‚</li><li>ä»»æ„ä¸€ä¸ªè¯éƒ½å¯ä»¥ç”¨ä¸€ä¸ªNç»´çš„one-hotå‘é‡æ¥è¡¨ç¤ºã€‚å…¶ä¸­ï¼ŒNæ˜¯è¯å…¸ä¸­åŒ…å«çš„è¯çš„ä¸ªæ•°ã€‚å‡è®¾ä¸€ä¸ªè¯åœ¨è¯å…¸ä¸­çš„ç¼–å·æ˜¯iï¼Œvæ˜¯è¡¨ç¤ºè¿™ä¸ªè¯çš„å‘é‡ï¼Œ$v_j$æ˜¯å‘é‡çš„ç¬¬jä¸ªå…ƒç´ ï¼Œåˆ™ï¼š</li></ol><p>$v_j=\begin{equation}\begin{cases}1\qquad j=i\\0\qquad j\ne i\end{cases}\end{equation} $</p><p>ä¸Šé¢è¿™ä¸ªå…¬å¼çš„å«ä¹‰ï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„å›¾æ¥ç›´è§‚çš„è¡¨ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-14ae8b4f92e90c5c.png)</p><p>ä½¿ç”¨è¿™ç§å‘é‡åŒ–æ–¹æ³•ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªé«˜ç»´ã€<strong>ç¨€ç–</strong>çš„å‘é‡ï¼ˆç¨€ç–æ˜¯æŒ‡ç»å¤§éƒ¨åˆ†å…ƒç´ çš„å€¼éƒ½æ˜¯0ï¼‰ã€‚å¤„ç†è¿™æ ·çš„å‘é‡ä¼šå¯¼è‡´æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæœ‰å¾ˆå¤šçš„å‚æ•°ï¼Œå¸¦æ¥åºå¤§çš„è®¡ç®—é‡ã€‚å› æ­¤ï¼Œå¾€å¾€ä¼šéœ€è¦ä½¿ç”¨ä¸€äº›é™ç»´æ–¹æ³•ï¼Œå°†é«˜ç»´çš„ç¨€ç–å‘é‡è½¬å˜ä¸ºä½ç»´çš„ç¨ å¯†å‘é‡ã€‚ä¸è¿‡è¿™ä¸ªè¯é¢˜æˆ‘ä»¬å°±ä¸å†è¿™ç¯‡æ–‡ç« ä¸­è®¨è®ºäº†ã€‚</p><p>è¯­è¨€æ¨¡å‹è¦æ±‚çš„è¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥è®©å¾ªç¯ç¥ç»ç½‘ç»œè®¡ç®—è®¡ç®—è¯å…¸ä¸­æ¯ä¸ªè¯æ˜¯ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ï¼Œè¿™æ ·ï¼Œæ¦‚ç‡æœ€å¤§çš„è¯å°±æ˜¯ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„è¯ã€‚å› æ­¤ï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºå‘é‡ä¹Ÿæ˜¯ä¸€ä¸ªNç»´å‘é‡ï¼Œå‘é‡ä¸­çš„æ¯ä¸ªå…ƒç´ å¯¹åº”ç€è¯å…¸ä¸­ç›¸åº”çš„è¯æ˜¯ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-3e1562c7031309f1.png)</p><h3 id="Softmaxå±‚"><a href="#Softmaxå±‚" class="headerlink" title="Softmaxå±‚"></a>Softmaxå±‚</h3><p>å‰é¢æåˆ°ï¼Œ<strong>è¯­è¨€æ¨¡å‹</strong>æ˜¯å¯¹ä¸‹ä¸€ä¸ªè¯å‡ºç°çš„<strong>æ¦‚ç‡</strong>è¿›è¡Œå»ºæ¨¡ã€‚é‚£ä¹ˆï¼Œæ€æ ·è®©ç¥ç»ç½‘ç»œè¾“å‡ºæ¦‚ç‡å‘¢ï¼Ÿæ–¹æ³•å°±æ˜¯ç”¨softmaxå±‚ä½œä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ã€‚</p><p>æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹softmaxå‡½æ•°çš„å®šä¹‰ï¼š</p><p>$\begin{align} g(z_i)=\frac{e^{z_i}}{\sum_{k}e^{z_k}} \end{align}$</p><p>è¿™ä¸ªå…¬å¼çœ‹èµ·æ¥å¯èƒ½å¾ˆæ™•ï¼Œæˆ‘ä»¬ä¸¾ä¸€ä¸ªä¾‹å­ã€‚Softmaxå±‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-5a3219fab80ab45f.png)</p><p>ä»ä¸Šå›¾æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œsoftmax layerçš„è¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¸¤ä¸ªå‘é‡çš„ç»´åº¦æ˜¯ä¸€æ ·çš„ï¼ˆåœ¨è¿™ä¸ªä¾‹å­é‡Œé¢æ˜¯4ï¼‰ã€‚è¾“å…¥å‘é‡x=[1 2 3 4]ç»è¿‡softmaxå±‚ä¹‹åï¼Œç»è¿‡ä¸Šé¢çš„softmaxå‡½æ•°è®¡ç®—ï¼Œè½¬å˜ä¸ºè¾“å‡ºå‘é‡y=[0.03 0.09 0.24 0.64]ã€‚è®¡ç®—è¿‡ç¨‹ä¸ºï¼š</p><p>$\begin{align} y_1&amp;=\frac{e^{x_1}}{\sum_{k}e^{x_k}}\\ &amp;=\frac{e^1}{e^1+e^2+e^3+e^4}\\ &amp;=0.03\\ y_2&amp;=\frac{e^2}{e^1+e^2+e^3+e^4}\\ &amp;=0.09\\ y_3&amp;=\frac{e^3}{e^1+e^2+e^3+e^4}\\ &amp;=0.24\\ y_4&amp;=\frac{e^4}{e^1+e^2+e^3+e^4}\\ &amp;=0.64\\ \end{align} $</p><p>æˆ‘ä»¬æ¥çœ‹çœ‹è¾“å‡ºå‘é‡yçš„ç‰¹å¾ï¼š</p><ol><li>æ¯ä¸€é¡¹ä¸ºå–å€¼ä¸º0-1ä¹‹é—´çš„æ­£æ•°ï¼›</li><li>æ‰€æœ‰é¡¹çš„æ€»å’Œæ˜¯1ã€‚</li></ol><p>æˆ‘ä»¬ä¸éš¾å‘ç°ï¼Œè¿™äº›ç‰¹å¾å’Œ<strong>æ¦‚ç‡</strong>çš„ç‰¹å¾æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠå®ƒä»¬çœ‹åšæ˜¯æ¦‚ç‡ã€‚å¯¹äº<strong>è¯­è¨€æ¨¡å‹</strong>æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è®¤ä¸ºæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ˜¯è¯å…¸ä¸­ç¬¬ä¸€ä¸ªè¯çš„æ¦‚ç‡æ˜¯0.03ï¼Œæ˜¯è¯å…¸ä¸­ç¬¬äºŒä¸ªè¯çš„æ¦‚ç‡æ˜¯0.09ï¼Œä»¥æ­¤ç±»æ¨ã€‚</p><h3 id="è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ"><a href="#è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ" class="headerlink" title="è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ"></a>è¯­è¨€æ¨¡å‹çš„è®­ç»ƒ</h3><p>å¯ä»¥ä½¿ç”¨<strong>ç›‘ç£å­¦ä¹ </strong>çš„æ–¹æ³•å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé¦–å…ˆï¼Œéœ€è¦å‡†å¤‡è®­ç»ƒæ•°æ®é›†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»æ€æ ·æŠŠè¯­æ–™</p><blockquote><p>æˆ‘ æ˜¨å¤© ä¸Šå­¦ è¿Ÿåˆ° äº†</p></blockquote><p>è½¬æ¢æˆè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬è·å–<strong>è¾“å…¥-æ ‡ç­¾</strong>å¯¹ï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">è¾“å…¥</th><th style="text-align:center">æ ‡ç­¾</th></tr></thead><tbody><tr><td style="text-align:center">s</td><td style="text-align:center">æˆ‘</td></tr><tr><td style="text-align:center">æˆ‘</td><td style="text-align:center">æ˜¨å¤©</td></tr><tr><td style="text-align:center">æ˜¨å¤©</td><td style="text-align:center">ä¸Šå­¦</td></tr><tr><td style="text-align:center">ä¸Šå­¦</td><td style="text-align:center">è¿Ÿåˆ°</td></tr><tr><td style="text-align:center">è¿Ÿåˆ°</td><td style="text-align:center">äº†</td></tr><tr><td style="text-align:center">äº†</td><td style="text-align:center">e</td></tr></tbody></table></div><p>ç„¶åï¼Œä½¿ç”¨å‰é¢ä»‹ç»è¿‡çš„<strong>å‘é‡åŒ–</strong>æ–¹æ³•ï¼Œå¯¹è¾“å…¥xå’Œæ ‡ç­¾yè¿›è¡Œ<strong>å‘é‡åŒ–</strong>ã€‚è¿™é‡Œé¢æœ‰æ„æ€çš„æ˜¯ï¼Œå¯¹æ ‡ç­¾yè¿›è¡Œå‘é‡åŒ–ï¼Œå…¶ç»“æœä¹Ÿæ˜¯ä¸€ä¸ªone-hotå‘é‡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯¹æ ‡ç­¾ã€æˆ‘ã€è¿›è¡Œå‘é‡åŒ–ï¼Œå¾—åˆ°çš„å‘é‡ä¸­ï¼Œåªæœ‰ç¬¬2019ä¸ªå…ƒç´ çš„å€¼æ˜¯1ï¼Œå…¶ä»–ä½ç½®çš„å…ƒç´ çš„å€¼éƒ½æ˜¯0ã€‚å®ƒçš„å«ä¹‰å°±æ˜¯ä¸‹ä¸€ä¸ªè¯æ˜¯ã€æˆ‘ã€çš„æ¦‚ç‡æ˜¯1ï¼Œæ˜¯å…¶å®ƒè¯çš„æ¦‚ç‡éƒ½æ˜¯0ã€‚</p><p>æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>äº¤å‰ç†µè¯¯å·®å‡½æ•°</strong>ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</p><p>åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤§é‡çš„è¯­æ–™æ¥å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè·å–è®­ç»ƒæ•°æ®å’Œè®­ç»ƒçš„æ–¹æ³•éƒ½æ˜¯ç›¸åŒçš„ã€‚</p><h3 id="äº¤å‰ç†µè¯¯å·®"><a href="#äº¤å‰ç†µè¯¯å·®" class="headerlink" title="äº¤å‰ç†µè¯¯å·®"></a>äº¤å‰ç†µè¯¯å·®</h3><p>ä¸€èˆ¬æ¥è¯´ï¼Œå½“ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚æ˜¯softmaxå±‚æ—¶ï¼Œå¯¹åº”çš„è¯¯å·®å‡½æ•°Eé€šå¸¸é€‰æ‹©äº¤å‰ç†µè¯¯å·®å‡½æ•°ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š</p><p>$\begin{align} L(y,o)=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}  \end{align}$</p><p>åœ¨ä¸Šå¼ä¸­ï¼ŒNæ˜¯è®­ç»ƒæ ·æœ¬çš„ä¸ªæ•°ï¼Œå‘é‡$y_n$æ˜¯æ ·æœ¬çš„æ ‡è®°ï¼Œå‘é‡$o_n$æ˜¯ç½‘ç»œçš„è¾“å‡ºã€‚æ ‡è®°$y_n$æ˜¯ä¸€ä¸ªone-hotå‘é‡ï¼Œä¾‹å¦‚$y_1=[1,0,0,0]$ï¼Œå¦‚æœç½‘ç»œçš„è¾“å‡º$o=[0.03,0.09,0.24,0.64]$ï¼Œé‚£ä¹ˆï¼Œäº¤å‰ç†µè¯¯å·®æ˜¯ï¼ˆå‡è®¾åªæœ‰ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå³N=1ï¼‰ï¼š</p><p>$\begin{align} L&amp;=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}\\ &amp;=-y_1logo_1\\ &amp;=-(1<em>log0.03+0</em>log0.09+0<em>log0.24+0</em>log0.64)\\ &amp;=3.51 \end{align} $</p><p>æˆ‘ä»¬å½“ç„¶å¯ä»¥é€‰æ‹©å…¶ä»–å‡½æ•°ä½œä¸ºæˆ‘ä»¬çš„è¯¯å·®å‡½æ•°ï¼Œæ¯”å¦‚æœ€å°å¹³æ–¹è¯¯å·®å‡½æ•°(MSE)ã€‚ä¸è¿‡å¯¹æ¦‚ç‡è¿›è¡Œå»ºæ¨¡æ—¶ï¼Œé€‰æ‹©äº¤å‰ç†µè¯¯å·®å‡½æ•°æ›´make senseã€‚å…·ä½“åŸå› ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…è¯·é˜…è¯»<a href="https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/" target="_blank" rel="noopener">å‚è€ƒæ–‡çŒ®7</a>ã€‚</p><h2 id="RNNçš„å®ç°"><a href="#RNNçš„å®ç°" class="headerlink" title="RNNçš„å®ç°"></a>RNNçš„å®ç°</h2><blockquote><p>å®Œæ•´ä»£ç è¯·å‚è€ƒGitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/rnn.py" target="_blank" rel="noopener">https://github.com/hanbt/learn_dl/blob/master/rnn.py</a> (python2.7)</p></blockquote><p>ä¸ºäº†åŠ æ·±æˆ‘ä»¬å¯¹å‰é¢ä»‹ç»çš„çŸ¥è¯†çš„ç†è§£ï¼Œæˆ‘ä»¬æ¥åŠ¨æ‰‹å®ç°ä¸€ä¸ªRNNå±‚ã€‚æˆ‘ä»¬å¤ç”¨äº†ä¸Šä¸€ç¯‡æ–‡ç« <a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (4) - å·ç§¯ç¥ç»ç½‘ç»œ</a>ä¸­çš„ä¸€äº›ä»£ç ï¼Œæ‰€ä»¥å…ˆæŠŠå®ƒä»¬å¯¼å…¥è¿›æ¥ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npfrom cnn import ReluActivator, IdentityActivator, element_wise_op</code></pre><p>æˆ‘ä»¬ç”¨RecurrentLayerç±»æ¥å®ç°ä¸€ä¸ª<strong>å¾ªç¯å±‚</strong>ã€‚ä¸‹é¢çš„ä»£ç æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå¾ªç¯å±‚ï¼Œå¯ä»¥åœ¨æ„é€ å‡½æ•°ä¸­è®¾ç½®å·ç§¯å±‚çš„è¶…å‚æ•°ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå¾ªç¯å±‚æœ‰ä¸¤ä¸ªæƒé‡æ•°ç»„ï¼ŒUå’ŒWã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">class RecurrentLayer(object):    def __init__(self, input_width, state_width,                 activator, learning_rate):        self.input_width = input_width        self.state_width = state_width        self.activator = activator        self.learning_rate = learning_rate        self.times = 0       # å½“å‰æ—¶åˆ»åˆå§‹åŒ–ä¸ºt0        self.state_list = [] # ä¿å­˜å„ä¸ªæ—¶åˆ»çš„state        self.state_list.append(np.zeros(            (state_width, 1)))           # åˆå§‹åŒ–s0        self.U = np.random.uniform(-1e-4, 1e-4,            (state_width, input_width))  # åˆå§‹åŒ–U        self.W = np.random.uniform(-1e-4, 1e-4,            (state_width, state_width))  # åˆå§‹åŒ–W</code></pre><p>åœ¨forwardæ–¹æ³•ä¸­ï¼Œå®ç°å¾ªç¯å±‚çš„å‰å‘è®¡ç®—ï¼Œè¿™éƒ¨åˆ†æ¯”è¾ƒç®€å•ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">    def forward(self, input_array):        '''        æ ¹æ®ã€å¼2ã€è¿›è¡Œå‰å‘è®¡ç®—        '''        self.times += 1        state = (np.dot(self.U, input_array) +                 np.dot(self.W, self.state_list[-1]))        element_wise_op(state, self.activator.forward)        self.state_list.append(state)</code></pre><p>åœ¨backwordæ–¹æ³•ä¸­ï¼Œå®ç°BPTTç®—æ³•ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">    def backward(self, sensitivity_array,                      activator):            '''            å®ç°BPTTç®—æ³•            '''            self.calc_delta(sensitivity_array, activator)            self.calc_gradient()    def calc_delta(self, sensitivity_array, activator):        self.delta_list = []  # ç”¨æ¥ä¿å­˜å„ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹        for i in range(self.times):            self.delta_list.append(np.zeros(                (self.state_width, 1)))        self.delta_list.append(sensitivity_array)        # è¿­ä»£è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹        for k in range(self.times - 1, 0, -1):            self.calc_delta_k(k, activator)    def calc_delta_k(self, k, activator):        '''        æ ¹æ®k+1æ—¶åˆ»çš„deltaè®¡ç®—kæ—¶åˆ»çš„delta        '''        state = self.state_list[k+1].copy()        element_wise_op(self.state_list[k+1],                    activator.backward)        self.delta_list[k] = np.dot(            np.dot(self.delta_list[k+1].T, self.W),            np.diag(state[:,0])).T    def calc_gradient(self):        self.gradient_list = [] # ä¿å­˜å„ä¸ªæ—¶åˆ»çš„æƒé‡æ¢¯åº¦        for t in range(self.times + 1):            self.gradient_list.append(np.zeros(                (self.state_width, self.state_width)))        for t in range(self.times, 0, -1):            self.calc_gradient_t(t)        # å®é™…çš„æ¢¯åº¦æ˜¯å„ä¸ªæ—¶åˆ»æ¢¯åº¦ä¹‹å’Œ        self.gradient = reduce(            lambda a, b: a + b, self.gradient_list,            self.gradient_list[0]) # [0]è¢«åˆå§‹åŒ–ä¸º0ä¸”æ²¡æœ‰è¢«ä¿®æ”¹è¿‡    def calc_gradient_t(self, t):        '''        è®¡ç®—æ¯ä¸ªæ—¶åˆ»tæƒé‡çš„æ¢¯åº¦        '''        gradient = np.dot(self.delta_list[t],            self.state_list[t-1].T)        self.gradient_list[t] = gradient</code></pre><p>æœ‰æ„æ€çš„æ˜¯ï¼ŒBPTTç®—æ³•è™½ç„¶æ•°å­¦æ¨å¯¼çš„è¿‡ç¨‹å¾ˆéº»çƒ¦ï¼Œä½†æ˜¯å†™æˆä»£ç å´å¹¶ä¸å¤æ‚ã€‚</p><p>åœ¨updateæ–¹æ³•ä¸­ï¼Œå®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">    def update(self):        '''        æŒ‰ç…§æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°æƒé‡        '''        self.W -= self.learning_rate * self.gradient</code></pre><p>ä¸Šé¢çš„ä»£ç ä¸åŒ…å«æƒé‡Uçš„æ›´æ–°ã€‚è¿™éƒ¨åˆ†å®é™…ä¸Šå’Œå…¨è¿æ¥ç¥ç»ç½‘ç»œæ˜¯ä¸€æ ·çš„ï¼Œç•™ç»™æ„Ÿå…´è¶£çš„è¯»è€…è‡ªå·±æ¥å®Œæˆå§ã€‚</p><p><strong>å¾ªç¯å±‚</strong>æ˜¯ä¸€ä¸ª<strong>å¸¦çŠ¶æ€</strong>çš„å±‚ï¼Œæ¯æ¬¡forwordéƒ½ä¼šæ”¹å˜å¾ªç¯å±‚çš„å†…éƒ¨çŠ¶æ€ï¼Œè¿™ç»™æ¢¯åº¦æ£€æŸ¥å¸¦æ¥äº†éº»çƒ¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªreset_stateæ–¹æ³•ï¼Œæ¥é‡ç½®å¾ªç¯å±‚çš„å†…éƒ¨çŠ¶æ€ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">    def reset_state(self):        self.times = 0       # å½“å‰æ—¶åˆ»åˆå§‹åŒ–ä¸ºt0        self.state_list = [] # ä¿å­˜å„ä¸ªæ—¶åˆ»çš„state        self.state_list.append(np.zeros(            (self.state_width, 1)))      # åˆå§‹åŒ–s0</code></pre><p>æœ€åï¼Œæ˜¯æ¢¯åº¦æ£€æŸ¥çš„ä»£ç ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">def gradient_check():    '''    æ¢¯åº¦æ£€æŸ¥    '''    # è®¾è®¡ä¸€ä¸ªè¯¯å·®å‡½æ•°ï¼Œå–æ‰€æœ‰èŠ‚ç‚¹è¾“å‡ºé¡¹ä¹‹å’Œ    error_function = lambda o: o.sum()    rl = RecurrentLayer(3, 2, IdentityActivator(), 1e-3)    # è®¡ç®—forwardå€¼    x, d = data_set()    rl.forward(x[0])    rl.forward(x[1])    # æ±‚å–sensitivity map    sensitivity_array = np.ones(rl.state_list[-1].shape,                                dtype=np.float64)    # è®¡ç®—æ¢¯åº¦    rl.backward(sensitivity_array, IdentityActivator())    # æ£€æŸ¥æ¢¯åº¦    epsilon = 10e-4    for i in range(rl.W.shape[0]):        for j in range(rl.W.shape[1]):            rl.W[i,j] += epsilon            rl.reset_state()            rl.forward(x[0])            rl.forward(x[1])            err1 = error_function(rl.state_list[-1])            rl.W[i,j] -= 2*epsilon            rl.reset_state()            rl.forward(x[0])            rl.forward(x[1])            err2 = error_function(rl.state_list[-1])            expect_grad = (err1 - err2) / (2 * epsilon)            rl.W[i,j] += epsilon            print 'weights(%d,%d): expected - actural %f - %f' % (                i, j, expect_grad, rl.gradient[i,j])</code></pre><p>éœ€è¦æ³¨æ„ï¼Œæ¯æ¬¡è®¡ç®—errorä¹‹å‰ï¼Œéƒ½è¦è°ƒç”¨reset_stateæ–¹æ³•é‡ç½®å¾ªç¯å±‚çš„å†…éƒ¨çŠ¶æ€ã€‚ä¸‹é¢æ˜¯æ¢¯åº¦æ£€æŸ¥çš„ç»“æœï¼Œæ²¡é—®é¢˜ï¼</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-1bdfa618b5dbaabd.png)</p><h2 id="å°èŠ‚"><a href="#å°èŠ‚" class="headerlink" title="å°èŠ‚"></a>å°èŠ‚</h2><p>è‡³æ­¤ï¼Œæˆ‘ä»¬è®²å®Œäº†åŸºæœ¬çš„<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ã€å®ƒçš„è®­ç»ƒç®—æ³•ï¼š<strong>BPTT</strong>ï¼Œä»¥åŠåœ¨è¯­è¨€æ¨¡å‹ä¸Šçš„åº”ç”¨ã€‚RNNæ¯”è¾ƒçƒ§è„‘ï¼Œç›¸ä¿¡æ‹¿ä¸‹å‰å‡ ç¯‡æ–‡ç« çš„è¯»è€…ä»¬æå®šè¿™ç¯‡æ–‡ç« ä¹Ÿä¸åœ¨è¯ä¸‹å§ï¼ç„¶è€Œï¼Œ<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>è¿™ä¸ªè¯é¢˜å¹¶æ²¡æœ‰å®Œç»“ã€‚æˆ‘ä»¬åœ¨å‰é¢è¯´åˆ°è¿‡ï¼ŒåŸºæœ¬çš„å¾ªç¯ç¥ç»ç½‘ç»œå­˜åœ¨æ¢¯åº¦çˆ†ç‚¸å’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶ä¸èƒ½çœŸæ­£çš„å¤„ç†å¥½é•¿è·ç¦»çš„ä¾èµ–ï¼ˆè™½ç„¶æœ‰ä¸€äº›æŠ€å·§å¯ä»¥å‡è½»è¿™äº›é—®é¢˜ï¼‰ã€‚äº‹å®ä¸Šï¼ŒçœŸæ­£å¾—åˆ°å¹¿æ³›çš„åº”ç”¨çš„æ˜¯å¾ªç¯ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªå˜ä½“ï¼š<strong>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ</strong>ã€‚å®ƒå†…éƒ¨æœ‰ä¸€äº›ç‰¹æ®Šçš„ç»“æ„ï¼Œå¯ä»¥å¾ˆå¥½çš„å¤„ç†é•¿è·ç¦»çš„ä¾èµ–ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­è¯¦ç»†çš„ä»‹ç»å®ƒã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç¨äº‹ä¼‘æ¯ï¼Œå‡†å¤‡æŒ‘æˆ˜æ›´ä¸ºçƒ§è„‘çš„<strong>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ</strong>å§ã€‚</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(5" alt="img">%20-%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2256672-253fd3d6688ea73e.jpg)</p><h2 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h2><ol><li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">RECURRENT NEURAL NETWORKS TUTORIAL</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li><li><a href="http://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a></li><li><a href="http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks, Bengio et al.</a></li><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" target="_blank" rel="noopener">Recurrent neural network based language model, Mikolov et al.</a></li><li><a href="https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/" target="_blank" rel="noopener">Neural Network Classification, Categorical Data, Softmax Activation, and Cross Entropy Error, McCaffrey</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ -é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ</title>
      <link href="/ling-ji-chu-ru-men-shen-du-xue-xi-6-chang-duan-shi-ji-yi-wang-luo-lstm/"/>
      <url>/ling-ji-chu-ru-men-shen-du-xue-xi-6-chang-duan-shi-ji-yi-wang-luo-lstm/</url>
      
        <content type="html"><![CDATA[<p>å‚è€ƒèµ„æ–™ï¼š<a href="https://www.zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/581764</a></p><hr><h2 id="å¾€æœŸå›é¡¾"><a href="#å¾€æœŸå›é¡¾" class="headerlink" title="å¾€æœŸå›é¡¾"></a>å¾€æœŸå›é¡¾</h2><p>åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ä»¥åŠå®ƒçš„è®­ç»ƒç®—æ³•ã€‚æˆ‘ä»¬ä¹Ÿä»‹ç»äº†<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å¾ˆéš¾è®­ç»ƒçš„åŸå› ï¼Œè¿™å¯¼è‡´äº†å®ƒåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾ˆéš¾å¤„ç†é•¿è·ç¦»çš„ä¾èµ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€ç§æ”¹è¿›ä¹‹åçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼š<strong>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ(Long Short Term Memory Network, LSTM)</strong>ï¼Œå®ƒæˆåŠŸçš„è§£å†³äº†åŸå§‹å¾ªç¯ç¥ç»ç½‘ç»œçš„ç¼ºé™·ï¼Œæˆä¸ºå½“å‰æœ€æµè¡Œçš„RNNï¼Œåœ¨è¯­éŸ³è¯†åˆ«ã€å›¾ç‰‡æè¿°ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰è®¸å¤šé¢†åŸŸä¸­æˆåŠŸåº”ç”¨ã€‚ä½†ä¸å¹¸çš„ä¸€é¢æ˜¯ï¼Œ<strong>LSTM</strong>çš„ç»“æ„å¾ˆå¤æ‚ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦èŠ±ä¸Šä¸€äº›åŠ›æ°”ï¼Œæ‰èƒ½æŠŠLSTMä»¥åŠå®ƒçš„è®­ç»ƒç®—æ³•å¼„æ˜ç™½ã€‚åœ¨ææ¸…æ¥š<strong>LSTM</strong>ä¹‹åï¼Œæˆ‘ä»¬å†ä»‹ç»ä¸€ç§<strong>LSTM</strong>çš„å˜ä½“ï¼š<strong>GRU (Gated Recurrent Unit)</strong>ã€‚ å®ƒçš„ç»“æ„æ¯”<strong>LSTM</strong>ç®€å•ï¼Œè€Œæ•ˆæœå´å’Œ<strong>LSTM</strong>ä¸€æ ·å¥½ï¼Œå› æ­¤ï¼Œå®ƒæ­£åœ¨é€æ¸æµè¡Œèµ·æ¥ã€‚æœ€åï¼Œæˆ‘ä»¬ä»ç„¶ä¼šåŠ¨æ‰‹å®ç°ä¸€ä¸ª<strong>LSTM</strong>ã€‚</p><h2 id="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œæ˜¯å•¥"><a href="#é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œæ˜¯å•¥" class="headerlink" title="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œæ˜¯å•¥"></a>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œæ˜¯å•¥</h2><p>æˆ‘ä»¬é¦–å…ˆäº†è§£ä¸€ä¸‹é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œäº§ç”Ÿçš„èƒŒæ™¯ã€‚å›é¡¾ä¸€ä¸‹<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (5) - å¾ªç¯ç¥ç»ç½‘ç»œ</a>ä¸­æ¨å¯¼çš„ï¼Œè¯¯å·®é¡¹æ²¿æ—¶é—´åå‘ä¼ æ’­çš„å…¬å¼ï¼š</p><p>$\begin{align} \delta_k^T=&amp;\delta_t^T\prod_{i=k}^{t-1}diag[fâ€™(\mathbf{net}_{i})]W\\ \end{align} $</p><p>æˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸‹é¢çš„ä¸ç­‰å¼ï¼Œæ¥è·å–$\delta_k^T$çš„æ¨¡çš„ä¸Šç•Œï¼ˆæ¨¡å¯ä»¥çœ‹åšå¯¹$\delta_k^T$ä¸­æ¯ä¸€é¡¹å€¼çš„å¤§å°çš„åº¦é‡ï¼‰ï¼š</p><p>$\begin{align} |\delta_k^T|\leqslant&amp;|\delta_t^T|\prod_{i=k}^{t-1}|diag[fâ€™(\mathbf{net}_{i})]||W|\\ \leqslant&amp;|\delta_t^T|(\beta_f\beta_W)^{t-k} \end{align} $</p><p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¯¯å·®é¡¹$\delta$ä»tæ—¶åˆ»ä¼ é€’åˆ°kæ—¶åˆ»ï¼Œå…¶å€¼çš„ä¸Šç•Œæ˜¯$\beta_f\beta_w$çš„æŒ‡æ•°å‡½æ•°ã€‚$\beta_f\beta_w$åˆ†åˆ«æ˜¯å¯¹è§’çŸ©é˜µ$diag[fâ€™(\mathbf{net}_{i})]$å’ŒçŸ©é˜µWæ¨¡çš„ä¸Šç•Œã€‚æ˜¾ç„¶ï¼Œé™¤éä¹˜ç§¯çš„å€¼ä½äº1é™„è¿‘ï¼Œå¦åˆ™ï¼Œå½“t-kå¾ˆå¤§æ—¶ï¼ˆä¹Ÿå°±æ˜¯è¯¯å·®ä¼ é€’å¾ˆå¤šä¸ªæ—¶åˆ»æ—¶ï¼‰ï¼Œæ•´ä¸ªå¼å­çš„å€¼å°±ä¼šå˜å¾—æå°ï¼ˆå½“$\beta_f\beta_w$ä¹˜ç§¯å°äº1ï¼‰æˆ–è€…æå¤§ï¼ˆå½“$\beta_f\beta_w$ä¹˜ç§¯å¤§äº1ï¼‰ï¼Œå‰è€…å°±æ˜¯<strong>æ¢¯åº¦æ¶ˆå¤±</strong>ï¼Œåè€…å°±æ˜¯<strong>æ¢¯åº¦çˆ†ç‚¸</strong>ã€‚è™½ç„¶ç§‘å­¦å®¶ä»¬æå‡ºäº†å¾ˆå¤šæŠ€å·§ï¼ˆæ¯”å¦‚æ€æ ·åˆå§‹åŒ–æƒé‡ï¼‰ï¼Œè®©$\beta_f\beta_w$çš„å€¼å°½å¯èƒ½è´´è¿‘äº1ï¼Œç»ˆç©¶è¿˜æ˜¯éš¾ä»¥æŠµæŒ¡æŒ‡æ•°å‡½æ•°çš„å¨åŠ›ã€‚</p><p><strong>æ¢¯åº¦æ¶ˆå¤±</strong>åˆ°åº•æ„å‘³ç€ä»€ä¹ˆï¼Ÿåœ¨<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (5) - å¾ªç¯ç¥ç»ç½‘ç»œ</a>ä¸­æˆ‘ä»¬å·²è¯æ˜ï¼Œæƒé‡æ•°ç»„Wæœ€ç»ˆçš„æ¢¯åº¦æ˜¯å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦ä¹‹å’Œï¼Œå³ï¼š</p><p>$\begin{align} \nabla_WE&amp;=\sum_{k=1}^t\nabla_{Wk}E\\ &amp;=\nabla_{Wt}E+\nabla_{Wt-1}E+\nabla_{Wt-2}E+â€¦+\nabla_{W1}E \end{align} $</p><p>å‡è®¾æŸè½®è®­ç»ƒä¸­ï¼Œå„æ—¶åˆ»çš„æ¢¯åº¦ä»¥åŠæœ€ç»ˆçš„æ¢¯åº¦ä¹‹å’Œå¦‚ä¸‹å›¾ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-48784f6366412472.png)</p><p>æˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°ï¼Œä»ä¸Šå›¾çš„t-3æ—¶åˆ»å¼€å§‹ï¼Œæ¢¯åº¦å·²ç»å‡ ä¹å‡å°‘åˆ°0äº†ã€‚é‚£ä¹ˆï¼Œä»è¿™ä¸ªæ—¶åˆ»å¼€å§‹å†å¾€ä¹‹å‰èµ°ï¼Œå¾—åˆ°çš„æ¢¯åº¦ï¼ˆå‡ ä¹ä¸ºé›¶ï¼‰å°±ä¸ä¼šå¯¹æœ€ç»ˆçš„æ¢¯åº¦å€¼æœ‰ä»»ä½•è´¡çŒ®ï¼Œè¿™å°±ç›¸å½“äºæ— è®ºt-3æ—¶åˆ»ä¹‹å‰çš„ç½‘ç»œçŠ¶æ€hæ˜¯ä»€ä¹ˆï¼Œåœ¨è®­ç»ƒä¸­éƒ½ä¸ä¼šå¯¹æƒé‡æ•°ç»„Wçš„æ›´æ–°äº§ç”Ÿå½±å“ï¼Œä¹Ÿå°±æ˜¯ç½‘ç»œäº‹å®ä¸Šå·²ç»å¿½ç•¥äº†t-3æ—¶åˆ»ä¹‹å‰çš„çŠ¶æ€ã€‚è¿™å°±æ˜¯åŸå§‹RNNæ— æ³•å¤„ç†é•¿è·ç¦»ä¾èµ–çš„åŸå› ã€‚</p><p>æ—¢ç„¶æ‰¾åˆ°äº†é—®é¢˜çš„åŸå› ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±èƒ½è§£å†³å®ƒã€‚ä»é—®é¢˜çš„å®šä½åˆ°è§£å†³ï¼Œç§‘å­¦å®¶ä»¬å¤§æ¦‚èŠ±äº†7ã€8å¹´æ—¶é—´ã€‚ç»ˆäºæœ‰ä¸€å¤©ï¼ŒHochreiterå’ŒSchmidhuberä¸¤ä½ç§‘å­¦å®¶å‘æ˜å‡º<strong>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ</strong>ï¼Œä¸€ä¸¾è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p><p>å…¶å®ï¼Œ<strong>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ</strong>çš„æ€è·¯æ¯”è¾ƒç®€å•ã€‚åŸå§‹RNNçš„éšè—å±‚åªæœ‰ä¸€ä¸ªçŠ¶æ€ï¼Œå³hï¼Œå®ƒå¯¹äºçŸ­æœŸçš„è¾“å…¥éå¸¸æ•æ„Ÿã€‚é‚£ä¹ˆï¼Œå‡å¦‚æˆ‘ä»¬å†å¢åŠ ä¸€ä¸ªçŠ¶æ€ï¼Œå³cï¼Œè®©å®ƒæ¥ä¿å­˜é•¿æœŸçš„çŠ¶æ€ï¼Œé‚£ä¹ˆé—®é¢˜ä¸å°±è§£å†³äº†ä¹ˆï¼Ÿå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-71de4194da5a5ec4.png)</p><p>æ–°å¢åŠ çš„çŠ¶æ€cï¼Œç§°ä¸º<strong>å•å…ƒçŠ¶æ€(cell state)</strong>ã€‚æˆ‘ä»¬æŠŠä¸Šå›¾æŒ‰ç…§æ—¶é—´ç»´åº¦å±•å¼€ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-715658c134b9d6f1.png)</p><p>ä¸Šå›¾ä»…ä»…æ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œåœ¨tæ—¶åˆ»ï¼ŒLSTMçš„è¾“å…¥æœ‰ä¸‰ä¸ªï¼šå½“å‰æ—¶åˆ»ç½‘ç»œçš„è¾“å…¥å€¼$\mathbf{x}_t$ã€ä¸Šä¸€æ—¶åˆ»LSTMçš„è¾“å‡ºå€¼$\mathbf{h}_{t-1}$ã€ä»¥åŠä¸Šä¸€æ—¶åˆ»çš„å•å…ƒçŠ¶æ€$\mathbf{c}_{t-1}$ï¼›LSTMçš„è¾“å‡ºæœ‰ä¸¤ä¸ªï¼šå½“å‰æ—¶åˆ»LSTMè¾“å‡ºå€¼$\mathbf{h}_t$ã€å’Œå½“å‰æ—¶åˆ»çš„å•å…ƒçŠ¶æ€$\mathbf{c}_t$ã€‚æ³¨æ„$\mathbf{x}$ã€$\mathbf{h}$ã€$\mathbf{c}$éƒ½æ˜¯<strong>å‘é‡</strong>ã€‚</p><p>LSTMçš„å…³é”®ï¼Œå°±æ˜¯æ€æ ·æ§åˆ¶é•¿æœŸçŠ¶æ€cã€‚åœ¨è¿™é‡Œï¼ŒLSTMçš„æ€è·¯æ˜¯ä½¿ç”¨ä¸‰ä¸ªæ§åˆ¶å¼€å…³ã€‚ç¬¬ä¸€ä¸ªå¼€å…³ï¼Œè´Ÿè´£æ§åˆ¶ç»§ç»­ä¿å­˜é•¿æœŸçŠ¶æ€cï¼›ç¬¬äºŒä¸ªå¼€å…³ï¼Œè´Ÿè´£æ§åˆ¶æŠŠå³æ—¶çŠ¶æ€è¾“å…¥åˆ°é•¿æœŸçŠ¶æ€cï¼›ç¬¬ä¸‰ä¸ªå¼€å…³ï¼Œè´Ÿè´£æ§åˆ¶æ˜¯å¦æŠŠé•¿æœŸçŠ¶æ€cä½œä¸ºå½“å‰çš„LSTMçš„è¾“å‡ºã€‚ä¸‰ä¸ªå¼€å…³çš„ä½œç”¨å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-bff9353b92b9c488.png)</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦æè¿°ä¸€ä¸‹ï¼Œè¾“å‡ºhå’Œå•å…ƒçŠ¶æ€cçš„å…·ä½“è®¡ç®—æ–¹æ³•ã€‚</p><h2 id="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å‰å‘è®¡ç®—"><a href="#é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å‰å‘è®¡ç®—" class="headerlink" title="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å‰å‘è®¡ç®—"></a>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å‰å‘è®¡ç®—</h2><p>å‰é¢æè¿°çš„å¼€å…³æ˜¯æ€æ ·åœ¨ç®—æ³•ä¸­å®ç°çš„å‘¢ï¼Ÿè¿™å°±ç”¨åˆ°äº†<strong>é—¨ï¼ˆgateï¼‰</strong>çš„æ¦‚å¿µã€‚é—¨å®é™…ä¸Šå°±æ˜¯ä¸€å±‚<strong>å…¨è¿æ¥å±‚</strong>ï¼Œå®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ª0åˆ°1ä¹‹é—´çš„å®æ•°å‘é‡ã€‚å‡è®¾Wæ˜¯é—¨çš„æƒé‡å‘é‡ï¼Œæ˜¯åç½®é¡¹ï¼Œé‚£ä¹ˆé—¨å¯ä»¥è¡¨ç¤ºä¸ºï¼š</p><p>$\begin{align}g(\mathbf{x})=\sigma(W\mathbf{x}+\mathbf{b}) \end{align} $</p><p>é—¨çš„ä½¿ç”¨ï¼Œå°±æ˜¯ç”¨é—¨çš„è¾“å‡ºå‘é‡æŒ‰å…ƒç´ ä¹˜ä»¥æˆ‘ä»¬éœ€è¦æ§åˆ¶çš„é‚£ä¸ªå‘é‡ã€‚å› ä¸ºé—¨çš„è¾“å‡ºæ˜¯0åˆ°1ä¹‹é—´çš„å®æ•°å‘é‡ï¼Œé‚£ä¹ˆï¼Œå½“é—¨è¾“å‡ºä¸º0æ—¶ï¼Œä»»ä½•å‘é‡ä¸ä¹‹ç›¸ä¹˜éƒ½ä¼šå¾—åˆ°0å‘é‡ï¼Œè¿™å°±ç›¸å½“äºå•¥éƒ½ä¸èƒ½é€šè¿‡ï¼›è¾“å‡ºä¸º1æ—¶ï¼Œä»»ä½•å‘é‡ä¸ä¹‹ç›¸ä¹˜éƒ½ä¸ä¼šæœ‰ä»»ä½•æ”¹å˜ï¼Œè¿™å°±ç›¸å½“äºå•¥éƒ½å¯ä»¥é€šè¿‡ã€‚å› ä¸º$\sigma$ï¼ˆä¹Ÿå°±æ˜¯sigmoidå‡½æ•°ï¼‰çš„å€¼åŸŸæ˜¯(0,1)ï¼Œæ‰€ä»¥é—¨çš„çŠ¶æ€éƒ½æ˜¯åŠå¼€åŠé—­çš„ã€‚</p><p>LSTMç”¨ä¸¤ä¸ªé—¨æ¥æ§åˆ¶å•å…ƒçŠ¶æ€cçš„å†…å®¹ï¼Œä¸€ä¸ªæ˜¯<strong>é—å¿˜é—¨ï¼ˆforget gateï¼‰</strong>ï¼Œå®ƒå†³å®šäº†ä¸Šä¸€æ—¶åˆ»çš„å•å…ƒçŠ¶æ€$\mathbf{c}_{t-1}$æœ‰å¤šå°‘ä¿ç•™åˆ°å½“å‰æ—¶åˆ»$\mathbf{c}_{t}$ï¼›å¦ä¸€ä¸ªæ˜¯<strong>è¾“å…¥é—¨ï¼ˆinput gateï¼‰</strong>ï¼Œå®ƒå†³å®šäº†å½“å‰æ—¶åˆ»ç½‘ç»œçš„è¾“å…¥$\mathbf{x}_t$æœ‰å¤šå°‘ä¿å­˜åˆ°å•å…ƒçŠ¶æ€$\mathbf{c}_t$ã€‚LSTMç”¨<strong>è¾“å‡ºé—¨ï¼ˆoutput gateï¼‰</strong>æ¥æ§åˆ¶å•å…ƒçŠ¶æ€$\mathbf{c}_t$æœ‰å¤šå°‘è¾“å‡ºåˆ°LSTMçš„å½“å‰è¾“å‡ºå€¼$\mathbf{h}_t$ã€‚</p><p>æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹é—å¿˜é—¨ï¼š</p><p>$\begin{align}\mathbf{f}_t=\sigma(W_f\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_f)\qquad\quad(å¼1) \end{align} $</p><p>ä¸Šå¼ä¸­ï¼Œ$W_f$æ˜¯é—å¿˜é—¨çš„æƒé‡çŸ©é˜µï¼Œ$[\mathbf{h}_{t-1},\mathbf{x}_t]$è¡¨ç¤ºæŠŠä¸¤ä¸ªå‘é‡è¿æ¥æˆä¸€ä¸ªæ›´é•¿çš„å‘é‡ï¼Œ$\mathbf{b}_f$æ˜¯é—å¿˜é—¨çš„åç½®é¡¹ï¼Œ$\sigma$æ˜¯sigmoidå‡½æ•°ã€‚å¦‚æœè¾“å…¥çš„ç»´åº¦æ˜¯$d_x$ï¼Œéšè—å±‚çš„ç»´åº¦æ˜¯$d_h$ï¼Œå•å…ƒçŠ¶æ€çš„ç»´åº¦æ˜¯$d_c$ï¼ˆé€šå¸¸$d_c=d_h$ï¼‰ï¼Œåˆ™é—å¿˜é—¨çš„æƒé‡çŸ©é˜µ$W_f$ç»´åº¦æ˜¯$d_c\times (d_h+d_x)$ã€‚äº‹å®ä¸Šï¼Œæƒé‡çŸ©é˜µ$W_f$éƒ½æ˜¯ä¸¤ä¸ªçŸ©é˜µæ‹¼æ¥è€Œæˆçš„ï¼šä¸€ä¸ªæ˜¯$W_{fh}$ï¼Œå®ƒå¯¹åº”ç€è¾“å…¥é¡¹$\mathbf{h}_{t-1}$ï¼Œå…¶ç»´åº¦ä¸º$d_c\times d_h$ï¼›ä¸€ä¸ªæ˜¯$W_{fx}$ï¼Œå®ƒå¯¹åº”ç€è¾“å…¥é¡¹$\mathbf{x}_t$ï¼Œå…¶ç»´åº¦ä¸º$d_c\times d_x$ã€‚$W_f$å¯ä»¥å†™ä¸ºï¼š</p><p>$\begin{align} \begin{bmatrix}W_f\end{bmatrix}\begin{bmatrix}\mathbf{h}_{t-1}\\ \mathbf{x}_t\end{bmatrix}&amp;= \begin{bmatrix}W_{fh}&amp;W_{fx}\end{bmatrix}\begin{bmatrix}\mathbf{h}_{t-1}\\ \mathbf{x}_t\end{bmatrix}\\ &amp;=W_{fh}\mathbf{h}_{t-1}+W_{fx}\mathbf{x}_t \end{align} $</p><p>ä¸‹å›¾æ˜¾ç¤ºäº†é—å¿˜é—¨çš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-c7f7ca0aa64b562f.png)</p><p>æ¥ä¸‹æ¥çœ‹çœ‹è¾“å…¥é—¨ï¼š</p><p>$\begin{align} \mathbf{i}_t=\sigma(W_i\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_i)\qquad\quad(å¼2) \end{align} $</p><p>ä¸Šå¼ä¸­ï¼Œæ˜¯è¾“å…¥é—¨çš„æƒé‡çŸ©é˜µï¼Œæ˜¯è¾“å…¥é—¨çš„åç½®é¡¹ã€‚ä¸‹å›¾è¡¨ç¤ºäº†è¾“å…¥é—¨çš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-89529fa23d9c8a7d.png)</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—ç”¨äºæè¿°å½“å‰è¾“å…¥çš„å•å…ƒçŠ¶æ€$\mathbf{\tilde{c}}_t$ï¼Œå®ƒæ˜¯æ ¹æ®ä¸Šä¸€æ¬¡çš„è¾“å‡ºå’Œæœ¬æ¬¡è¾“å…¥æ¥è®¡ç®—çš„ï¼š</p><p>$\begin{align} \mathbf{\tilde{c}}_t=\tanh(W_c\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_c)\qquad\quad(å¼3) \end{align} $</p><p>ä¸‹å›¾æ˜¯$\mathbf{\tilde{c}}_t$çš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-73a0246cafc1d10d.png)</p><p>ç°åœ¨ï¼Œæˆ‘ä»¬è®¡ç®—å½“å‰æ—¶åˆ»çš„å•å…ƒçŠ¶æ€$\mathbf{\tilde{c}}_t$ã€‚å®ƒæ˜¯ç”±ä¸Šä¸€æ¬¡çš„å•å…ƒçŠ¶æ€$\mathbf{\tilde{c}}_{t-1}$æŒ‰å…ƒç´ ä¹˜ä»¥é—å¿˜é—¨$f_t$ï¼Œå†ç”¨å½“å‰è¾“å…¥çš„å•å…ƒçŠ¶æ€$\mathbf{\tilde{c}}_t$æŒ‰å…ƒç´ ä¹˜ä»¥è¾“å…¥é—¨$i_t$ï¼Œå†å°†ä¸¤ä¸ªç§¯åŠ å’Œäº§ç”Ÿçš„ï¼š</p><p>$\begin{align} \mathbf{c}_t=f_t\circ{\mathbf{c}_{t-1}}+i_t\circ{\mathbf{\tilde{c}}_t}\qquad\quad(å¼4) \end{align} $</p><p>ç¬¦å·è¡¨ç¤º<strong>æŒ‰å…ƒç´ ä¹˜</strong>ã€‚ä¸‹å›¾æ˜¯$\mathbf{c}_t$çš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-5c766f3d734334b1.png)</p><p>è¿™æ ·ï¼Œæˆ‘ä»¬å°±æŠŠLSTMå…³äºå½“å‰çš„è®°å¿†$\mathbf{\tilde{c}}_t$å’Œé•¿æœŸçš„è®°å¿†$\mathbf{\tilde{c}}_{t-1}$ç»„åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆäº†æ–°çš„å•å…ƒçŠ¶æ€$\mathbf{\tilde{c}}_t$ã€‚ç”±äºé—å¿˜é—¨çš„æ§åˆ¶ï¼Œå®ƒå¯ä»¥ä¿å­˜å¾ˆä¹…å¾ˆä¹…ä¹‹å‰çš„ä¿¡æ¯ï¼Œç”±äºè¾“å…¥é—¨çš„æ§åˆ¶ï¼Œå®ƒåˆå¯ä»¥é¿å…å½“å‰æ— å…³ç´§è¦çš„å†…å®¹è¿›å…¥è®°å¿†ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬è¦çœ‹çœ‹è¾“å‡ºé—¨ï¼Œå®ƒæ§åˆ¶äº†é•¿æœŸè®°å¿†å¯¹å½“å‰è¾“å‡ºçš„å½±å“ï¼š</p><p>$\begin{align} \mathbf{o}_t=\sigma(W_o\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_o)\qquad\quad(å¼5)\end{align} $</p><p>ä¸‹å›¾è¡¨ç¤ºè¾“å‡ºé—¨çš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-fd4d91d1b68b3759.png)</p><p>LSTMæœ€ç»ˆçš„è¾“å‡ºï¼Œæ˜¯ç”±è¾“å‡ºé—¨å’Œå•å…ƒçŠ¶æ€å…±åŒç¡®å®šçš„ï¼š</p><p>$\begin{align}\mathbf{h}_t=\mathbf{o}_t\circ \tanh(\mathbf{c}_t)\qquad\quad(å¼6)\end{align} $</p><p>ä¸‹å›¾è¡¨ç¤ºLSTMæœ€ç»ˆè¾“å‡ºçš„è®¡ç®—ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-7ea82e4f1ac6cd75.png)</p><p><strong>å¼1</strong>åˆ°<strong>å¼6</strong>å°±æ˜¯LSTMå‰å‘è®¡ç®—çš„å…¨éƒ¨å…¬å¼ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬å°±æŠŠLSTMå‰å‘è®¡ç®—è®²å®Œäº†ã€‚</p><h2 id="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„è®­ç»ƒ"><a href="#é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„è®­ç»ƒ" class="headerlink" title="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„è®­ç»ƒ"></a>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„è®­ç»ƒ</h2><p>ç†Ÿæ‚‰æˆ‘ä»¬è¿™ä¸ªç³»åˆ—æ–‡ç« çš„åŒå­¦éƒ½æ¸…æ¥šï¼Œè®­ç»ƒéƒ¨åˆ†å¾€å¾€æ¯”å‰å‘è®¡ç®—éƒ¨åˆ†å¤æ‚å¤šäº†ã€‚LSTMçš„å‰å‘è®¡ç®—éƒ½è¿™ä¹ˆå¤æ‚ï¼Œé‚£ä¹ˆï¼Œå¯æƒ³è€ŒçŸ¥ï¼Œå®ƒçš„è®­ç»ƒç®—æ³•ä¸€å®šæ˜¯éå¸¸éå¸¸å¤æ‚çš„ã€‚ç°åœ¨åªæœ‰åšå‡ æ¬¡æ·±å‘¼å¸ï¼Œå†ä¸€å¤´æ‰è¿›å…¬å¼æµ·æ´‹å§ã€‚</p><h3 id="LSTMè®­ç»ƒç®—æ³•æ¡†æ¶"><a href="#LSTMè®­ç»ƒç®—æ³•æ¡†æ¶" class="headerlink" title="LSTMè®­ç»ƒç®—æ³•æ¡†æ¶"></a>LSTMè®­ç»ƒç®—æ³•æ¡†æ¶</h3><p>LSTMçš„è®­ç»ƒç®—æ³•ä»ç„¶æ˜¯åå‘ä¼ æ’­ç®—æ³•ï¼Œå¯¹äºè¿™ä¸ªç®—æ³•ï¼Œæˆ‘ä»¬å·²ç»éå¸¸ç†Ÿæ‚‰äº†ã€‚ä¸»è¦æœ‰ä¸‹é¢ä¸‰ä¸ªæ­¥éª¤ï¼š</p><ol><li>å‰å‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå€¼ï¼Œå¯¹äºLSTMæ¥è¯´ï¼Œå³$\mathbf{f}_t$ã€$\mathbf{i}_t$ã€$\mathbf{c}_t$ã€$\mathbf{o}_t$ã€$\mathbf{h}_t$äº”ä¸ªå‘é‡çš„å€¼ã€‚è®¡ç®—æ–¹æ³•å·²ç»åœ¨ä¸Šä¸€èŠ‚ä¸­æè¿°è¿‡äº†ã€‚</li><li>åå‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„<strong>è¯¯å·®é¡¹</strong>$\sigma $å€¼ã€‚ä¸<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>ä¸€æ ·ï¼ŒLSTMè¯¯å·®é¡¹çš„åå‘ä¼ æ’­ä¹Ÿæ˜¯åŒ…æ‹¬ä¸¤ä¸ªæ–¹å‘ï¼šä¸€ä¸ªæ˜¯æ²¿æ—¶é—´çš„åå‘ä¼ æ’­ï¼Œå³ä»å½“å‰tæ—¶åˆ»å¼€å§‹ï¼Œè®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹ï¼›ä¸€ä¸ªæ˜¯å°†è¯¯å·®é¡¹å‘ä¸Šä¸€å±‚ä¼ æ’­ã€‚</li><li>æ ¹æ®ç›¸åº”çš„è¯¯å·®é¡¹ï¼Œè®¡ç®—æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ã€‚</li></ol><h3 id="å…³äºå…¬å¼å’Œç¬¦å·çš„è¯´æ˜"><a href="#å…³äºå…¬å¼å’Œç¬¦å·çš„è¯´æ˜" class="headerlink" title="å…³äºå…¬å¼å’Œç¬¦å·çš„è¯´æ˜"></a>å…³äºå…¬å¼å’Œç¬¦å·çš„è¯´æ˜</h3><p>é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹æ¨å¯¼ä¸­ç”¨åˆ°çš„ä¸€äº›å…¬å¼ã€ç¬¦å·åšä¸€ä¸‹å¿…è¦çš„è¯´æ˜ã€‚</p><p>æ¥ä¸‹æ¥çš„æ¨å¯¼ä¸­ï¼Œæˆ‘ä»¬è®¾å®šgateçš„æ¿€æ´»å‡½æ•°ä¸ºsigmoidå‡½æ•°ï¼Œè¾“å‡ºçš„æ¿€æ´»å‡½æ•°ä¸ºtanhå‡½æ•°ã€‚ä»–ä»¬çš„å¯¼æ•°åˆ†åˆ«ä¸ºï¼š</p><p>$\begin{align} \sigma(z)&amp;=y=\frac{1}{1+e^{-z}}\\ \sigmaâ€™(z)&amp;=y(1-y)\\ \tanh(z)&amp;=y=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\ \tanhâ€™(z)&amp;=1-y^2 \end{align} $</p><p>ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œsigmoidå’Œtanhå‡½æ•°çš„å¯¼æ•°éƒ½æ˜¯åŸå‡½æ•°çš„å‡½æ•°ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬ä¸€æ—¦è®¡ç®—åŸå‡½æ•°çš„å€¼ï¼Œå°±å¯ä»¥ç”¨å®ƒæ¥è®¡ç®—å‡ºå¯¼æ•°çš„å€¼ã€‚</p><p>LSTMéœ€è¦å­¦ä¹ çš„å‚æ•°å…±æœ‰8ç»„ï¼Œåˆ†åˆ«æ˜¯ï¼šé—å¿˜é—¨çš„æƒé‡çŸ©é˜µ$W_f$å’Œåç½®é¡¹$\mathbf{b}_f$ã€è¾“å…¥é—¨çš„æƒé‡çŸ©é˜µ$W_i$å’Œåç½®é¡¹$\mathbf{b}_i$ã€è¾“å‡ºé—¨çš„æƒé‡çŸ©é˜µ$W_o$å’Œåç½®é¡¹$\mathbf{b}_o$ï¼Œä»¥åŠè®¡ç®—å•å…ƒçŠ¶æ€çš„æƒé‡çŸ©é˜µ$W_c$å’Œåç½®é¡¹$\mathbf{b}_c$ã€‚å› ä¸ºæƒé‡çŸ©é˜µçš„ä¸¤éƒ¨åˆ†åœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨ä¸åŒçš„å…¬å¼ï¼Œå› æ­¤åœ¨åç»­çš„æ¨å¯¼ä¸­ï¼Œæƒé‡çŸ©é˜µ$W_f$ã€$W_i$ã€$W_c$ã€$W_o$éƒ½å°†è¢«å†™ä¸ºåˆ†å¼€çš„ä¸¤ä¸ªçŸ©é˜µï¼š$W_{fh}$ã€$W_{fx}$ã€$W_{ih}$ã€$W_{ix}$ã€$W_{oh}$ã€$W_{ox}$ã€$W_{ch}$ã€$W_{cx}$ã€‚</p><p>æˆ‘ä»¬è§£é‡Šä¸€ä¸‹æŒ‰å…ƒç´ ä¹˜$\circ$ç¬¦å·ã€‚å½“$\circ$ä½œç”¨äºä¸¤ä¸ª<strong>å‘é‡</strong>æ—¶ï¼Œè¿ç®—å¦‚ä¸‹ï¼š</p><p>$\begin{align}\mathbf{a}\circ\mathbf{b}=\begin{bmatrix} a_1\\a_2\\a_3\...\\a_n \end{bmatrix}\circ\begin{bmatrix} b_1\\b_2\\b_3\...\\b_n \end{bmatrix}=\begin{bmatrix} a_1b_1\\a_2b_2\\a_3b_3\...\\a_nb_n \end{bmatrix} \end{align} $</p><p>å½“ä½œç”¨äºä¸€ä¸ª<strong>å‘é‡</strong>å’Œä¸€ä¸ª<strong>çŸ©é˜µ</strong>æ—¶ï¼Œè¿ç®—å¦‚ä¸‹ï¼š</p><p>$\begin{align} \mathbf{a}\circ X&amp;=\begin{bmatrix} a_1\\a_2\\a_3\...\\a_n \end{bmatrix}\circ\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; â€¦ &amp; x_{1n}\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; â€¦ &amp; x_{2n}\\ x_{31} &amp; x_{32} &amp; x_{33} &amp; â€¦ &amp; x_{3n}\\ &amp; &amp; â€¦\\ x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; â€¦ &amp; x_{nn}\\ \end{bmatrix}\\ &amp;=\begin{bmatrix} a_1x_{11} &amp; a_1x_{12} &amp; a_1x_{13} &amp; â€¦ &amp; a_1x_{1n}\\ a_2x_{21} &amp; a_2x_{22} &amp; a_2x_{23} &amp; â€¦ &amp; a_2x_{2n}\\ a_3x_{31} &amp; a_3x_{32} &amp; a_3x_{33} &amp; â€¦ &amp; a_3x_{3n}\\ &amp; &amp; â€¦\\ a_nx_{n1} &amp; a_nx_{n2} &amp; a_nx_{n3} &amp; â€¦ &amp; a_nx_{nn}\\ \end{bmatrix} \end{align} $</p><p>å½“ä½œç”¨äºä¸¤ä¸ª<strong>çŸ©é˜µ</strong>æ—¶ï¼Œä¸¤ä¸ªçŸ©é˜µå¯¹åº”ä½ç½®çš„å…ƒç´ ç›¸ä¹˜ã€‚æŒ‰å…ƒç´ ä¹˜å¯ä»¥åœ¨æŸäº›æƒ…å†µä¸‹ç®€åŒ–çŸ©é˜µå’Œå‘é‡è¿ç®—ã€‚ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªå¯¹è§’çŸ©é˜µå³ä¹˜ä¸€ä¸ªçŸ©é˜µæ—¶ï¼Œç›¸å½“äºç”¨å¯¹è§’çŸ©é˜µçš„å¯¹è§’çº¿ç»„æˆçš„å‘é‡æŒ‰å…ƒç´ ä¹˜é‚£ä¸ªçŸ©é˜µï¼š</p><p>$\begin{align} diag[\mathbf{a}]X=\mathbf{a}\circ X \end{align}$</p><p>å½“ä¸€ä¸ªè¡Œå‘é‡å³ä¹˜ä¸€ä¸ªå¯¹è§’çŸ©é˜µæ—¶ï¼Œç›¸å½“äºè¿™ä¸ªè¡Œå‘é‡æŒ‰å…ƒç´ ä¹˜é‚£ä¸ªçŸ©é˜µå¯¹è§’çº¿ç»„æˆçš„å‘é‡ï¼š</p><p>$\begin{align} \mathbf{a}^Tdiag[\mathbf{b}]=\mathbf{a}\circ\mathbf{b} \end{align} $</p><p>ä¸Šé¢è¿™ä¸¤ç‚¹ï¼Œåœ¨æˆ‘ä»¬åç»­æ¨å¯¼ä¸­ä¼šå¤šæ¬¡ç”¨åˆ°ã€‚</p><p>åœ¨tæ—¶åˆ»ï¼ŒLSTMçš„è¾“å‡ºå€¼ä¸º$\mathbf{h}_t$ã€‚æˆ‘ä»¬å®šä¹‰tæ—¶åˆ»çš„è¯¯å·®é¡¹$\delta_t$ä¸ºï¼š</p><p>$\begin{align}\delta_t\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{h}_t}} \end{align} $</p><p>æ³¨æ„ï¼Œå’Œå‰é¢å‡ ç¯‡æ–‡ç« ä¸åŒï¼Œæˆ‘ä»¬è¿™é‡Œå‡è®¾è¯¯å·®é¡¹æ˜¯æŸå¤±å‡½æ•°å¯¹è¾“å‡ºå€¼çš„å¯¼æ•°ï¼Œè€Œä¸æ˜¯å¯¹åŠ æƒè¾“å…¥$net_t^l$çš„å¯¼æ•°ã€‚å› ä¸ºLSTMæœ‰å››ä¸ªåŠ æƒè¾“å…¥ï¼Œåˆ†åˆ«å¯¹åº”$\mathbf{f}_t$ã€$\mathbf{i}_t$ã€$\mathbf{c}_t$ã€$\mathbf{o}_t$ï¼Œæˆ‘ä»¬å¸Œæœ›å¾€ä¸Šä¸€å±‚ä¼ é€’ä¸€ä¸ªè¯¯å·®é¡¹è€Œä¸æ˜¯å››ä¸ªã€‚ä½†æˆ‘ä»¬ä»ç„¶éœ€è¦å®šä¹‰å‡ºè¿™å››ä¸ªåŠ æƒè¾“å…¥ï¼Œä»¥åŠä»–ä»¬å¯¹åº”çš„è¯¯å·®é¡¹ã€‚</p><p>$\begin{align} \mathbf{net}_{f,t}&amp;=W_f[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_f\\ &amp;=W_{fh}\mathbf{h}_{t-1}+W_{fx}\mathbf{x}_t+\mathbf{b}_f\\ \mathbf{net}_{i,t}&amp;=W_i[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_i\\ &amp;=W_{ih}\mathbf{h}_{t-1}+W_{ix}\mathbf{x}_t+\mathbf{b}_i\\ \mathbf{net}_{\tilde{c},t}&amp;=W_c[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_c\\ &amp;=W_{ch}\mathbf{h}_{t-1}+W_{cx}\mathbf{x}_t+\mathbf{b}_c\\ \mathbf{net}_{o,t}&amp;=W_o[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_o\\ &amp;=W_{oh}\mathbf{h}_{t-1}+W_{ox}\mathbf{x}_t+\mathbf{b}_o\\ \delta_{f,t}&amp;\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{net}_{f,t}}}\\ \delta_{i,t}&amp;\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{net}_{i,t}}}\\ \delta_{\tilde{c},t}&amp;\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{net}_{\tilde{c},t}}}\\ \delta_{o,t}&amp;\overset{def}{=}\frac{\partial{E}}{\partial{\mathbf{net}_{o,t}}}\\ \end{align} $</p><h3 id="è¯¯å·®é¡¹æ²¿æ—¶é—´çš„åå‘ä¼ é€’"><a href="#è¯¯å·®é¡¹æ²¿æ—¶é—´çš„åå‘ä¼ é€’" class="headerlink" title="è¯¯å·®é¡¹æ²¿æ—¶é—´çš„åå‘ä¼ é€’"></a>è¯¯å·®é¡¹æ²¿æ—¶é—´çš„åå‘ä¼ é€’</h3><p>æ²¿æ—¶é—´åå‘ä¼ é€’è¯¯å·®é¡¹ï¼Œå°±æ˜¯è¦è®¡ç®—å‡ºt-1æ—¶åˆ»çš„è¯¯å·®é¡¹$\sigma_{t-1}$ã€‚</p><p>$\begin{align} \delta_{t-1}^T&amp;=\frac{\partial{E}}{\partial{\mathbf{h_{t-1}}}}\\ &amp;=\frac{\partial{E}}{\partial{\mathbf{h_t}}}\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{h_{t-1}}}}\\ &amp;=\delta_{t}^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{h_{t-1}}}} \end{align} $</p><p>æˆ‘ä»¬çŸ¥é“ï¼Œ$\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{h_{t-1}}}}$æ˜¯ä¸€ä¸ªJacobiançŸ©é˜µã€‚å¦‚æœéšè—å±‚hçš„ç»´åº¦æ˜¯Nçš„è¯ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ª$N\times N$çŸ©é˜µã€‚ä¸ºäº†æ±‚å‡ºå®ƒï¼Œæˆ‘ä»¬åˆ—å‡º$\mathbf{h}_t$çš„è®¡ç®—å…¬å¼ï¼Œå³å‰é¢çš„<strong>å¼6</strong>å’Œ<strong>å¼4</strong>ï¼š</p><p>$\begin{align} \mathbf{h}_t&amp;=\mathbf{o}_t\circ \tanh(\mathbf{c}_t)\\ \mathbf{c}_t&amp;=\mathbf{f}_t\circ\mathbf{c}_{t-1}+\mathbf{i}_t\circ\mathbf{\tilde{c}}_t \end{align} $</p><p>æ˜¾ç„¶ï¼Œ$\mathbf{o}_t$ã€$\mathbf{f}_t$ã€$\mathbf{i}_t$ã€$\mathbf{\tilde{c}}_t$éƒ½æ˜¯$\mathbf{h}_{t-1}$çš„å‡½æ•°ï¼Œé‚£ä¹ˆï¼Œåˆ©ç”¨å…¨å¯¼æ•°å…¬å¼å¯å¾—ï¼š</p><p>$\begin{align} \delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{h_{t-1}}}}&amp;=\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{o}_t}}\frac{\partial{\mathbf{o}_t}}{\partial{\mathbf{net}_{o,t}}}\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{f_{t}}}}\frac{\partial{\mathbf{f}_t}}{\partial{\mathbf{net}_{f,t}}}\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{i_{t}}}}\frac{\partial{\mathbf{i}_t}}{\partial{\mathbf{net}_{i,t}}}\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_t^T\frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}\frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{\tilde{c}}_{t}}}\frac{\partial{\mathbf{\tilde{c}}_t}}{\partial{\mathbf{net}_{\tilde{c},t}}}\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}}\\ &amp;=\delta_{o,t}^T\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{f,t}^T\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{i,t}^T\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{\tilde{c},t}^T\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}}\qquad\quad(å¼7) \end{align} $</p><p>ä¸‹é¢ï¼Œæˆ‘ä»¬è¦æŠŠ<strong>å¼7</strong>ä¸­çš„æ¯ä¸ªåå¯¼æ•°éƒ½æ±‚å‡ºæ¥ã€‚æ ¹æ®<strong>å¼6</strong>ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚å‡ºï¼š</p><p>$\begin{align} \frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{o}_t}}&amp;=diag[\tanh(\mathbf{c}_t)]\\ \frac{\partial{\mathbf{h_t}}}{\partial{\mathbf{c}_t}}&amp;=diag[\mathbf{o}_t\circ(1-\tanh(\mathbf{c}_t)^2)] \end{align} $</p><p>æ ¹æ®<strong>å¼4</strong>ï¼Œæˆ‘ä»¬å¯ä»¥æ±‚å‡ºï¼š</p><p>$\begin{align} \frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{f_{t}}}}&amp;=diag[\mathbf{c}_{t-1}]\\ \frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{i_{t}}}}&amp;=diag[\mathbf{\tilde{c}}_t]\\ \frac{\partial{\mathbf{c}_t}}{\partial{\mathbf{\tilde{c}_{t}}}}&amp;=diag[\mathbf{i}_t]\\ \end{align} $</p><p>å› ä¸ºï¼š</p><p>$\begin{align} \mathbf{o}_t&amp;=\sigma(\mathbf{net}_{o,t})\\ \mathbf{net}_{o,t}&amp;=W_{oh}\mathbf{h}_{t-1}+W_{ox}\mathbf{x}_t+\mathbf{b}_o\\\\ \mathbf{f}_t&amp;=\sigma(\mathbf{net}_{f,t})\\ \mathbf{net}_{f,t}&amp;=W_{fh}\mathbf{h}_{t-1}+W_{fx}\mathbf{x}_t+\mathbf{b}_f\\\\ \mathbf{i}_t&amp;=\sigma(\mathbf{net}_{i,t})\\ \mathbf{net}_{i,t}&amp;=W_{ih}\mathbf{h}_{t-1}+W_{ix}\mathbf{x}_t+\mathbf{b}_i\\\\ \mathbf{\tilde{c}}_t&amp;=\tanh(\mathbf{net}_{\tilde{c},t})\\ \mathbf{net}_{\tilde{c},t}&amp;=W_{ch}\mathbf{h}_{t-1}+W_{cx}\mathbf{x}_t+\mathbf{b}_c\\ \end{align} $</p><p>æˆ‘ä»¬å¾ˆå®¹æ˜“å¾—å‡ºï¼š</p><p>$\begin{align} \frac{\partial{\mathbf{o}_t}}{\partial{\mathbf{net}_{o,t}}}&amp;=diag[\mathbf{o}_t\circ(1-\mathbf{o}_t)]\\ \frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}}&amp;=W_{oh}\\ \frac{\partial{\mathbf{f}_t}}{\partial{\mathbf{net}_{f,t}}}&amp;=diag[\mathbf{f}_t\circ(1-\mathbf{f}_t)]\\ \frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h}_{t-1}}}&amp;=W_{fh}\\ \frac{\partial{\mathbf{i}_t}}{\partial{\mathbf{net}_{i,t}}}&amp;=diag[\mathbf{i}_t\circ(1-\mathbf{i}_t)]\\ \frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h}_{t-1}}}&amp;=W_{ih}\\ \frac{\partial{\mathbf{\tilde{c}}_t}}{\partial{\mathbf{net}_{\tilde{c},t}}}&amp;=diag[1-\mathbf{\tilde{c}}_t^2]\\ \frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h}_{t-1}}}&amp;=W_{ch} \end{align} $</p><p>å°†ä¸Šè¿°åå¯¼æ•°å¸¦å…¥åˆ°<strong>å¼7</strong>ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p><p>$\begin{align} \delta_{t-1}&amp;=\delta_{o,t}^T\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{f,t}^T\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{i,t}^T\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{h_{t-1}}}} +\delta_{\tilde{c},t}^T\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{h_{t-1}}}}\\ &amp;=\delta_{o,t}^T W_{oh} +\delta_{f,t}^TW_{fh} +\delta_{i,t}^TW_{ih} +\delta_{\tilde{c},t}^TW_{ch}\qquad\quad(å¼8)\\ \end{align} $</p><p>æ ¹æ®$\delta_{o,t}$ã€$\delta_{f,t}$ã€$\delta_{i,t}$ã€$\delta_{\tilde{c},t}$çš„å®šä¹‰ï¼Œå¯çŸ¥ï¼š</p><p>$\begin{align} \delta_{o,t}^T&amp;=\delta_t^T\circ\tanh(\mathbf{c}_t)\circ\mathbf{o}_t\circ(1-\mathbf{o}_t)\qquad\quad(å¼9)\\ \delta_{f,t}^T&amp;=\delta_t^T\circ\mathbf{o}_t\circ(1-\tanh(\mathbf{c}_t)^2)\circ\mathbf{c}_{t-1}\circ\mathbf{f}_t\circ(1-\mathbf{f}_t)\qquad(å¼10)\\ \delta_{i,t}^T&amp;=\delta_t^T\circ\mathbf{o}_t\circ(1-\tanh(\mathbf{c}_t)^2)\circ\mathbf{\tilde{c}}_t\circ\mathbf{i}_t\circ(1-\mathbf{i}_t)\qquad\quad(å¼11)\\ \delta_{\tilde{c},t}^T&amp;=\delta_t^T\circ\mathbf{o}_t\circ(1-\tanh(\mathbf{c}_t)^2)\circ\mathbf{i}_t\circ(1-\mathbf{\tilde{c}}^2)\qquad\quad(å¼12)\\ \end{align} $</p><p><strong>å¼8</strong>åˆ°<strong>å¼12</strong>å°±æ˜¯å°†è¯¯å·®æ²¿æ—¶é—´åå‘ä¼ æ’­ä¸€ä¸ªæ—¶åˆ»çš„å…¬å¼ã€‚æœ‰äº†å®ƒï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºå°†è¯¯å·®é¡¹å‘å‰ä¼ é€’åˆ°ä»»æ„kæ—¶åˆ»çš„å…¬å¼ï¼š</p><p>$\begin{align} \delta_k^T=\prod_{j=k}^{t-1}\delta_{o,j}^TW_{oh} +\delta_{f,j}^TW_{fh} +\delta_{i,j}^TW_{ih} +\delta_{\tilde{c},j}^TW_{ch}\qquad\quad(å¼13) \end{align}$</p><h3 id="å°†è¯¯å·®é¡¹ä¼ é€’åˆ°ä¸Šä¸€å±‚"><a href="#å°†è¯¯å·®é¡¹ä¼ é€’åˆ°ä¸Šä¸€å±‚" class="headerlink" title="å°†è¯¯å·®é¡¹ä¼ é€’åˆ°ä¸Šä¸€å±‚"></a>å°†è¯¯å·®é¡¹ä¼ é€’åˆ°ä¸Šä¸€å±‚</h3><p>æˆ‘ä»¬å‡è®¾å½“å‰ä¸ºç¬¬lå±‚ï¼Œå®šä¹‰l-1å±‚çš„è¯¯å·®é¡¹æ˜¯è¯¯å·®å‡½æ•°å¯¹l-1å±‚<strong>åŠ æƒè¾“å…¥</strong>çš„å¯¼æ•°ï¼Œå³ï¼š</p><p>$\begin {align} \delta_t^{l-1}\overset{def}{=}\frac{\partial{E}}{\mathbf{net}_t^{l-1}} \end {align} $</p><p>æœ¬æ¬¡LSTMçš„è¾“å…¥$x_t$ç”±ä¸‹é¢çš„å…¬å¼è®¡ç®—ï¼š</p><p>$\begin {align} \mathbf{x}_t^l=f^{l-1}(\mathbf{net}_t^{l-1}) \end{align} $</p><p>ä¸Šå¼ä¸­ï¼Œ$f^{l-1}$è¡¨ç¤ºç¬¬l-1å±‚çš„<strong>æ¿€æ´»å‡½æ•°</strong>ã€‚</p><p>å› ä¸º$\mathbf{net}_{f,t}^l$ã€$\mathbf{net}_{i,t}^l$ã€$\mathbf{net}_{\tilde{c},t}^l$ã€$\mathbf{net}_{o,t}^l$éƒ½æ˜¯$\mathbf{x}_t$çš„å‡½æ•°ï¼Œ$\mathbf{x}_t$åˆæ˜¯$\mathbf{net}_t^{l-1}$çš„å‡½æ•°ï¼Œå› æ­¤ï¼Œè¦æ±‚å‡ºEå¯¹$\mathbf{net}_t^{l-1}$çš„å¯¼æ•°ï¼Œå°±éœ€è¦ä½¿ç”¨å…¨å¯¼æ•°å…¬å¼ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{\mathbf{net}_t^{l-1}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{\mathbf{net}_{f,t}^l}}}\frac{\partial{\mathbf{\mathbf{net}_{f,t}^l}}}{\partial{\mathbf{x}_t^l}}\frac{\partial{\mathbf{x}_t^l}}{\partial{\mathbf{\mathbf{net}_t^{l-1}}}} +\frac{\partial{E}}{\partial{\mathbf{\mathbf{net}_{i,t}^l}}}\frac{\partial{\mathbf{\mathbf{net}_{i,t}^l}}}{\partial{\mathbf{x}_t^l}}\frac{\partial{\mathbf{x}_t^l}}{\partial{\mathbf{\mathbf{net}_t^{l-1}}}} +\frac{\partial{E}}{\partial{\mathbf{\mathbf{net}_{\tilde{c},t}^l}}}\frac{\partial{\mathbf{\mathbf{net}_{\tilde{c},t}^l}}}{\partial{\mathbf{x}_t^l}}\frac{\partial{\mathbf{x}_t^l}}{\partial{\mathbf{\mathbf{net}_t^{l-1}}}} +\frac{\partial{E}}{\partial{\mathbf{\mathbf{net}_{o,t}^l}}}\frac{\partial{\mathbf{\mathbf{net}_{o,t}^l}}}{\partial{\mathbf{x}_t^l}}\frac{\partial{\mathbf{x}_t^l}}{\partial{\mathbf{\mathbf{net}_t^{l-1}}}}\\ &amp;=\delta_{f,t}^TW_{fx}\circ fâ€™(\mathbf{net}_t^{l-1})+\delta_{i,t}^TW_{ix}\circ fâ€™(\mathbf{net}_t^{l-1})+\delta_{\tilde{c},t}^TW_{cx}\circ fâ€™(\mathbf{net}_t^{l-1})+\delta_{o,t}^TW_{ox}\circ fâ€™(\mathbf{net}_t^{l-1})\\ &amp;=(\delta_{f,t}^TW_{fx}+\delta_{i,t}^TW_{ix}+\delta_{\tilde{c},t}^TW_{cx}+\delta_{o,t}^TW_{ox})\circ fâ€™(\mathbf{net}_t^{l-1})\qquad\quad(å¼14) \end{align} $</p><p><strong>å¼14</strong>å°±æ˜¯å°†è¯¯å·®ä¼ é€’åˆ°ä¸Šä¸€å±‚çš„å…¬å¼ã€‚</p><h3 id="æƒé‡æ¢¯åº¦çš„è®¡ç®—"><a href="#æƒé‡æ¢¯åº¦çš„è®¡ç®—" class="headerlink" title="æƒé‡æ¢¯åº¦çš„è®¡ç®—"></a>æƒé‡æ¢¯åº¦çš„è®¡ç®—</h3><p>å¯¹äº$W_{fh}$ã€$W_{ih}$ã€$W_{ch}$ã€$W_{oh}$çš„æƒé‡æ¢¯åº¦ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒçš„æ¢¯åº¦æ˜¯å„ä¸ªæ—¶åˆ»æ¢¯åº¦ä¹‹å’Œï¼ˆè¯æ˜è¿‡ç¨‹è¯·å‚è€ƒæ–‡ç« <a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">é›¶åŸºç¡€å…¥é—¨æ·±åº¦å­¦ä¹ (5) - å¾ªç¯ç¥ç»ç½‘ç»œ</a>ï¼‰ï¼Œæˆ‘ä»¬é¦–å…ˆæ±‚å‡ºå®ƒä»¬åœ¨tæ—¶åˆ»çš„æ¢¯åº¦ï¼Œç„¶åå†æ±‚å‡ºä»–ä»¬æœ€ç»ˆçš„æ¢¯åº¦ã€‚</p><p>æˆ‘ä»¬å·²ç»æ±‚å¾—äº†è¯¯å·®é¡¹$\delta_{o,t}$ã€$\delta_{f,t}$ã€$\delta_{i,t}$ã€$\delta_{\tilde{c},t}$ï¼Œå¾ˆå®¹æ˜“æ±‚å‡ºtæ—¶åˆ»çš„$W_{oh}$ã€$W_{ih}$ã€$W_{fh}$ã€$W_{ch}$çš„ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{W_{oh,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{o,t}}}\frac{\partial{\mathbf{net}_{o,t}}}{\partial{W_{oh,t}}}\\ &amp;=\delta_{o,t}\mathbf{h}_{t-1}^T\\\\ \frac{\partial{E}}{\partial{W_{fh,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{f,t}}}\frac{\partial{\mathbf{net}_{f,t}}}{\partial{W_{fh,t}}}\\ &amp;=\delta_{f,t}\mathbf{h}_{t-1}^T\\\\ \frac{\partial{E}}{\partial{W_{ih,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{i,t}}}\frac{\partial{\mathbf{net}_{i,t}}}{\partial{W_{ih,t}}}\\ &amp;=\delta_{i,t}\mathbf{h}_{t-1}^T\\\\ \frac{\partial{E}}{\partial{W_{ch,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{\tilde{c},t}}}\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{W_{ch,t}}}\\ &amp;=\delta_{\tilde{c},t}\mathbf{h}_{t-1}^T\\ \end{align} $</p><p>å°†å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦åŠ åœ¨ä¸€èµ·ï¼Œå°±èƒ½å¾—åˆ°æœ€ç»ˆçš„æ¢¯åº¦ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{W_{oh}}}&amp;=\sum_{j=1}^t\delta_{o,j}\mathbf{h}_{j-1}^T\\ \frac{\partial{E}}{\partial{W_{fh}}}&amp;=\sum_{j=1}^t\delta_{f,j}\mathbf{h}_{j-1}^T\\ \frac{\partial{E}}{\partial{W_{ih}}}&amp;=\sum_{j=1}^t\delta_{i,j}\mathbf{h}_{j-1}^T\\ \frac{\partial{E}}{\partial{W_{ch}}}&amp;=\sum_{j=1}^t\delta_{\tilde{c},j}\mathbf{h}_{j-1}^T\\ \end{align} $</p><p>å¯¹äºåç½®é¡¹$\mathbf{b}_f$ã€$\mathbf{b}_i$ã€$\mathbf{b}_c$ã€$\mathbf{b}_o$çš„æ¢¯åº¦ï¼Œä¹Ÿæ˜¯å°†å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦åŠ åœ¨ä¸€èµ·ã€‚ä¸‹é¢æ˜¯å„ä¸ªæ—¶åˆ»çš„åç½®é¡¹æ¢¯åº¦ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{\mathbf{b}_{o,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{o,t}}}\frac{\partial{\mathbf{net}_{o,t}}}{\partial{\mathbf{b}_{o,t}}}\\ &amp;=\delta_{o,t}\\\\ \frac{\partial{E}}{\partial{\mathbf{b}_{f,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{f,t}}}\frac{\partial{\mathbf{net}_{f,t}}}{\partial{\mathbf{b}_{f,t}}}\\ &amp;=\delta_{f,t}\\\\ \frac{\partial{E}}{\partial{\mathbf{b}_{i,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{i,t}}}\frac{\partial{\mathbf{net}_{i,t}}}{\partial{\mathbf{b}_{i,t}}}\\ &amp;=\delta_{i,t}\\\\ \frac{\partial{E}}{\partial{\mathbf{b}_{c,t}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{\tilde{c},t}}}\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{\mathbf{b}_{c,t}}}\\ &amp;=\delta_{\tilde{c},t}\\ \end{align} $</p><p>ä¸‹é¢æ˜¯æœ€ç»ˆçš„åç½®é¡¹æ¢¯åº¦ï¼Œå³å°†å„ä¸ªæ—¶åˆ»çš„åç½®é¡¹æ¢¯åº¦åŠ åœ¨ä¸€èµ·ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{\mathbf{b}_o}}&amp;=\sum_{j=1}^t\delta_{o,j}\\ \frac{\partial{E}}{\partial{\mathbf{b}_i}}&amp;=\sum_{j=1}^t\delta_{i,j}\\ \frac{\partial{E}}{\partial{\mathbf{b}_f}}&amp;=\sum_{j=1}^t\delta_{f,j}\\ \frac{\partial{E}}{\partial{\mathbf{b}_c}}&amp;=\sum_{j=1}^t\delta_{\tilde{c},j}\\ \end{align} $</p><p>å¯¹äº$W_{fx}$ã€$W_{ix}$ã€$W_{cx}$ã€$W_{ox}$çš„æƒé‡æ¢¯åº¦ï¼Œåªéœ€è¦æ ¹æ®ç›¸åº”çš„è¯¯å·®é¡¹ç›´æ¥è®¡ç®—å³å¯ï¼š</p><p>$\begin{align} \frac{\partial{E}}{\partial{W_{ox}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{o,t}}}\frac{\partial{\mathbf{net}_{o,t}}}{\partial{W_{ox}}}\\ &amp;=\delta_{o,t}\mathbf{x}_{t}^T\\\\ \frac{\partial{E}}{\partial{W_{fx}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{f,t}}}\frac{\partial{\mathbf{net}_{f,t}}}{\partial{W_{fx}}}\\ &amp;=\delta_{f,t}\mathbf{x}_{t}^T\\\\ \frac{\partial{E}}{\partial{W_{ix}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{i,t}}}\frac{\partial{\mathbf{net}_{i,t}}}{\partial{W_{ix}}}\\ &amp;=\delta_{i,t}\mathbf{x}_{t}^T\\\\ \frac{\partial{E}}{\partial{W_{cx}}}&amp;=\frac{\partial{E}}{\partial{\mathbf{net}_{\tilde{c},t}}}\frac{\partial{\mathbf{net}_{\tilde{c},t}}}{\partial{W_{cx}}}\\ &amp;=\delta_{\tilde{c},t}\mathbf{x}_{t}^T\\ \end{align} $</p><p>ä»¥ä¸Šå°±æ˜¯LSTMçš„è®­ç»ƒç®—æ³•çš„å…¨éƒ¨å…¬å¼ã€‚å› ä¸ºè¿™é‡Œé¢å­˜åœ¨å¾ˆå¤šé‡å¤çš„æ¨¡å¼ï¼Œä»”ç»†çœ‹çœ‹ï¼Œä¼šå‘è§‰å¹¶ä¸æ˜¯å¤ªå¤æ‚ã€‚</p><p>å½“ç„¶ï¼ŒLSTMå­˜åœ¨ç€ç›¸å½“å¤šçš„å˜ä½“ï¼Œè¯»è€…å¯ä»¥åœ¨äº’è”ç½‘ä¸Šæ‰¾åˆ°å¾ˆå¤šèµ„æ–™ã€‚å› ä¸ºå¤§å®¶å·²ç»ç†Ÿæ‚‰äº†åŸºæœ¬LSTMçš„ç®—æ³•ï¼Œå› æ­¤ç†è§£è¿™äº›å˜ä½“æ¯”è¾ƒå®¹æ˜“ï¼Œå› æ­¤æœ¬æ–‡å°±ä¸å†èµ˜è¿°äº†ã€‚</p><h2 id="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å®ç°"><a href="#é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å®ç°" class="headerlink" title="é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å®ç°"></a>é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„å®ç°</h2><blockquote><p>å®Œæ•´ä»£ç è¯·å‚è€ƒGitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/lstm.py" target="_blank" rel="noopener">https://github.com/hanbt/learn_dl/blob/master/lstm.py</a> (python2.7)</p></blockquote><p>åœ¨ä¸‹é¢çš„å®ç°ä¸­ï¼ŒLSTMLayerçš„å‚æ•°åŒ…æ‹¬è¾“å…¥ç»´åº¦ã€è¾“å‡ºç»´åº¦ã€éšè—å±‚ç»´åº¦ï¼Œå•å…ƒçŠ¶æ€ç»´åº¦ç­‰äºéšè—å±‚ç»´åº¦ã€‚gateçš„æ¿€æ´»å‡½æ•°ä¸ºsigmoidå‡½æ•°ï¼Œè¾“å‡ºçš„æ¿€æ´»å‡½æ•°ä¸ºtanhã€‚</p><h3 id="æ¿€æ´»å‡½æ•°çš„å®ç°"><a href="#æ¿€æ´»å‡½æ•°çš„å®ç°" class="headerlink" title="æ¿€æ´»å‡½æ•°çš„å®ç°"></a>æ¿€æ´»å‡½æ•°çš„å®ç°</h3><p>æˆ‘ä»¬å…ˆå®ç°ä¸¤ä¸ªæ¿€æ´»å‡½æ•°ï¼šsigmoidå’Œtanhã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">class SigmoidActivator(object):    def forward(self, weighted_input):        return 1.0 / (1.0 + np.exp(-weighted_input))    def backward(self, output):        return output * (1 - output)class TanhActivator(object):    def forward(self, weighted_input):        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0    def backward(self, output):        return 1 - output * output</code></pre><h3 id="LSTMåˆå§‹åŒ–"><a href="#LSTMåˆå§‹åŒ–" class="headerlink" title="LSTMåˆå§‹åŒ–"></a>LSTMåˆå§‹åŒ–</h3><p>å’Œå‰ä¸¤ç¯‡æ–‡ç« ä»£ç æ¶æ„ä¸€æ ·ï¼Œæˆ‘ä»¬æŠŠLSTMçš„å®ç°æ”¾åœ¨LstmLayerç±»ä¸­ã€‚</p><p>æ ¹æ®LSTMå‰å‘è®¡ç®—å’Œæ–¹å‘ä¼ æ’­ç®—æ³•ï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–ä¸€ç³»åˆ—çŸ©é˜µå’Œå‘é‡ã€‚è¿™äº›çŸ©é˜µå’Œå‘é‡æœ‰ä¸¤ç±»ç”¨é€”ï¼Œä¸€ç±»æ˜¯ç”¨äºä¿å­˜æ¨¡å‹å‚æ•°ï¼Œä¾‹å¦‚$W_f$ã€$W_i$ã€$W_o$ã€$W_c$ã€$\mathbf{b}_f$ã€$\mathbf{b}_i$ã€$\mathbf{b}_o$ã€$\mathbf{b}_c$ï¼›å¦ä¸€ç±»æ˜¯ä¿å­˜å„ç§ä¸­é—´è®¡ç®—ç»“æœï¼Œä»¥ä¾¿äºåå‘ä¼ æ’­ç®—æ³•ä½¿ç”¨ï¼Œå®ƒä»¬åŒ…æ‹¬$\mathbf{h}_t$ã€$\mathbf{f}_t$ã€$\mathbf{i}_t$ã€$\mathbf{o}_t$ã€$\mathbf{\tilde{c}}_t$ã€$\mathbf{\tilde{c}}_t$ã€$\delta_t$ã€$\delta_{f,t}$ã€$\delta_{i,t}$ã€$\delta_{o,t}$ã€$\delta_{\tilde{c},t}$ï¼Œä»¥åŠå„ä¸ªæƒé‡å¯¹åº”çš„æ¢¯åº¦ã€‚</p><p>åœ¨æ„é€ å‡½æ•°çš„åˆå§‹åŒ–ä¸­ï¼Œåªåˆå§‹åŒ–äº†ä¸forwardè®¡ç®—ç›¸å…³çš„å˜é‡ï¼Œä¸backwardç›¸å…³çš„å˜é‡æ²¡æœ‰åˆå§‹åŒ–ã€‚è¿™æ˜¯å› ä¸ºæ„é€ LSTMå¯¹è±¡çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¿˜ä¸çŸ¥é“å®ƒæœªæ¥æ˜¯ç”¨äºè®­ç»ƒï¼ˆæ—¢æœ‰forwardåˆæœ‰backwardï¼‰è¿˜æ˜¯æ¨ç†ï¼ˆåªæœ‰forwardï¼‰ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">class LstmLayer(object):    def __init__(self, input_width, state_width,                  learning_rate):        self.input_width = input_width        self.state_width = state_width        self.learning_rate = learning_rate        # é—¨çš„æ¿€æ´»å‡½æ•°        self.gate_activator = SigmoidActivator()        # è¾“å‡ºçš„æ¿€æ´»å‡½æ•°        self.output_activator = TanhActivator()        # å½“å‰æ—¶åˆ»åˆå§‹åŒ–ä¸ºt0        self.times = 0               # å„ä¸ªæ—¶åˆ»çš„å•å…ƒçŠ¶æ€å‘é‡c        self.c_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å‡ºå‘é‡h        self.h_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„é—å¿˜é—¨f        self.f_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å…¥é—¨i        self.i_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å‡ºé—¨o        self.o_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„å³æ—¶çŠ¶æ€c~        self.ct_list = self.init_state_vec()        # é—å¿˜é—¨æƒé‡çŸ©é˜µWfh, Wfx, åç½®é¡¹bf        self.Wfh, self.Wfx, self.bf = (            self.init_weight_mat())        # è¾“å…¥é—¨æƒé‡çŸ©é˜µWfh, Wfx, åç½®é¡¹bf        self.Wih, self.Wix, self.bi = (            self.init_weight_mat())        # è¾“å‡ºé—¨æƒé‡çŸ©é˜µWfh, Wfx, åç½®é¡¹bf        self.Woh, self.Wox, self.bo = (            self.init_weight_mat())        # å•å…ƒçŠ¶æ€æƒé‡çŸ©é˜µWfh, Wfx, åç½®é¡¹bf        self.Wch, self.Wcx, self.bc = (            self.init_weight_mat())    def init_state_vec(self):        '''        åˆå§‹åŒ–ä¿å­˜çŠ¶æ€çš„å‘é‡        '''        state_vec_list = []        state_vec_list.append(np.zeros(            (self.state_width, 1)))        return state_vec_list    def init_weight_mat(self):        '''        åˆå§‹åŒ–æƒé‡çŸ©é˜µ        '''        Wh = np.random.uniform(-1e-4, 1e-4,            (self.state_width, self.state_width))        Wx = np.random.uniform(-1e-4, 1e-4,            (self.state_width, self.input_width))        b = np.zeros((self.state_width, 1))        return Wh, Wx, b</code></pre><h3 id="å‰å‘è®¡ç®—çš„å®ç°"><a href="#å‰å‘è®¡ç®—çš„å®ç°" class="headerlink" title="å‰å‘è®¡ç®—çš„å®ç°"></a>å‰å‘è®¡ç®—çš„å®ç°</h3><p>forwardæ–¹æ³•å®ç°äº†LSTMçš„å‰å‘è®¡ç®—ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">        def forward(self, x):        '''        æ ¹æ®å¼1-å¼6è¿›è¡Œå‰å‘è®¡ç®—        '''        self.times += 1        # é—å¿˜é—¨        fg = self.calc_gate(x, self.Wfx, self.Wfh,             self.bf, self.gate_activator)        self.f_list.append(fg)        # è¾“å…¥é—¨        ig = self.calc_gate(x, self.Wix, self.Wih,            self.bi, self.gate_activator)        self.i_list.append(ig)        # è¾“å‡ºé—¨        og = self.calc_gate(x, self.Wox, self.Woh,            self.bo, self.gate_activator)        self.o_list.append(og)        # å³æ—¶çŠ¶æ€        ct = self.calc_gate(x, self.Wcx, self.Wch,            self.bc, self.output_activator)        self.ct_list.append(ct)        # å•å…ƒçŠ¶æ€        c = fg * self.c_list[self.times - 1] + ig * ct        self.c_list.append(c)        # è¾“å‡º        h = og * self.output_activator.forward(c)        self.h_list.append(h)    def calc_gate(self, x, Wx, Wh, b, activator):        '''        è®¡ç®—é—¨        '''        h = self.h_list[self.times - 1] # ä¸Šæ¬¡çš„LSTMè¾“å‡º        net = np.dot(Wh, h) + np.dot(Wx, x) + b        gate = activator.forward(net)        return gate</code></pre><p>ä»ä¸Šé¢çš„ä»£ç æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé—¨çš„è®¡ç®—éƒ½æ˜¯ç›¸åŒçš„ç®—æ³•ï¼Œè€Œé—¨å’Œçš„è®¡ç®—ä»…ä»…æ˜¯æ¿€æ´»å‡½æ•°ä¸åŒã€‚å› æ­¤æˆ‘ä»¬æå‡ºäº†calc_gateæ–¹æ³•ï¼Œè¿™æ ·å‡å°‘äº†å¾ˆå¤šé‡å¤ä»£ç ã€‚</p><h3 id="åå‘ä¼ æ’­ç®—æ³•çš„å®ç°"><a href="#åå‘ä¼ æ’­ç®—æ³•çš„å®ç°" class="headerlink" title="åå‘ä¼ æ’­ç®—æ³•çš„å®ç°"></a>åå‘ä¼ æ’­ç®—æ³•çš„å®ç°</h3><p>backwardæ–¹æ³•å®ç°äº†LSTMçš„åå‘ä¼ æ’­ç®—æ³•ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸backwordç›¸å…³çš„å†…éƒ¨çŠ¶æ€å˜é‡æ˜¯åœ¨è°ƒç”¨backwardæ–¹æ³•ä¹‹åæ‰åˆå§‹åŒ–çš„ã€‚è¿™ç§å»¶è¿Ÿåˆå§‹åŒ–çš„ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œå¦‚æœLSTMåªæ˜¯ç”¨æ¥æ¨ç†ï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦åˆå§‹åŒ–è¿™äº›å˜é‡ï¼ŒèŠ‚çœäº†å¾ˆå¤šå†…å­˜ã€‚</p><pre class=" language-lang-python"><code class="language-lang-python">    def backward(self, x, delta_h, activator):        '''        å®ç°LSTMè®­ç»ƒç®—æ³•        '''        self.calc_delta(delta_h, activator)        self.calc_gradient(x)</code></pre><p>ç®—æ³•ä¸»è¦åˆ†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ä½¿è®¡ç®—è¯¯å·®é¡¹ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">        def calc_delta(self, delta_h, activator):        # åˆå§‹åŒ–å„ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹        self.delta_h_list = self.init_delta()  # è¾“å‡ºè¯¯å·®é¡¹        self.delta_o_list = self.init_delta()  # è¾“å‡ºé—¨è¯¯å·®é¡¹        self.delta_i_list = self.init_delta()  # è¾“å…¥é—¨è¯¯å·®é¡¹        self.delta_f_list = self.init_delta()  # é—å¿˜é—¨è¯¯å·®é¡¹        self.delta_ct_list = self.init_delta() # å³æ—¶è¾“å‡ºè¯¯å·®é¡¹        # ä¿å­˜ä»ä¸Šä¸€å±‚ä¼ é€’ä¸‹æ¥çš„å½“å‰æ—¶åˆ»çš„è¯¯å·®é¡¹        self.delta_h_list[-1] = delta_h        # è¿­ä»£è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹        for k in range(self.times, 0, -1):            self.calc_delta_k(k)    def init_delta(self):        '''        åˆå§‹åŒ–è¯¯å·®é¡¹        '''        delta_list = []        for i in range(self.times + 1):            delta_list.append(np.zeros(                (self.state_width, 1)))        return delta_list    def calc_delta_k(self, k):        '''        æ ¹æ®kæ—¶åˆ»çš„delta_hï¼Œè®¡ç®—kæ—¶åˆ»çš„delta_fã€        delta_iã€delta_oã€delta_ctï¼Œä»¥åŠk-1æ—¶åˆ»çš„delta_h        '''        # è·å¾—kæ—¶åˆ»å‰å‘è®¡ç®—çš„å€¼        ig = self.i_list[k]        og = self.o_list[k]        fg = self.f_list[k]        ct = self.ct_list[k]        c = self.c_list[k]        c_prev = self.c_list[k-1]        tanh_c = self.output_activator.forward(c)        delta_k = self.delta_h_list[k]        # æ ¹æ®å¼9è®¡ç®—delta_o        delta_o = (delta_k * tanh_c *             self.gate_activator.backward(og))        delta_f = (delta_k * og *             (1 - tanh_c * tanh_c) * c_prev *            self.gate_activator.backward(fg))        delta_i = (delta_k * og *             (1 - tanh_c * tanh_c) * ct *            self.gate_activator.backward(ig))        delta_ct = (delta_k * og *             (1 - tanh_c * tanh_c) * ig *            self.output_activator.backward(ct))        delta_h_prev = (                np.dot(delta_o.transpose(), self.Woh) +                np.dot(delta_i.transpose(), self.Wih) +                np.dot(delta_f.transpose(), self.Wfh) +                np.dot(delta_ct.transpose(), self.Wch)            ).transpose()        # ä¿å­˜å…¨éƒ¨deltaå€¼        self.delta_h_list[k-1] = delta_h_prev        self.delta_f_list[k] = delta_f        self.delta_i_list[k] = delta_i        self.delta_o_list[k] = delta_o        self.delta_ct_list[k] = delta_ct</code></pre><p>å¦ä¸€éƒ¨åˆ†æ˜¯è®¡ç®—æ¢¯åº¦ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">     def calc_gradient(self, x):        # åˆå§‹åŒ–é—å¿˜é—¨æƒé‡æ¢¯åº¦çŸ©é˜µå’Œåç½®é¡¹        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (            self.init_weight_gradient_mat())        # åˆå§‹åŒ–è¾“å…¥é—¨æƒé‡æ¢¯åº¦çŸ©é˜µå’Œåç½®é¡¹        self.Wih_grad, self.Wix_grad, self.bi_grad = (            self.init_weight_gradient_mat())        # åˆå§‹åŒ–è¾“å‡ºé—¨æƒé‡æ¢¯åº¦çŸ©é˜µå’Œåç½®é¡¹        self.Woh_grad, self.Wox_grad, self.bo_grad = (            self.init_weight_gradient_mat())        # åˆå§‹åŒ–å•å…ƒçŠ¶æ€æƒé‡æ¢¯åº¦çŸ©é˜µå’Œåç½®é¡¹        self.Wch_grad, self.Wcx_grad, self.bc_grad = (            self.init_weight_gradient_mat())       # è®¡ç®—å¯¹ä¸Šä¸€æ¬¡è¾“å‡ºhçš„æƒé‡æ¢¯åº¦        for t in range(self.times, 0, -1):            # è®¡ç®—å„ä¸ªæ—¶åˆ»çš„æ¢¯åº¦            (Wfh_grad, bf_grad,            Wih_grad, bi_grad,            Woh_grad, bo_grad,            Wch_grad, bc_grad) = (                self.calc_gradient_t(t))            # å®é™…æ¢¯åº¦æ˜¯å„æ—¶åˆ»æ¢¯åº¦ä¹‹å’Œ            self.Wfh_grad += Wfh_grad            self.bf_grad += bf_grad            self.Wih_grad += Wih_grad            self.bi_grad += bi_grad            self.Woh_grad += Woh_grad            self.bo_grad += bo_grad            self.Wch_grad += Wch_grad            self.bc_grad += bc_grad            print '-----%d-----' % t            print Wfh_grad            print self.Wfh_grad        # è®¡ç®—å¯¹æœ¬æ¬¡è¾“å…¥xçš„æƒé‡æ¢¯åº¦        xt = x.transpose()        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)    def init_weight_gradient_mat(self):        '''        åˆå§‹åŒ–æƒé‡çŸ©é˜µ        '''        Wh_grad = np.zeros((self.state_width,            self.state_width))        Wx_grad = np.zeros((self.state_width,            self.input_width))        b_grad = np.zeros((self.state_width, 1))        return Wh_grad, Wx_grad, b_grad    def calc_gradient_t(self, t):        '''        è®¡ç®—æ¯ä¸ªæ—¶åˆ»tæƒé‡çš„æ¢¯åº¦        '''        h_prev = self.h_list[t-1].transpose()        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)        bf_grad = self.delta_f_list[t]        Wih_grad = np.dot(self.delta_i_list[t], h_prev)        bi_grad = self.delta_f_list[t]        Woh_grad = np.dot(self.delta_o_list[t], h_prev)        bo_grad = self.delta_f_list[t]        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)        bc_grad = self.delta_ct_list[t]        return Wfh_grad, bf_grad, Wih_grad, bi_grad, \               Woh_grad, bo_grad, Wch_grad, bc_grad</code></pre><h3 id="æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®ç°"><a href="#æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®ç°" class="headerlink" title="æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®ç°"></a>æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å®ç°</h3><p>ä¸‹é¢æ˜¯ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥æ›´æ–°æƒé‡ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">    def update(self):        '''        æŒ‰ç…§æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°æƒé‡        '''        self.Wfh -= self.learning_rate * self.Whf_grad        self.Wfx -= self.learning_rate * self.Whx_grad        self.bf -= self.learning_rate * self.bf_grad        self.Wih -= self.learning_rate * self.Whi_grad        self.Wix -= self.learning_rate * self.Whi_grad        self.bi -= self.learning_rate * self.bi_grad        self.Woh -= self.learning_rate * self.Wof_grad        self.Wox -= self.learning_rate * self.Wox_grad        self.bo -= self.learning_rate * self.bo_grad        self.Wch -= self.learning_rate * self.Wcf_grad        self.Wcx -= self.learning_rate * self.Wcx_grad        self.bc -= self.learning_rate * self.bc_grad</code></pre><h3 id="æ¢¯åº¦æ£€æŸ¥çš„å®ç°"><a href="#æ¢¯åº¦æ£€æŸ¥çš„å®ç°" class="headerlink" title="æ¢¯åº¦æ£€æŸ¥çš„å®ç°"></a>æ¢¯åº¦æ£€æŸ¥çš„å®ç°</h3><p>å’ŒRecurrentLayerä¸€æ ·ï¼Œä¸ºäº†æ”¯æŒæ¢¯åº¦æ£€æŸ¥ï¼Œæˆ‘ä»¬éœ€è¦æ”¯æŒé‡ç½®å†…éƒ¨çŠ¶æ€ï¼š</p><pre><code>    def reset_state(self):        # å½“å‰æ—¶åˆ»åˆå§‹åŒ–ä¸ºt0        self.times = 0               # å„ä¸ªæ—¶åˆ»çš„å•å…ƒçŠ¶æ€å‘é‡c        self.c_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å‡ºå‘é‡h        self.h_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„é—å¿˜é—¨f        self.f_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å…¥é—¨i        self.i_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„è¾“å‡ºé—¨o        self.o_list = self.init_state_vec()        # å„ä¸ªæ—¶åˆ»çš„å³æ—¶çŠ¶æ€c~        self.ct_list = self.init_state_vec()</code></pre><p>æœ€åï¼Œæ˜¯æ¢¯åº¦æ£€æŸ¥çš„ä»£ç ï¼š</p><pre class=" language-lang-python"><code class="language-lang-python">def data_set():    x = [np.array([[1], [2], [3]]),         np.array([[2], [3], [4]])]    d = np.array([[1], [2]])    return x, ddef gradient_check():    '''    æ¢¯åº¦æ£€æŸ¥    '''    # è®¾è®¡ä¸€ä¸ªè¯¯å·®å‡½æ•°ï¼Œå–æ‰€æœ‰èŠ‚ç‚¹è¾“å‡ºé¡¹ä¹‹å’Œ    error_function = lambda o: o.sum()    lstm = LstmLayer(3, 2, 1e-3)    # è®¡ç®—forwardå€¼    x, d = data_set()    lstm.forward(x[0])    lstm.forward(x[1])    # æ±‚å–sensitivity map    sensitivity_array = np.ones(lstm.h_list[-1].shape,                                dtype=np.float64)    # è®¡ç®—æ¢¯åº¦    lstm.backward(x[1], sensitivity_array, IdentityActivator())    # æ£€æŸ¥æ¢¯åº¦    epsilon = 10e-4    for i in range(lstm.Wfh.shape[0]):        for j in range(lstm.Wfh.shape[1]):            lstm.Wfh[i,j] += epsilon            lstm.reset_state()            lstm.forward(x[0])            lstm.forward(x[1])            err1 = error_function(lstm.h_list[-1])            lstm.Wfh[i,j] -= 2*epsilon            lstm.reset_state()            lstm.forward(x[0])            lstm.forward(x[1])            err2 = error_function(lstm.h_list[-1])            expect_grad = (err1 - err2) / (2 * epsilon)            lstm.Wfh[i,j] += epsilon            print 'weights(%d,%d): expected - actural %.4e - %.4e' % (                i, j, expect_grad, lstm.Wfh_grad[i,j])    return lstm</code></pre><p>æˆ‘ä»¬åªå¯¹$W_{fh}$åšäº†æ£€æŸ¥ï¼Œè¯»è€…å¯ä»¥è‡ªè¡Œå¢åŠ å¯¹å…¶ä»–æ¢¯åº¦çš„æ£€æŸ¥ã€‚ä¸‹é¢æ˜¯æŸæ¬¡æ¢¯åº¦æ£€æŸ¥çš„ç»“æœï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-cb1c4561375c22a1.png)</p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>å‰é¢æˆ‘ä»¬è®²äº†ä¸€ç§æ™®é€šçš„LSTMï¼Œäº‹å®ä¸ŠLSTMå­˜åœ¨å¾ˆå¤š<strong>å˜ä½“</strong>ï¼Œè®¸å¤šè®ºæ–‡ä¸­çš„LSTMéƒ½æˆ–å¤šæˆ–å°‘çš„ä¸å¤ªä¸€æ ·ã€‚åœ¨ä¼—å¤šçš„LSTMå˜ä½“ä¸­ï¼Œ<strong>GRU (Gated Recurrent Unit)</strong>ä¹Ÿè®¸æ˜¯æœ€æˆåŠŸçš„ä¸€ç§ã€‚å®ƒå¯¹LSTMåšäº†å¾ˆå¤šç®€åŒ–ï¼ŒåŒæ—¶å´ä¿æŒç€å’ŒLSTMç›¸åŒçš„æ•ˆæœã€‚å› æ­¤ï¼ŒGRUæœ€è¿‘å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚</p><p>GRUå¯¹LSTMåšäº†ä¸¤ä¸ªå¤§æ”¹åŠ¨ï¼š</p><ol><li>å°†è¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨å˜ä¸ºä¸¤ä¸ªé—¨ï¼šæ›´æ–°é—¨ï¼ˆUpdate Gateï¼‰$\mathbf{z}_t$å’Œé‡ç½®é—¨ï¼ˆReset Gateï¼‰$\mathbf{r}_t$ã€‚</li><li>å°†å•å…ƒçŠ¶æ€ä¸è¾“å‡ºåˆå¹¶ä¸ºä¸€ä¸ªçŠ¶æ€ï¼š$\mathbf{h}$ã€‚</li></ol><p>GRUçš„å‰å‘è®¡ç®—å…¬å¼ä¸ºï¼š</p><p>$\begin{align} \mathbf{z}_t&amp;=\sigma(W_z\cdot[\mathbf{h}_{t-1},\mathbf{x}_t])\\ \mathbf{r}_t&amp;=\sigma(W_r\cdot[\mathbf{h}_{t-1},\mathbf{x}_t])\\ \mathbf{\tilde{h}}_t&amp;=\tanh(W\cdot[\mathbf{r}_t\circ\mathbf{h}_{t-1},\mathbf{x}_t])\\ \mathbf{h}&amp;=(1-\mathbf{z}_t)\circ\mathbf{h}_{t-1}+\mathbf{z}_t\circ\mathbf{\tilde{h}}_t \end{align} $</p><p>ä¸‹å›¾æ˜¯GRUçš„ç¤ºæ„å›¾ï¼š</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-b784d887bf693253.png)</p><p>GRUçš„è®­ç»ƒç®—æ³•æ¯”LSTMç®€å•ä¸€äº›ï¼Œç•™ç»™è¯»è€…è‡ªè¡Œæ¨å¯¼ï¼Œæœ¬æ–‡å°±ä¸å†èµ˜è¿°äº†ã€‚</p><h2 id="å°ç»“"><a href="#å°ç»“" class="headerlink" title="å°ç»“"></a>å°ç»“</h2><p>è‡³æ­¤ï¼ŒLSTMâ€”â€”ä¹Ÿè®¸æ˜¯ç»“æ„æœ€å¤æ‚çš„ä¸€ç±»ç¥ç»ç½‘ç»œâ€”â€”å°±è®²å®Œäº†ï¼Œç›¸ä¿¡æ‹¿ä¸‹å‰å‡ ç¯‡æ–‡ç« çš„è¯»è€…ä»¬æå®šè¿™ç¯‡æ–‡ç« ä¹Ÿä¸åœ¨è¯ä¸‹å§ï¼ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£<strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong>å’Œå®ƒæœ€æµè¡Œçš„å˜ä½“â€”â€”<strong>LSTM</strong>ï¼Œå®ƒä»¬éƒ½å¯ä»¥ç”¨æ¥å¤„ç†åºåˆ—ã€‚ä½†æ˜¯ï¼Œæœ‰æ—¶å€™ä»…ä»…æ‹¥æœ‰å¤„ç†åºåˆ—çš„èƒ½åŠ›è¿˜ä¸å¤Ÿï¼Œè¿˜éœ€è¦å¤„ç†æ¯”åºåˆ—æ›´ä¸ºå¤æ‚çš„ç»“æ„ï¼ˆæ¯”å¦‚æ ‘ç»“æ„ï¼‰ï¼Œè¿™æ—¶å€™å°±éœ€è¦ç”¨åˆ°å¦å¤–ä¸€ç±»ç½‘ç»œï¼š<strong>é€’å½’ç¥ç»ç½‘ç»œ(Recursive Neural Network)</strong>ï¼Œå·§åˆçš„æ˜¯ï¼Œå®ƒçš„ç¼©å†™ä¹Ÿæ˜¯<strong>RNN</strong>ã€‚åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»<strong>é€’å½’ç¥ç»ç½‘ç»œ</strong>å’Œå®ƒçš„è®­ç»ƒç®—æ³•ã€‚ç°åœ¨ï¼Œæ¼«é•¿çš„çƒ§è„‘æš‚å‘Šä¸€æ®µè½ï¼Œä¼‘æ¯ä¸€ä¸‹å§:)</p><p><img src="%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(6" alt="img">%20-%20%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(LSTM)/2256672-9ba33f65294bfb98.jpg)</p><h2 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h2><ol><li><a href="http://cs224d.stanford.edu/" target="_blank" rel="noopener">CS224d: Deep Learning for Natural Language Processing</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="http://arunmallya.github.io/writeups/nn/lstm/index.html" target="_blank" rel="noopener">LSTM Forward and Backward Pass</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> æ·±åº¦å­¦ä¹  </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Generate Tweets Using Markov Chains</title>
      <link href="/other/generate-tweets-using-markov-chain/"/>
      <url>/other/generate-tweets-using-markov-chain/</url>
      
        <content type="html"><![CDATA[<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><pre class=" language-lang-python"><code class="language-lang-python">import markovify</code></pre><h2 id="Load-Corpus"><a href="#Load-Corpus" class="headerlink" title="Load Corpus"></a>Load Corpus</h2><p>The corpus I am using is just one I found online. The corpus you choose is central to generating realistic text.</p><pre class=" language-lang-python"><code class="language-lang-python"># Get raw text as stringwith open("brown.txt") as f:    text = f.read()</code></pre><h2 id="Build-Markov-Chain"><a href="#Build-Markov-Chain" class="headerlink" title="Build Markov Chain"></a>Build Markov Chain</h2><pre class=" language-lang-python"><code class="language-lang-python"># Build the model.text_model = markovify.Text(text)</code></pre><h1 id="Generate-One-Tweet"><a href="#Generate-One-Tweet" class="headerlink" title="Generate One Tweet"></a>Generate One Tweet</h1><pre class=" language-lang-python"><code class="language-lang-python"># Print three randomly-generated sentences of no more than 140 charactersfor i in range(3):    print(text_model.make_short_sentence(140))</code></pre><pre><code>Within a month, calls were still productive and most devotees of baseball attended the dozens of them.Even death, therefore, has a leather bolo drawn through a local rajah in 1949.He had a rather sharp and confident.</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mine Twitter&#39;s Stream For Hashtags Or Words</title>
      <link href="/other/mine-a-twitter-hashtags-and-words/"/>
      <url>/other/mine-a-twitter-hashtags-and-words/</url>
      
        <content type="html"><![CDATA[<p>This is a script which monitorâ€™s Twitter for tweets containing certain hashtags, words, or phrases. When one of those appears, it saves that tweet, and the userâ€™s information to a csv file. A similar version of this script is available on <a href="https://github.com/chrisalbon/twitter_miner" target="_blank" rel="noopener">GitHub here</a>. The main difference between the code presented here and the repo is that here I am added extensive comments in the code explaining what is happening. Also, the code below runs as a Jupyter notebook.</p><p>To get the code below to run, you need to added your own Twitter API credentials.</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><pre class=" language-lang-python"><code class="language-lang-python">#Import librariesfrom tweepy.streaming import StreamListenerfrom tweepy import OAuthHandlerfrom tweepy import Streamimport timeimport csvimport sys</code></pre><h2 id="Create-A-Twitter-Stream-Miner"><a href="#Create-A-Twitter-Stream-Miner" class="headerlink" title="Create A Twitter Stream Miner"></a>Create A Twitter Stream Miner</h2><pre class=" language-lang-python"><code class="language-lang-python"># Create a streamer objectclass StdOutListener(StreamListener):    # Define a function that is initialized when the miner is called    def __init__(self, api = None):        # That sets the api        self.api = api        # Create a file with 'data_' and the current time        self.filename = 'data'+'_'+time.strftime('%Y%m%d-%H%M%S')+'.csv'        # Create a new file with that filename        csvFile = open(self.filename, 'w')        # Create a csv writer        csvWriter = csv.writer(csvFile)        # Write a single row with the headers of the columns        csvWriter.writerow(['text',                            'created_at',                            'geo',                            'lang',                            'place',                            'coordinates',                            'user.favourites_count',                            'user.statuses_count',                            'user.description',                            'user.location',                            'user.id',                            'user.created_at',                            'user.verified',                            'user.following',                            'user.url',                            'user.listed_count',                            'user.followers_count',                            'user.default_profile_image',                            'user.utc_offset',                            'user.friends_count',                            'user.default_profile',                            'user.name',                            'user.lang',                            'user.screen_name',                            'user.geo_enabled',                            'user.profile_background_color',                            'user.profile_image_url',                            'user.time_zone',                            'id',                            'favorite_count',                            'retweeted',                            'source',                            'favorited',                            'retweet_count'])    # When a tweet appears    def on_status(self, status):        # Open the csv file created previously        csvFile = open(self.filename, 'a')        # Create a csv writer        csvWriter = csv.writer(csvFile)        # If the tweet is not a retweet        if not 'RT @' in status.text:            # Try to             try:                # Write the tweet's information to the csv file                csvWriter.writerow([status.text,                                    status.created_at,                                    status.geo,                                    status.lang,                                    status.place,                                    status.coordinates,                                    status.user.favourites_count,                                    status.user.statuses_count,                                    status.user.description,                                    status.user.location,                                    status.user.id,                                    status.user.created_at,                                    status.user.verified,                                    status.user.following,                                    status.user.url,                                    status.user.listed_count,                                    status.user.followers_count,                                    status.user.default_profile_image,                                    status.user.utc_offset,                                    status.user.friends_count,                                    status.user.default_profile,                                    status.user.name,                                    status.user.lang,                                    status.user.screen_name,                                    status.user.geo_enabled,                                    status.user.profile_background_color,                                    status.user.profile_image_url,                                    status.user.time_zone,                                    status.id,                                    status.favorite_count,                                    status.retweeted,                                    status.source,                                    status.favorited,                                    status.retweet_count])            # If some error occurs            except Exception as e:                # Print the error                print(e)                # and continue                pass        # Close the csv file        csvFile.close()        # Return nothing        return    # When an error occurs    def on_error(self, status_code):        # Print the error code        print('Encountered error with status code:', status_code)        # If the error code is 401, which is the error for bad credentials        if status_code == 401:            # End the stream            return False    # When a deleted tweet appears    def on_delete(self, status_id, user_id):        # Print message        print("Delete notice")        # Return nothing        return    # When reach the rate limit    def on_limit(self, track):        # Print rate limiting error        print("Rate limited, continuing")        # Continue mining tweets        return True    # When timed out    def on_timeout(self):        # Print timeout message        print(sys.stderr, 'Timeout...')        # Wait 10 seconds        time.sleep(10)        # Return nothing        return</code></pre><h2 id="Create-A-Wrapper-For-The-Miner"><a href="#Create-A-Wrapper-For-The-Miner" class="headerlink" title="Create A Wrapper For The Miner"></a>Create A Wrapper For The Miner</h2><pre class=" language-lang-python"><code class="language-lang-python"># Create a mining functiondef start_mining(queries):    '''    Inputs list of strings. Returns tweets containing those strings.    '''    #Variables that contains the user credentials to access Twitter API    consumer_key = "YOUR_CREDENTIALS"    consumer_secret = "YOUR_CREDENTIALS"    access_token = "YOUR_CREDENTIALS"    access_token_secret = "YOUR_CREDENTIALS"    # Create a listener    l = StdOutListener()    # Create authorization info    auth = OAuthHandler(consumer_key, consumer_secret)    auth.set_access_token(access_token, access_token_secret)    # Create a stream object with listener and authorization    stream = Stream(auth, l)    # Run the stream object using the user defined queries    stream.filter(track=queries)</code></pre><h2 id="Run-The-Stream-Miner"><a href="#Run-The-Stream-Miner" class="headerlink" title="Run The Stream Miner"></a>Run The Stream Miner</h2><pre class=" language-lang-python"><code class="language-lang-python"># Start the minerstart_mining(['python', '#Python'])</code></pre><pre><code>Encountered error with status code: 401</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>What Is The Probability An Economy Class Seat Is An Aisle Seat?</title>
      <link href="/other/aisle-seat-probabilities/"/>
      <url>/other/aisle-seat-probabilities/</url>
      
        <content type="html"><![CDATA[<p>There are two types of people in the world, aisle seaters and window seaters. I am an aisle seater, nothing is worse than limited bathroom access on a long flight. The first thing I do when I get my ticket is check to see if I have a window seat. If not, I immediately head over to the airline counter and try to get one.</p><p>Last flight, on Turkish Airlines, I ran into a curious situation. I recieved my boarding pass with my seat number, 18C, but the ticket did not specify if C was an aisle seat or not. Making matters worse, the airline counter was swamped with a few dozen people. So I asked myself: <strong>given only the seat letter, C, what is the probability that it is an aisle seat?</strong></p><p>Later, on the flight, I decided to find out.</p><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><pre class=" language-lang-python"><code class="language-lang-python"># Import required modulesimport pandas as pdimport numpy as np# Set plots to display in the iPython notebook%matplotlib inline</code></pre><h2 id="Setup-possible-seat-configurations"><a href="#Setup-possible-seat-configurations" class="headerlink" title="Setup possible seat configurations"></a>Setup possible seat configurations</h2><p>I am a pretty frequently flyer on a variety of airlines and aircraft. There are a variety of seating configurations out there, but typically they follow some basic rules:</p><ul><li>No window cluster of seats has more than three seats.</li><li>On small flights with three seats, the single seat is on the left side.</li><li>No flight has more than nine rows.</li></ul><p>Based on these rules, here are the â€œtypicalâ€ seating configurations from aircraft with between two and nine seats per row. A â€˜1â€™ codifies that a seat is an aisle seat, a â€˜0â€™ codifies that it is a non-aisle seat (i.e. window or middle), and â€˜np.nanâ€™ denotes that the aircraft has less than nine seats (this is so all the list lengths are the same). </p><pre class=" language-lang-python"><code class="language-lang-python"># An aircraft with two seats per rowrows2 = [1,1, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]# An aircraft with three seats per rowrows3 = [1,1,0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,]# An aircraft with four seats per rowrows4 = [0,1,1,0, np.nan, np.nan, np.nan, np.nan, np.nan]# An aircraft with five seats per rowrows5 = [0,1,1,0,0, np.nan, np.nan,np.nan, np.nan]# An aircraft with six seats per rowrows6 = [0,1,1,1,1,0, np.nan, np.nan, np.nan]# An aircraft with seven seats per rowrows7 = [0,1,1,0,1,1,0, np.nan, np.nan]# An aircraft with eight seats per rowrows8 = [0,0,1,1,1,1,0,0, np.nan]# An aircraft with nine seats per rowrows9 = [0,0,1,1,0,1,1,0,0]</code></pre><p>For example, in an aircraft with five seats per row, <code>rows5</code>, the seating arrangement would be:</p><ol><li>window</li><li>aisle</li><li>aisle</li><li>middle</li><li>window</li><li>no seat</li><li>no seat</li><li>no seat</li><li>no seat</li></ol><p>Next, Iâ€™m take advantage of pandas row summation options, but to do this I need to wrangle the data into a pandas dataframe. Essentially I am using the pandas dataframe as a matrix.</p><pre class=" language-lang-python"><code class="language-lang-python"># Create a list variable of all possible aircraft configurationsseating_map = [rows2, rows3, rows4, rows5, rows6, rows7, rows8, rows9]</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># Create a dataframe from the seating_map variabledf = pd.DataFrame(seating_map,                   columns=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],                  index=['rows2', 'rows3', 'rows4', 'rows5', 'rows6', 'rows7', 'rows8', 'rows9'])</code></pre><p>Here is all the data we need to construct our probabilities. The columns represent individual seat letters (A, B, etc.) while the rows represent the number of seats-per-row in the aircraft.</p><pre class=" language-lang-python"><code class="language-lang-python"># View the dataframedf</code></pre><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }    .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>      <th>F</th>      <th>G</th>      <th>H</th>      <th>I</th>    </tr>  </thead>  <tbody>    <tr>      <th>rows2</th>      <td>1</td>      <td>1</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows3</th>      <td>1</td>      <td>1</td>      <td>0.0</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows4</th>      <td>0</td>      <td>1</td>      <td>1.0</td>      <td>0.0</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows5</th>      <td>0</td>      <td>1</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows6</th>      <td>0</td>      <td>1</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>NaN</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows7</th>      <td>0</td>      <td>1</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>rows8</th>      <td>0</td>      <td>0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>      <td>NaN</td>    </tr>    <tr>      <th>rows9</th>      <td>0</td>      <td>0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>0.0</td>    </tr>  </tbody></table></div><h2 id="Calculate-aisle-probability"><a href="#Calculate-aisle-probability" class="headerlink" title="Calculate aisle probability"></a>Calculate aisle probability</h2><p>Because each aircraft seats-per-row configuration (i.e. row) is binary (1 if aisle, 0 if non-aisle), the probability that a seat is an aisle is simply the mean value of each seat letter (i.e. column).</p><pre class=" language-lang-python"><code class="language-lang-python"># Create a list wherein each element is the mean value of a columnaisle_probability = [df['A'].mean(),                      df['B'].mean(),                     df['C'].mean(),                     df['D'].mean(),                     df['E'].mean(),                     df['F'].mean(),                     df['G'].mean(),                     df['H'].mean(),                     df['I'].mean()]</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># Display the variableaisle_probability</code></pre><pre><code>[0.25, 0.75, 0.8571428571428571, 0.5, 0.6, 0.75, 0.3333333333333333, 0.0, 0.0]</code></pre><p>So there you have it, the probability that each seat letter is an aisle. However, we can make the presentation a little more intituative.</p><h2 id="Visualize-seat-letter-probabilities"><a href="#Visualize-seat-letter-probabilities" class="headerlink" title="Visualize seat letter probabilities"></a>Visualize seat letter probabilities</h2><p>The most obvious visualization to convey the probabilities would be seat letters on the x-axis and probabilities on the y-axis. Pandaâ€™s plot function makes that easy.</p><pre class=" language-lang-python"><code class="language-lang-python"># Create a list of strings to use as the x-axis labelsseats = ['Seat A', 'Seat B', 'Seat C', 'Seat D',          'Seat E', 'Seat F', 'Seat G', 'Seat H', 'Seat I']</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># Plot the probabilities, using 'seats' as the index as a bar chartpd.Series(aisle_probability, index=seats).plot(kind='bar', # set y to range between 0 and 1                                                    ylim=[0,1],                                                    # set the figure size                                                    figsize=[10,6],                                                    # set the figure title                                                    title='Probabilty of being an Aisle Seat in Economy Class')</code></pre><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x10f1231d0&gt;</code></pre><p><img src="aisle_seat_probabilities_20_1.png" alt="png"></p><p>So there we have it! If given a boarding pass with seat C you have a 86% probability of being in an aisle seat!</p><p>I hope this was helpful!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Simple Clustering With SciPy</title>
      <link href="/other/scipy-simple-clustering/"/>
      <url>/other/scipy-simple-clustering/</url>
      
        <content type="html"><![CDATA[<h3 id="Import-modules"><a href="#Import-modules" class="headerlink" title="Import modules"></a>Import modules</h3><pre class=" language-lang-python"><code class="language-lang-python">import numpy as np%matplotlib inlineimport matplotlib.pyplot as pltfrom scipy.cluster import vq</code></pre><h3 id="Create-coordinates-for-battles-for-each-year-of-the-war"><a href="#Create-coordinates-for-battles-for-each-year-of-the-war" class="headerlink" title="Create coordinates for battles for each year of the war"></a>Create coordinates for battles for each year of the war</h3><pre class=" language-lang-python"><code class="language-lang-python"># create 100 coordinate pairs (i.e. two values), then add 5 to all of themyear_1 = np.random.randn(100, 2) + 5# create 30 coordinatee pairs (i.e. two values), then subtract 5 to all of themyear_2 = np.random.randn(30, 2) - 5# create 50 coordinatee pairs (i.e. two values)year_3 = np.random.randn(50, 2)</code></pre><h3 id="View-the-first-3-entries-of-each-year-of-battles"><a href="#View-the-first-3-entries-of-each-year-of-battles" class="headerlink" title="View the first 3 entries of each year of battles"></a>View the first 3 entries of each year of battles</h3><pre class=" language-lang-python"><code class="language-lang-python">print('year 1 battles:',  year_1[0:3])print('year 2 battles:', year_2[0:3])print('year 3 battles:', year_3[0:3])</code></pre><pre><code>year 1 battles: [[ 5.25720722  4.78051294] [ 4.11980541  6.24062638] [ 4.04612449  5.23819217]]year 2 battles: [[-3.90607071 -5.20880154] [-4.14244415 -4.52520445] [-6.01162308 -5.53489708]]year 3 battles: [[-0.54820297 -0.97483204] [ 0.12813873  0.55198748] [-0.55677223 -0.68900608]]</code></pre><h3 id="Pool-all-three-years-of-coordinates"><a href="#Pool-all-three-years-of-coordinates" class="headerlink" title="Pool all three years of coordinates"></a>Pool all three years of coordinates</h3><pre class=" language-lang-python"><code class="language-lang-python"># vertically stack year_1, year_2, and year_3 elementsbattles = np.vstack([year_1, year_2, year_3])</code></pre><h3 id="Cluster-the-battle-locations-into-three-groups"><a href="#Cluster-the-battle-locations-into-three-groups" class="headerlink" title="Cluster the battle locations into three groups"></a>Cluster the battle locations into three groups</h3><pre class=" language-lang-python"><code class="language-lang-python"># calculate the centroid coordinates of each cluster # and the variance of all the clusterscentroids, variance  = vq.kmeans(battles, 3)</code></pre><h3 id="View-the-centroid-coordinate-for-each-of-the-three-clusters"><a href="#View-the-centroid-coordinate-for-each-of-the-three-clusters" class="headerlink" title="View the centroid coordinate for each of the three clusters"></a>View the centroid coordinate for each of the three clusters</h3><pre class=" language-lang-python"><code class="language-lang-python">centroids</code></pre><pre><code>array([[ 5.02707263,  5.03041508],       [-0.05392784,  0.12892838],       [-4.88957266, -4.85051116]])</code></pre><h3 id="View-the-variance-of-the-clusters-they-all-share-the-same"><a href="#View-the-variance-of-the-clusters-they-all-share-the-same" class="headerlink" title="View the variance of the clusters (they all share the same)"></a>View the variance of the clusters (they all share the same)</h3><pre class=" language-lang-python"><code class="language-lang-python">variance</code></pre><pre><code>1.2948126660038406</code></pre><h3 id="Seperate-the-battle-data-into-clusters"><a href="#Seperate-the-battle-data-into-clusters" class="headerlink" title="Seperate the battle data into clusters"></a>Seperate the battle data into clusters</h3><pre class=" language-lang-python"><code class="language-lang-python">identified, distance = vq.vq(battles, centroids)</code></pre><h3 id="View-the-cluster-of-each-battle"><a href="#View-the-cluster-of-each-battle" class="headerlink" title="View the cluster of each battle"></a>View the cluster of each battle</h3><pre class=" language-lang-python"><code class="language-lang-python">identified</code></pre><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)</code></pre><h3 id="View-the-distance-of-each-individual-battle-from-their-clusterâ€™s-centroid"><a href="#View-the-distance-of-each-individual-battle-from-their-clusterâ€™s-centroid" class="headerlink" title="View the distance of each individual battle from their clusterâ€™s centroid"></a>View the distance of each individual battle from their clusterâ€™s centroid</h3><pre class=" language-lang-python"><code class="language-lang-python">distance</code></pre><pre><code>array([ 0.3397249 ,  1.51252941,  1.00271161,  0.7583883 ,  0.58103782,        1.81905849,  1.45452846,  1.34523274,  0.69254441,  3.32123157,        1.73900653,  1.01999434,  1.5392708 ,  0.64417605,  1.25822142,        1.68913457,  1.09543587,  0.20750281,  2.90778804,  1.62549404,        1.0224336 ,  1.05196193,  0.98434964,  0.25634371,  1.19779956,        1.73517217,  2.69339667,  1.32792584,  0.97809768,  1.52654056,        2.20554365,  1.0403091 ,  0.93698624,  1.53359041,  0.91717984,        0.3008527 ,  0.42901893,  0.95824461,  1.93321831,  1.89139314,        1.49982335,  0.63265951,  1.48579627,  1.04574742,  0.83477916,        2.80489932,  1.50671741,  0.35230994,  1.18607368,  1.36078497,        1.17298152,  0.95961251,  0.95348923,  1.41903574,  1.7816999 ,        1.32087763,  0.94807163,  2.22741733,  0.66198152,  0.97404075,        0.24009773,  1.22021557,  1.36298565,  1.77358477,  0.62586652,        1.45234278,  1.87925214,  2.18673534,  0.97113871,  1.0436524 ,        1.63491437,  1.43922603,  1.8066756 ,  2.55661988,  0.64905457,        0.6939938 ,  1.41183181,  2.72140674,  1.70390906,  3.53986459,        1.52044903,  1.98702847,  1.2488108 ,  2.61774172,  2.66067284,        0.80078946,  0.79648259,  2.72215296,  1.26904383,  1.16048896,        1.42571458,  1.18519189,  0.46592397,  0.63831379,  0.2294296 ,        0.90199062,  0.99296186,  1.79154225,  0.23854105,  1.19095902,        1.0467321 ,  0.81487758,  1.31429876,  0.14625493,  1.04421102,        0.72132375,  2.2209666 ,  1.00145286,  1.30465026,  1.57217776,        1.31999891,  0.80321763,  2.12942642,  0.81168612,  1.40294667,        0.89994242,  1.70402817,  0.79621269,  1.29554062,  1.87340273,        2.40582742,  2.99089606,  1.01348705,  0.54974364,  0.39367389,        2.28343779,  1.51924388,  0.52095884,  1.54219385,  0.62972955,        1.20937793,  0.46057272,  0.96014023,  0.2287637 ,  0.84009151,        1.34393522,  1.5983523 ,  0.46066181,  0.49504327,  2.22788557,        1.74688212,  1.99998478,  0.25864751,  1.06955924,  1.68029793,        3.41862662,  1.9273365 ,  0.91580509,  0.94390424,  1.42991149,        0.64314749,  0.26250126,  1.09000179,  0.42658645,  0.40866344,        0.47829004,  0.47718204,  0.53641019,  1.42037169,  2.20413065,        1.85270104,  1.9544685 ,  1.40727147,  0.85730366,  1.63316935,        1.09642325,  1.36490331,  1.307389  ,  1.9727463 ,  1.35859479,        2.43699622,  0.80833152,  2.50758584,  0.95216108,  0.16936114,        0.98714981,  0.19962377,  1.13262204,  2.47056129,  2.00154513])</code></pre><h3 id="Index-the-battles-data-by-the-cluster-to-which-they-belong"><a href="#Index-the-battles-data-by-the-cluster-to-which-they-belong" class="headerlink" title="Index the battles data by the cluster to which they belong"></a>Index the battles data by the cluster to which they belong</h3><pre class=" language-lang-python"><code class="language-lang-python">cluster_1 = battles[identified == 0]cluster_2 = battles[identified == 1]cluster_3 = battles[identified == 2]</code></pre><h3 id="Print-the-first-three-coordinate-pairs-of-each-cluster"><a href="#Print-the-first-three-coordinate-pairs-of-each-cluster" class="headerlink" title="Print the first three coordinate pairs of each cluster"></a>Print the first three coordinate pairs of each cluster</h3><pre class=" language-lang-python"><code class="language-lang-python">print(cluster_1[0:3])print(cluster_2[0:3])print(cluster_3[0:3])</code></pre><pre><code>[[ 5.25720722  4.78051294] [ 4.11980541  6.24062638] [ 4.04612449  5.23819217]][[-0.54820297 -0.97483204] [ 0.12813873  0.55198748] [-0.55677223 -0.68900608]][[-3.90607071 -5.20880154] [-4.14244415 -4.52520445] [-6.01162308 -5.53489708]]</code></pre><h3 id="Plot-all-the-battles-color-each-battle-by-cluster"><a href="#Plot-all-the-battles-color-each-battle-by-cluster" class="headerlink" title="Plot all the battles, color each battle by cluster"></a>Plot all the battles, color each battle by cluster</h3><pre class=" language-lang-python"><code class="language-lang-python"># create a scatter plot there the x-axis is the first column of battles# the y-axis is the second column of battles, the size is 100, and# the color of each point is determined by the indentified variableplt.scatter(battles[:,0], battles[:,1], s=100, c=identified)</code></pre><pre><code>&lt;matplotlib.collections.PathCollection at 0x10d43f588&gt;</code></pre><p><img src="scipy_simple_clustering_26_1.png" alt="png"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
